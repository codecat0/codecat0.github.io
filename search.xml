<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Activation</title>
    <url>/2022/08/30/Activation/</url>
    <content><![CDATA[<h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><p>如果不用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，最终的输出都是输入的线性组合。<br>激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数。<br><span id="more"></span></p>
<h2 id="1-激活函数的种类"><a href="#1-激活函数的种类" class="headerlink" title="1. 激活函数的种类"></a>1. 激活函数的种类</h2><h3 id="1-sigmoid"><a href="#1-sigmoid" class="headerlink" title="1. sigmoid"></a>1. sigmoid</h3><p>函数定义：</p>
<script type="math/tex; mode=display">{ f }(x)=\sigma (x)=\frac { 1 }{ 1+{ e }^{ -x } }</script><p>导数：</p>
<script type="math/tex; mode=display">{ f }^{ ' }(x)=f(x)(1-f(x))</script><p>优点：</p>
<ol>
<li>$sigmoid$ 函数的输出映射在 $(0,1)$ 之间，单调连续，输出范围有限，优化稳定，可以用作输出层；</li>
<li>求导容易；</li>
</ol>
<p>缺点：</p>
<ol>
<li>由于其软饱和性，一旦落入饱和区梯度就会接近于0，根据反向传播的链式法则，容易产生梯度消失，导致训练出现问题；</li>
<li>Sigmoid函数的输出恒大于0。非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（Bias Shift），并进一步使得梯度下降的收敛速度变慢；</li>
<li>计算时，由于具有幂运算，计算复杂度较高，运算速度较慢。</li>
</ol>
<h3 id="2-tanh"><a href="#2-tanh" class="headerlink" title="2. tanh"></a>2. tanh</h3><p>函数定义：</p>
<script type="math/tex; mode=display">{ f }(x)=tanh(x)=\frac { { e }^{ x }-{ e }^{ -x } }{ { e }^{ x }+{ e }^{ -x } }</script><p>导数：</p>
<script type="math/tex; mode=display">{ f }^{ ' }(x)=1-f(x)^{ 2 }</script><p>优点：</p>
<ol>
<li>$tanh$ 比 $sigmoid$ 函数收敛速度更快；</li>
<li>相比 $sigmoid$ 函数，$tanh$ 是以 $0$ 为中心的；</li>
</ol>
<p>缺点：</p>
<ol>
<li>与 $sigmoid$ 函数相同，由于饱和性容易产生的梯度消失；</li>
<li>与 $sigmoid$ 函数相同，由于具有幂运算，计算复杂度较高，运算速度较慢。</li>
</ol>
<h3 id="3-ReLU"><a href="#3-ReLU" class="headerlink" title="3. ReLU"></a>3. ReLU</h3><p>函数定义：</p>
<script type="math/tex; mode=display">f(x)=\begin{cases} \begin{matrix} 0 & x<0 \end{matrix} \\ \begin{matrix} x & x\ge 0 \end{matrix} \end{cases}</script><p>导数：</p>
<script type="math/tex; mode=display">{ { f }(x) }^{ ' }=\begin{cases} \begin{matrix} 0 & x<0 \end{matrix} \\ \begin{matrix} 1 & x\ge 0 \end{matrix} \end{cases}</script><p>优点：</p>
<ol>
<li>收敛速度快；</li>
<li>相较于 $sigmoid$ 和 $tanh$ 中涉及了幂运算，导致计算复杂度高， ReLU​可以更加简单的实现；</li>
<li>当输入 $x&gt;=0$ 时，ReLU​ 的导数为常数，这样可有效缓解梯度消失问题；</li>
<li>当 $x&lt;0$ 时，ReLU​ 的梯度总是 $0$，提供了神经网络的稀疏表达能力；</li>
</ol>
<p>缺点：</p>
<ol>
<li>ReLU​ 的输出不是以 $0$ 为中心的；</li>
<li>神经元坏死现象，某些神经元可能永远不会被激活，导致相应参数永远不会被更新；</li>
<li>不能避免梯度爆炸问题；</li>
</ol>
<h3 id="4-LReLU"><a href="#4-LReLU" class="headerlink" title="4. LReLU"></a>4. LReLU</h3><p>函数定义：</p>
<script type="math/tex; mode=display">f(x)=\begin{cases} \begin{matrix} \alpha x & x<0 \end{matrix} \\ \begin{matrix} x & x\ge 0 \end{matrix} \end{cases}</script><p>导数：</p>
<script type="math/tex; mode=display">{ { f }(x) }^{ ' }=\begin{cases} \begin{matrix} \alpha & x<0 \end{matrix} \\ \begin{matrix} 1 & x\ge 0 \end{matrix} \end{cases}</script><p>其中，$\alpha$ 常设置为0.01。函数图如 <strong>图6</strong> 所示：</p>
<p>优点：</p>
<ol>
<li>避免梯度消失；</li>
<li>由于导数总是不为零，因此可减少死神经元的出现；</li>
</ol>
<p>缺点：</p>
<ol>
<li>LReLU​ 表现并不一定比 ReLU​ 好；</li>
<li>无法避免梯度爆炸问题；</li>
</ol>
<h3 id="5-PReLU"><a href="#5-PReLU" class="headerlink" title="5. PReLU"></a>5. PReLU</h3><p>函数定义：</p>
<script type="math/tex; mode=display">f(\alpha ,x)=\begin{cases} \begin{matrix} \alpha x  & x<0 \end{matrix} \\ \begin{matrix} x & x\ge 0 \end{matrix} \end{cases}</script><p>导数：</p>
<script type="math/tex; mode=display">{ { f }(\alpha ,x) }^{ ' }=\begin{cases} \begin{matrix} \alpha  & x<0 \end{matrix} \\ \begin{matrix} 1 & x\ge 0 \end{matrix} \end{cases}</script><p>优点：</p>
<ol>
<li>PReLU​ 是 LReLU 的改进，可以自适应地从数据中学习参数；</li>
<li>收敛速度快、错误率低；</li>
<li>PReLU 可以用于反向传播的训练，可以与其他层同时优化；</li>
</ol>
<h3 id="6-RReLU"><a href="#6-RReLU" class="headerlink" title="6. RReLU"></a>6. RReLU</h3><p>函数定义：</p>
<script type="math/tex; mode=display">f(\alpha ,x)=\begin{cases} \begin{matrix} \alpha  & x<0 \end{matrix} \\ \begin{matrix} x & x\ge 0 \end{matrix} \end{cases}</script><p>导数：</p>
<script type="math/tex; mode=display">{ { f }(\alpha ,x) }^{ ' }=\begin{cases} \begin{matrix} \alpha  & x<0 \end{matrix} \\ \begin{matrix} 1 & x\ge 0 \end{matrix} \end{cases}</script><p>优点：为负值输入添加了一个线性项，这个线性项的斜率在每一个节点上都是随机分配的（通常服从均匀分布）。</p>
<h3 id="7-ELU"><a href="#7-ELU" class="headerlink" title="7. ELU"></a>7. ELU</h3><p>函数定义：</p>
<script type="math/tex; mode=display">f(\alpha ,x)=\begin{cases} \begin{matrix} \alpha \left( { e }^{ x }-1 \right)  & x<0 \end{matrix} \\ \begin{matrix} x & x\ge 0 \end{matrix} \end{cases}</script><p>导数：</p>
<script type="math/tex; mode=display">{ { f }(\alpha ,x) }^{ ' }=\begin{cases} \begin{matrix} f(\alpha ,x)+\alpha  & x<0 \end{matrix} \\ \begin{matrix} 1 & x\ge 0 \end{matrix} \end{cases}</script><p>优点：</p>
<ol>
<li>导数收敛为零，从而提高学习效率；</li>
<li>能得到负值输出，这能帮助网络向正确的方向推动权重和偏置变化；</li>
<li>防止死神经元出现。</li>
</ol>
<p>缺点：</p>
<ol>
<li>计算量大，其表现并不一定比 ReLU 好；</li>
<li>无法避免梯度爆炸问题；</li>
</ol>
<h3 id="8-SELU"><a href="#8-SELU" class="headerlink" title="8. SELU"></a>8. SELU</h3><p>函数定义：</p>
<script type="math/tex; mode=display">f(\alpha ,x)=\lambda \begin{cases} \begin{matrix} \alpha \left( { e }^{ x }-1 \right)  & x<0 \end{matrix} \\ \begin{matrix} x & x\ge 0 \end{matrix} \end{cases}</script><p>导数：</p>
<script type="math/tex; mode=display">{ { f }(\alpha ,x) }^{ ' }=\lambda \begin{cases} \begin{matrix} \alpha \left( { e }^{ x } \right)  & x<0 \end{matrix} \\ \begin{matrix} 1 & x\ge 0 \end{matrix} \end{cases}</script><p>优点：</p>
<ol>
<li>SELU 是 ELU 的一个变种。其中 λ 和 α 是固定数值（分别为 $1.0507$ 和 $1.6726$）;</li>
<li>经过该激活函数后使得样本分布自动归一化到 $0$ 均值和单位方差;</li>
<li>不会出现梯度消失或爆炸问题;</li>
</ol>
<h3 id="9-softsign"><a href="#9-softsign" class="headerlink" title="9. softsign"></a>9. softsign</h3><p>函数定义：</p>
<script type="math/tex; mode=display">f(x)=\frac { x }{ \left| x \right| +1 }</script><p>导数：</p>
<script type="math/tex; mode=display">{ f }^{ ' }(x)=\frac { 1 }{ { (1+\left| x \right| ) }^{ 2 } }</script><p>优点：</p>
<ol>
<li>$softsign$ 是 $tanh$ 激活函数的另一个替代选择；</li>
<li>$softsign$ 是反对称、去中心、可微分，并返回 $-1$ 和 $1$ 之间的值；</li>
<li>$softsign$ 更平坦的曲线与更慢的下降导数表明它可以更高效地学习；</li>
</ol>
<p>缺点：</p>
<ol>
<li>导数的计算比$tanh$ 更麻烦；</li>
</ol>
<h3 id="10-softplus"><a href="#10-softplus" class="headerlink" title="10. softplus"></a>10. softplus</h3><p>函数定义：</p>
<script type="math/tex; mode=display">f(x)=\ln { (1+{ e }^{ x }) }</script><p>导数：</p>
<script type="math/tex; mode=display">{ f }^{ ' }(x)=\frac { 1 }{ 1+{ e }^{ -x } }</script><p>优点：</p>
<ol>
<li>作为 $relu$ 的一个不错的替代选择，$softplus$ 能够返回任何大于 $0$ 的值。</li>
<li>与 $relu$ 不同，$softplus$ 的导数是连续的、非零的，无处不在，从而防止出现死神经元。</li>
</ol>
<p>缺点：</p>
<ol>
<li>导数常常小于 $1$ ，也可能出现梯度消失的问题。</li>
<li>$softplus$ 另一个不同于 $relu$ 的地方在于其不对称性，不以零为中心，可能会妨碍学习。</li>
</ol>
<h3 id="11-softmax"><a href="#11-softmax" class="headerlink" title="11. softmax"></a>11. softmax</h3><p>softmax 函数一般用于多分类问题中，它是对逻辑斯蒂（logistic）回归的一种推广，也被称为多项逻辑斯蒂回归模型(multi-nominal logistic mode)。假设要实现 k 个类别的分类任务，Softmax 函数将输入数据 $x_i$ 映射到第 $i$ 个类别的概率 $y_i$ 如下计算：</p>
<script type="math/tex; mode=display">
y_i=soft\max \left( x_i \right) =\frac{e^{x_i}}{\sum_{j=1}^k{e^{x_j}}}</script><h3 id="12-swish"><a href="#12-swish" class="headerlink" title="12. swish"></a>12. swish</h3><p>函数定义：</p>
<script type="math/tex; mode=display">f\left( x \right) =x\cdot \sigma \left( x \right)</script><p>其中，$\sigma$ 是 $sigmoid$ 函数。</p>
<p>$swish$ 激活函数的一阶导数如下：</p>
<script type="math/tex; mode=display">\begin{array}{c}
    f^{'}\left( x \right) =\sigma \left( x \right) +x\cdot \sigma \left( x \right) \left( 1-\sigma \left( x \right) \right)\\
    =\sigma \left( x \right) +x\cdot \sigma \left( x \right) -x\cdot \sigma \left( x \right) ^2\\
    =x\cdot \sigma \left( x \right) +\sigma \left( x \right) \left( 1-x\cdot \sigma \left( x \right) \right)\\
    =f\left( x \right) +\sigma \left( x \right) \left( 1-f\left( x \right) \right)\\
\end{array}</script><p>超参数版 $swish$ 激活函数：</p>
<script type="math/tex; mode=display">f\left( x \right) =x\cdot \sigma \left( \beta x \right)</script><p>其中，$\beta$ 是超参数。超参数版 $swish$ 激活函数的图形如 <strong>图16</strong> 所示：</p>
<p>优点：</p>
<ol>
<li>当 $x&gt;0$ 时，不存在梯度消失的情况；当 $x&lt;0$ 时，神经元也不会像 ReLU 一样出现死亡的情况；</li>
<li>$swish$ 处处可导，连续光滑；</li>
<li>$swish$ 并非一个单调的函数；</li>
<li>提升了模型的性能；</li>
</ol>
<p>缺点：</p>
<ol>
<li>计算量大；</li>
</ol>
<h3 id="13-hswish"><a href="#13-hswish" class="headerlink" title="13. hswish"></a>13. hswish</h3><p>函数定义：</p>
<script type="math/tex; mode=display">f\left( x \right) =x\frac{\text{Re}LU6\left( x+3 \right)}{6}</script><p>优点：<br>与 $swish$ 相比 $hard \ swish$ 减少了计算量，具有和 $swish$ 同样的性质。</p>
<p>缺点：<br>与 $relu6$ 相比 $hard \ swish$ 的计算量仍然较大。</p>
<h2 id="2-激活函数的选择"><a href="#2-激活函数的选择" class="headerlink" title="2. 激活函数的选择"></a>2. 激活函数的选择</h2><ol>
<li>浅层网络在分类器时，$sigmoid$ 函数及其组合通常效果更好。</li>
<li>由于梯度消失问题，有时要避免使用 $sigmoid$ 和 $tanh$ 函数。</li>
<li>$relu$ 函数是一个通用的激活函数，目前在大多数情况下使用。</li>
<li>如果神经网络中出现死神经元，那么 $prelu$ 函数就是最好的选择。</li>
<li>$relu$ 函数只能在隐藏层中使用。</li>
<li>通常，可以从 $relu$ 函数开始，如果 $relu$ 函数没有提供最优结果，再尝试其他激活函数。</li>
</ol>
<h2 id="3-激活函数相关问题"><a href="#3-激活函数相关问题" class="headerlink" title="3. 激活函数相关问题"></a>3. 激活函数相关问题</h2><blockquote>
<p>为什么 $relu$ 不是全程可微/可导也能用于基于梯度的学习？</p>
</blockquote>
<p>从数学的角度看 $relu$ 在 $0$ 点不可导，因为它的左导数和右导数不相等；但在实现时通常会返回左导数或右导数的其中一个，而不是报告一个导数不存在的错误，从而避免了这个问题。</p>
<blockquote>
<p>为什么 $tanh$ 的收敛速度比 $sigmoid$ 快？</p>
</blockquote>
<script type="math/tex; mode=display">\tan\text{h}^{'}\left( x \right) =1-\tan\text{h}\left( x \right) ^2\in \left( 0,1 \right)</script><script type="math/tex; mode=display">s^{'}\left( x \right) =s\left( x \right) \left( 1-s\left( x \right) \right) \in \left( 0,\frac{1}{4} \right]</script><p>由上面两个公式可知 $tanh$ 引起的梯度消失问题没有 $sigmoid$ 严重，所以 $tanh$ 收敛速度比 $sigmoid$ 快。</p>
<blockquote>
<p>sigmoid 和 softmax 有什么区别？</p>
</blockquote>
<ol>
<li>二分类问题时 $sigmoid$ 和 $softmax$ 是一样的，都是求 $cross \ entropy \ loss$ ，而 $softmax$ 可以用于多分类问题。</li>
<li>$softmax$ 是 $sigmoid$ 的扩展，因为，当类别数 $k=2$ 时，$softmax$ 回归退化为 $logistic$ 回归。</li>
<li>$softmax$ 建模使用的分布是多项式分布，而 $logistic$ 则基于伯努利分布。</li>
<li>多个 $logistic$ 回归通过叠加也同样可以实现多分类的效果，但是 $softmax$ 回归进行的多分类，类与类之间是互斥的，即一个输入只能被归为一类；多 $logistic$ 回归进行多分类，输出的类别并不是互斥的。</li>
</ol>
]]></content>
      <categories>
        <category>深度学习基础</category>
      </categories>
      <tags>
        <tag>激活函数</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title>AlexNet</title>
    <url>/2021/04/17/AlexNet/</url>
    <content><![CDATA[<h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a><a href="https://www.nvidia.cn/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf">AlexNet</a></h1><h2 id="1-模型介绍"><a href="#1-模型介绍" class="headerlink" title="1. 模型介绍"></a>1. 模型介绍</h2><p>AlexNet是2012年ImageNet竞赛的冠军模型，其作者是神经网络领域三巨头之一的Hinton和他的学生Alex Krizhevsky。</p>
<p>AlexNet以极大的优势领先2012年ImageNet竞赛的第二名，也因此给当时的学术界和工业界带来了很大的冲击。此后，更多更深的神经网络相继被提出，比如优秀的VGG，GoogLeNet，ResNet等。</p>
<span id="more"></span>
<h2 id="2-模型结构"><a href="#2-模型结构" class="headerlink" title="2. 模型结构"></a>2. 模型结构</h2><p><img src="https://pic.imgdb.cn/item/6310606c16f2c2beb146a293.png" alt=""></p>
<ul>
<li>第一模块：对于$224×224$的彩色图像，先用96个$11×11×3$的卷积核对其进行卷积，然后以$2×2$大小进行池化，得到了96个$27×27$大小的特征图</li>
<li>第二模块：包含256个$5×5$的卷积和$2×2$池化，卷积操作后图像尺寸不变，经过池化后，图像尺寸变成$13×13$</li>
<li>第三模块：包含384个$3×3$的卷积，卷积操作后图像尺寸不变</li>
<li>第四模块：包含384个$3×3$的卷积，卷积操作后图像尺寸不变</li>
<li>第五模块：包含256个$3×3$的卷积和$2×2$的池化，卷积操作后图像尺寸不变，经过池化后变成256个$6×6$大小的特征图</li>
</ul>
<p>将经过第5次卷积提取到的特征图输入到全连接层，得到原始图像的向量表达。前两个全连接层的输出神经元的个数是4096，第三个全连接层的输出神经元个数是分类标签的类别数（ImageNet比赛的分类类别数是1000），然后使用Softmax激活函数即可计算出每个类别的预测概率。</p>
<h2 id="3-模型实现"><a href="#3-模型实现" class="headerlink" title="3. 模型实现"></a>3. 模型实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span>, init_weight=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(AlexNet, self).__init__()</span><br><span class="line">        self.feature_extraction = nn.Sequential(</span><br><span class="line">            <span class="comment"># [3, 224, 224] -&gt; [96, 55, 55]</span></span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">96</span>, kernel_size=(<span class="number">11</span>, <span class="number">11</span>), stride=(<span class="number">4</span>, <span class="number">4</span>), padding=(<span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># [96, 55, 55] -&gt; [96, 27, 27]</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line"></span><br><span class="line">            <span class="comment"># [96, 27, 27] -&gt; [256, 27, 27] same padding</span></span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">96</span>, out_channels=<span class="number">256</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), padding=(<span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># [256, 27, 27] -&gt; [256, 13, 13]</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line"></span><br><span class="line">            <span class="comment"># [256, 13, 13] -&gt; [384, 13, 13] same padding</span></span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">256</span>, out_channels=<span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            <span class="comment"># [384, 13, 13] -&gt; [384, 13, 13] same padding</span></span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">384</span>, out_channels=<span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            <span class="comment"># [384, 13, 13] -&gt; [256, 13, 13] same padding</span></span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">384</span>, out_channels=<span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># [256, 13, 13] -&gt; [256, 6, 6]</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">        )</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(in_features=<span class="number">256</span>*<span class="number">6</span>*<span class="number">6</span>, out_features=<span class="number">4096</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">4096</span>),</span><br><span class="line">            nn.Linear(in_features=<span class="number">4096</span>, out_features=num_classes),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.feature_extraction(x)</span><br><span class="line">        x = torch.flatten(x, start_dim=<span class="number">1</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_initialize_weight</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                nn.init.normal_(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="4-模型特点"><a href="#4-模型特点" class="headerlink" title="4. 模型特点"></a>4. 模型特点</h2><p>AlexNet中包含了几个比较新的技术点，也首次在CNN中成功应用了ReLU、Dropout和LRN等Trick。同时AlexNet也使用了GPU进行运算加速。</p>
<ol>
<li>成功使用<strong>ReLU</strong>作为CNN的激活函数，并验证其效果在较深的网络超过了Sigmoid，成功解决了Sigmoid在网络较深时的梯度弥散问题。</li>
<li>训练时使用<strong>Dropout</strong>随机忽略一部分神经元，以避免模型过拟合。</li>
<li>在CNN中使用重叠的<strong>最大池化</strong>。此前CNN中普遍使用平均池化，AlexNet全部使用最大池化，避免平均池化的模糊化效果。</li>
<li>提出了<strong>LRN局部响应归一化层</strong>，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。</li>
<li>使用<strong>CUDA</strong>加速深度卷积网络的训练，利用GPU强大的并行计算能力，处理神经网络训练时大量的矩阵运算。</li>
<li>使用<strong>数据增强</strong>，随机地从$256×256$大小的原始图像中截取$224×224$大小的区域（以及水平翻转的镜像），相当于增加了$2×(256−224)^2=2048$倍的数据量。如果没有数据增强，仅靠原始的数据量，参数众多的CNN会陷入过拟合中，使用了数据增强后可以大大减轻过拟合，提升泛化能力。</li>
</ol>
]]></content>
      <categories>
        <category>图像分类</category>
      </categories>
      <tags>
        <tag>模型</tag>
        <tag>backbone</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepLab</title>
    <url>/2022/06/17/DeepLab/</url>
    <content><![CDATA[<h1 id="DeepLab系列"><a href="#DeepLab系列" class="headerlink" title="DeepLab系列"></a>DeepLab系列</h1><blockquote>
<p>本文介绍了DeepLabV1、DeepLabV2、DeepLabV3和DeepLabV3+四种模型，并使用Pytorch进行了实现。<br><span id="more"></span></p>
</blockquote>
<h2 id="1-DeepLabV1"><a href="#1-DeepLabV1" class="headerlink" title="1. DeepLabV1"></a>1. DeepLabV1</h2><h3 id="1-1-语义分割中存在的问题"><a href="#1-1-语义分割中存在的问题" class="headerlink" title="1.1 语义分割中存在的问题"></a>1.1 语义分割中存在的问题</h3><p><strong>1. 信号下采样导致分辨率降低</strong></p>
<ul>
<li>造成原因：采用<code>MaxPooling</code>造成的</li>
<li>解决办法：引入<code>atrous Conv</code>空洞卷积</li>
</ul>
<p><strong>2. CNN的空间不变性</strong></p>
<ul>
<li>造成原因：重复的池化和下采样</li>
<li>解决办法：使用<code>DenseCRF</code></li>
</ul>
<h3 id="1-2-空洞卷积"><a href="#1-2-空洞卷积" class="headerlink" title="1.2 空洞卷积"></a>1.2 空洞卷积</h3><p>空洞卷积通过引入扩张率<code>Dilation Rate</code>这一参数使得同样尺寸的卷积核获得更大的感受野</p>
<p><strong>1. 扩张率为1的3*3卷积，其与标准卷积一样</strong><br><img src="https://img-blog.csdnimg.cn/31476414ebdc41c68c896051c1c83e18.png#pic_center" alt="在这里插入图片描述"></p>
<p><strong>2. 扩张率为2的3*3卷积</strong><br><img src="https://img-blog.csdnimg.cn/fbe8fc72eca34ab6ab9a4b205c2b2d8a.png#pic_center" alt="在这里插入图片描述"></p>
<p><strong>3. 扩张率为4的3*3卷积</strong><br><img src="https://img-blog.csdnimg.cn/e03b1634326e46b39c5d68231e612ec4.png#pic_center" alt="在这里插入图片描述"></p>
<h3 id="1-3-网络结构"><a href="#1-3-网络结构" class="headerlink" title="1.3 网络结构"></a>1.3 网络结构</h3><p><strong>使用VGG16模型作为backbone</strong></p>
<ul>
<li>将VGG16中的全连接层转化为卷积层</li>
<li>将VGG16中的<code>MaxPooling</code>层由<code>kernel=2, stride=2</code>转化为<code>kernel=3, stride=2, padding=1</code>，并且最后两个<code>MaxPooling</code>层的<code>stride</code>全部设置为<code>1</code></li>
<li>将VGG16中的最后三个卷积层修改为空洞卷积，扩张率为2；并且第一个全连接层卷积化也修改为空洞卷积，在论文中<code>LargeFOV</code>中的设置为<code>12</code></li>
</ul>
<p><strong>Pytorch实现</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># File       : backbone.py</span></span><br><span class="line"><span class="string"># Author     ：CodeCat</span></span><br><span class="line"><span class="string"># version    ：python 3.7</span></span><br><span class="line"><span class="string"># Software   ：Pycharm</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv3x3</span>(<span class="params">in_channels, out_channels, stride=<span class="number">1</span>, dilation=<span class="number">1</span>, padding=<span class="number">1</span></span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, stride=stride, padding=padding, dilation=dilation),</span><br><span class="line">        nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv1x1</span>(<span class="params">in_channels, out_channels, stride=<span class="number">1</span>, dilation=<span class="number">1</span>, padding=<span class="number">1</span></span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>, stride=stride, padding=padding, dilation=dilation),</span><br><span class="line">        nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, num_depth=<span class="number">2</span>, padding=<span class="number">1</span>, dilation=<span class="number">1</span>, pool_stride=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Block, self).__init__()</span><br><span class="line">        self.num_depth = num_depth</span><br><span class="line">        self.conv1 = conv3x3(in_channels, out_channels, padding=padding, dilation=dilation)</span><br><span class="line">        self.conv2 = conv3x3(out_channels, out_channels, padding=padding, dilation=dilation)</span><br><span class="line">        <span class="keyword">if</span> self.num_depth == <span class="number">3</span>:</span><br><span class="line">            self.conv3 = conv3x3(out_channels, out_channels, padding=padding, dilation=dilation)</span><br><span class="line">        self.pool = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=pool_stride, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="keyword">if</span> self.num_depth == <span class="number">3</span>:</span><br><span class="line">            x = self.conv3(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DeepLabV1</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">21</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(DeepLabV1, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.block1 = Block(in_channels=<span class="number">3</span>, out_channels=<span class="number">64</span>)</span><br><span class="line">        self.block2 = Block(in_channels=<span class="number">64</span>, out_channels=<span class="number">128</span>)</span><br><span class="line">        self.block3 = Block(in_channels=<span class="number">128</span>, out_channels=<span class="number">256</span>, num_depth=<span class="number">3</span>)</span><br><span class="line">        self.block4 = Block(in_channels=<span class="number">256</span>, out_channels=<span class="number">512</span>, num_depth=<span class="number">3</span>, pool_stride=<span class="number">1</span>)</span><br><span class="line">        self.block5 = Block(in_channels=<span class="number">512</span>, out_channels=<span class="number">512</span>, num_depth=<span class="number">3</span>, pool_stride=<span class="number">1</span>, dilation=<span class="number">2</span>, padding=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.avg_pool = nn.AvgPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.conv6 = conv3x3(in_channels=<span class="number">512</span>, out_channels=<span class="number">1024</span>, padding=<span class="number">12</span>, dilation=<span class="number">12</span>)</span><br><span class="line">        self.drop6 = nn.Dropout2d(p=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        self.conv7 = conv1x1(in_channels=<span class="number">1024</span>, out_channels=<span class="number">1024</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.drop7 = nn.Dropout2d(p=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        self.conv8 = conv1x1(in_channels=<span class="number">1024</span>, out_channels=num_classes, padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        input_size = x.shape[-<span class="number">2</span>:]</span><br><span class="line">        <span class="comment"># (b, 3, 224, 224) -&gt; (b, 64, 112, 112)</span></span><br><span class="line">        x = self.block1(x)</span><br><span class="line">        <span class="comment"># (b, 64, 112, 112) -&gt; (b, 128, 56, 56)</span></span><br><span class="line">        x = self.block2(x)</span><br><span class="line">        <span class="comment"># (b, 128, 56, 56) -&gt; (b, 256, 28, 28)</span></span><br><span class="line">        x = self.block3(x)</span><br><span class="line">        <span class="comment"># (b, 256, 28, 28) -&gt; (b, 512, 28, 28)</span></span><br><span class="line">        x = self.block4(x)</span><br><span class="line">        <span class="comment"># (b, 512, 28, 28) -&gt; (b, 512, 28, 28)</span></span><br><span class="line">        x = self.block5(x)</span><br><span class="line">        <span class="comment"># (b, 512, 28, 28) -&gt; (b, 512, 28, 28)</span></span><br><span class="line">        x = self.avg_pool(x)</span><br><span class="line">        <span class="comment"># (b, 512, 28, 28) -&gt; (b, 1024, 28, 28)</span></span><br><span class="line">        x = self.conv6(x)</span><br><span class="line">        x = self.drop6(x)</span><br><span class="line">        <span class="comment"># (b, 1024, 28, 28) -&gt; (b, 1024, 28, 28)</span></span><br><span class="line">        x = self.conv7(x)</span><br><span class="line">        x = self.drop7(x)</span><br><span class="line">        <span class="comment"># (b, 1024, 28, 28) -&gt; (b, num_classes, 28, 28)</span></span><br><span class="line">        x = self.conv8(x)</span><br><span class="line">        <span class="comment"># (b, num_classes, 28, 28) -&gt; (b, num_classes, 224, 224)</span></span><br><span class="line">        x = F.interpolate(x, size=input_size, mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = DeepLabV1()</span><br><span class="line">    inputs = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    <span class="built_in">print</span>(outputs.shape)</span><br></pre></td></tr></table></figure></p>
<h2 id="2-DeepLabV2"><a href="#2-DeepLabV2" class="headerlink" title="2. DeepLabV2"></a>2. DeepLabV2</h2><h3 id="2-1-语义分割中存在的问题"><a href="#2-1-语义分割中存在的问题" class="headerlink" title="2.1 语义分割中存在的问题"></a>2.1 语义分割中存在的问题</h3><ol>
<li>分辨率被降低导致特征层丢失细节信息：主要由于下采样<code>stride&gt;1</code>的层造成的</li>
<li>目标的多尺度问题</li>
<li>DCNNs的不变性会降低定位精度</li>
</ol>
<h3 id="2-1-论文中的解决办法"><a href="#2-1-论文中的解决办法" class="headerlink" title="2.1 论文中的解决办法"></a>2.1 论文中的解决办法</h3><ol>
<li>针对问题1，一般是将最后几个<code>MaxPooling</code>层的<code>stride</code>设置为1（若是通过卷积进行下采样，也是将其<code>stride</code>设置为1），然后再配合空洞卷积</li>
<li>针对问题2，本文提出了<code>ASPP</code>模块</li>
<li>针对问题3，和DeepLabV1一样采用<code>DenseCRF</code></li>
</ol>
<h3 id="2-3-ASPP模块"><a href="#2-3-ASPP模块" class="headerlink" title="2.3 ASPP模块"></a>2.3 ASPP模块</h3><p><strong>并行采用多个采样率的空洞卷积提取特征，再将特征进行融合，该结构称为空洞空间金字塔池化</strong><br><img src="https://img-blog.csdnimg.cn/c381fd7f04344a5da13c03ef66598439.png#pic_center" alt="在这里插入图片描述"></p>
<h3 id="2-4-网络结构"><a href="#2-4-网络结构" class="headerlink" title="2.4 网络结构"></a>2.4 网络结构</h3><p><strong>与DeepLabV1使用VGG16作为backbone不同的是，DeepLabV2使用ResNet101作为backbone，做出的修改如下：</strong></p>
<ol>
<li>将<code>Layer3</code>中的<code>Bottleneck1</code>中原本进行下采样的<code>3x3</code>卷积层(<code>stride=2</code>)的<code>stride</code>设置为<code>1</code>，即不进行下采样，并且<code>3x3</code>卷积层全部采用扩张率为<code>2</code>的空洞卷积</li>
<li>将<code>Layer4</code>中的<code>Bottleneck1</code>中原本进行下采样的<code>3x3</code>卷积层(<code>stride=2</code>)的<code>stride</code>设置为<code>1</code>，即不进行下采样，并且<code>3x3</code>卷积层全部采用扩张率为<code>4</code>的空洞卷积</li>
<li><code>ASPP</code>模块中的每一个分支只有一个<code>3x3</code>的空洞卷积层，并且卷积核的个数等于<code>num_classes</code></li>
</ol>
<p><strong>Pytorch实现</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*-coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># File       : backbone.py</span></span><br><span class="line"><span class="string"># Author     ：CodeCat</span></span><br><span class="line"><span class="string"># version    ：python 3.7</span></span><br><span class="line"><span class="string"># Software   ：Pycharm</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(nn.Module):</span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, stride=<span class="number">1</span>, dilation=<span class="number">1</span>, downsample=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Bottleneck, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>, stride=stride, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(out_channels)</span><br><span class="line"></span><br><span class="line">        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=dilation, dilation=dilation, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_channels)</span><br><span class="line"></span><br><span class="line">        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)</span><br><span class="line"></span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        identity = x</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out = self.relu(self.bn1(self.conv1(x)))</span><br><span class="line"></span><br><span class="line">        out = self.relu(self.bn2(self.conv2(out)))</span><br><span class="line"></span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.bn3(out)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ASPP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, atrous_rates</span>):</span><br><span class="line">        <span class="built_in">super</span>(ASPP, self).__init__()</span><br><span class="line">        <span class="keyword">for</span> i, rate <span class="keyword">in</span> <span class="built_in">enumerate</span>(atrous_rates):</span><br><span class="line">            self.add_module(<span class="string">&#x27;c%d&#x27;</span>%i, nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=rate, dilation=rate, bias=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>([stage(x) <span class="keyword">for</span> stage <span class="keyword">in</span> self.children()])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DeepLabV2</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes, block_nums, atrous_rates</span>):</span><br><span class="line">        <span class="built_in">super</span>(DeepLabV2, self).__init__()</span><br><span class="line">        self.in_channels = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, self.in_channels, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(self.in_channels)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.maxpool = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.layer1 = self._make_layer(<span class="number">64</span>, block_nums[<span class="number">0</span>])</span><br><span class="line">        self.layer2 = self._make_layer(<span class="number">128</span>, block_nums[<span class="number">1</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self._make_layer(<span class="number">256</span>, block_nums[<span class="number">2</span>], dilation=<span class="number">2</span>)</span><br><span class="line">        self.layer4 = self._make_layer(<span class="number">512</span>, block_nums[<span class="number">3</span>], dilation=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        self.aspp = ASPP(in_channels=<span class="number">512</span> * Bottleneck.expansion, out_channels=num_classes, atrous_rates=atrous_rates)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        input_size = x.shape[-<span class="number">2</span>:]</span><br><span class="line">        <span class="comment"># (b, 3, 224, 224) -&gt; (b, 64, 112, 112)</span></span><br><span class="line">        x = self.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        <span class="comment"># (b, 64, 112, 112) -&gt; (b, 64, 56, 56)</span></span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (b, 64, 56, 56) -&gt; (b, 256, 56, 56)</span></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        <span class="comment"># (b, 256, 56, 56) -&gt; (b, 512, 28, 28)</span></span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        <span class="comment"># (b, 512, 28, 28) -&gt; (b, 1024, 28, 28)</span></span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        <span class="comment"># (b, 1024, 28, 28) -&gt; (b, 2048, 28, 28)</span></span><br><span class="line">        x = self.layer4(x)</span><br><span class="line">        <span class="comment"># (b, 2048, 28, 28) -&gt; (b, num_classes, 28, 28)</span></span><br><span class="line">        x = self.aspp(x)</span><br><span class="line">        <span class="comment"># (b, num_classes, 28, 28) -&gt; (b, num_classes, 224, 224)</span></span><br><span class="line">        x = F.interpolate(x, size=input_size, mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, channels, block_num, stride=<span class="number">1</span>, dilation=<span class="number">1</span></span>):</span><br><span class="line">        downsample = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.in_channels != channels * Bottleneck.expansion:</span><br><span class="line">            downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(self.in_channels, channels * Bottleneck.expansion, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(channels * Bottleneck.expansion)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        layers = <span class="built_in">list</span>()</span><br><span class="line"></span><br><span class="line">        layers.append(Bottleneck(self.in_channels, channels, downsample=downsample, stride=stride, dilation=dilation))</span><br><span class="line">        self.in_channels = channels * Bottleneck.expansion</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, block_num):</span><br><span class="line">            layers.append(Bottleneck(self.in_channels, channels, dilation=dilation))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = DeepLabV2(</span><br><span class="line">        num_classes=<span class="number">21</span>,</span><br><span class="line">        block_nums=[<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>],</span><br><span class="line">        atrous_rates=[<span class="number">6</span>, <span class="number">12</span>, <span class="number">18</span>, <span class="number">24</span>]</span><br><span class="line">    )</span><br><span class="line">    inputs = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    <span class="built_in">print</span>(outputs.shape)</span><br></pre></td></tr></table></figure></p>
<h2 id="3-DeepLabV3"><a href="#3-DeepLabV3" class="headerlink" title="3. DeepLabV3"></a>3. DeepLabV3</h2><p><strong>改进了ASPP模块</strong></p>
<h3 id="3-1-ASPP模块"><a href="#3-1-ASPP模块" class="headerlink" title="3.1 ASPP模块"></a>3.1 ASPP模块</h3><p><img src="https://img-blog.csdnimg.cn/img_convert/3adb3fe220965bd35443645f8d50fafb.png" alt=""></p>
<p><strong>ASPP模块</strong></p>
<ul>
<li>一个<code>1x1</code>卷积</li>
<li>三个<code>3x3</code>空洞卷积，当<code>output_stride=16</code>时，<code>rates=(6, 12, 18)</code>；当<code>output_stride=8</code>时，扩张率翻倍，即<code>rates=(12, 24, 36)</code></li>
<li>图像级特征：将特征做全局平均池化，后<code>1x1</code>卷积，再上采样</li>
<li>每个卷积层后面会加入<code>BN</code>层</li>
</ul>
<p><strong><code>output_stride</code>说明</strong></p>
<ul>
<li>表示输入图像大小与输出特征图大小的比值</li>
<li>在图像分类任务中，最终的输出特征图比输入图像大小小32倍，即<code>output_stride=32</code></li>
<li>在语义分割任务中，<code>output_stride</code>一般为<code>8</code>或<code>16</code>，通常通过改变最后1个或2个block，使其不再缩小特征图的大小，并应用空洞卷积来密集提取特征<br><strong>Pytorch实现</strong><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ASPP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, rate=<span class="number">1</span>, bn_momentum=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ASPP, self).__init__()</span><br><span class="line">        self.branch1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels, momentum=bn_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        self.branch2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=<span class="number">1</span>, padding=<span class="number">6</span>*rate, dilation=<span class="number">6</span>*rate),</span><br><span class="line">            nn.BatchNorm2d(out_channels, momentum=bn_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        self.branch3 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=<span class="number">1</span>, padding=<span class="number">12</span>*rate, dilation=<span class="number">12</span>*rate),</span><br><span class="line">            nn.BatchNorm2d(out_channels, momentum=bn_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        self.branch4 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=<span class="number">1</span>, padding=<span class="number">18</span>*rate, dilation=<span class="number">18</span>*rate),</span><br><span class="line">            nn.BatchNorm2d(out_channels, momentum=bn_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        self.branch5 = nn.Sequential(</span><br><span class="line">            nn.AdaptiveAvgPool2d(output_size=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels, momentum=bn_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.conv_cat = nn.Sequential(</span><br><span class="line">            nn.Conv2d(out_channels*<span class="number">5</span>, out_channels, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=<span class="number">1</span>, padding=<span class="number">0</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels, momentum=bn_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        b, c, h, w = x.size()</span><br><span class="line"></span><br><span class="line">        conv1x1 = self.branch1(x)</span><br><span class="line">        conv3x3_1 = self.branch2(x)</span><br><span class="line">        conv3x3_2 = self.branch3(x)</span><br><span class="line">        conv3x3_3 = self.branch4(x)</span><br><span class="line"></span><br><span class="line">        global_feature = self.branch5(x)</span><br><span class="line">        global_feature = F.interpolate(global_feature, size=(h, w), mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        feature_cat = torch.cat([conv1x1, conv3x3_1, conv3x3_2, conv3x3_3, global_feature], dim=<span class="number">1</span>)</span><br><span class="line">        result = self.conv_cat(feature_cat)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = ASPP(in_channels=<span class="number">2048</span>, out_channels=<span class="number">2048</span>)</span><br><span class="line">    inputs = torch.randn(<span class="number">2</span>, <span class="number">2048</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    <span class="built_in">print</span>(outputs.shape)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="4-DeepLabV3"><a href="#4-DeepLabV3" class="headerlink" title="4. DeepLabV3+"></a>4. DeepLabV3+</h2><h3 id="4-1-简介"><a href="#4-1-简介" class="headerlink" title="4.1 简介"></a>4.1 简介</h3><p><strong>DeepLabV3+不仅采用了ASPP结构，而且还采用了编码器-解码器结构</strong></p>
<h3 id="4-2-网络结构"><a href="#4-2-网络结构" class="headerlink" title="4.2 网络结构"></a>4.2 网络结构</h3><p><img src="https://img-blog.csdnimg.cn/img_convert/68e93e65300bce0b7042a77ddcb20e90.png" alt=""><br>网络的<strong>编码器</strong>结构与<code>DeepLabV3</code>一样。<br><strong>解码器</strong>与<code>DeepLabV3</code>不一样，<code>DeepLabV3</code>以<code>factor=16</code>直接进行上采样，而<code>DeepLabV3+</code>采用层级解码器，不是一步到位的，而是通过如下步骤：</p>
<ul>
<li>首先将编码器提取的特征图进行<code>factor=4</code>的上采样，然后和尺寸相同的<code>low-level</code>特征进行<code>concat</code>拼接</li>
<li><code>low-level</code>特征首先采用<code>1x1</code>卷积进行降维，这样能够使得编码器提取的特征有一个偏重</li>
<li>最后再采用<code>3x3</code>卷积进一步提取特征，再以<code>factor=4</code>进行上采样</li>
</ul>
<hr>
<p><strong>完整代码：</strong> <a href="https://github.com/codecat0/CV/tree/main/Semantic_Segmentation/DeepLabV3%2B">https://github.com/codecat0/CV/tree/main/Semantic_Segmentation/DeepLabV3+</a></p>
]]></content>
      <categories>
        <category>图像分割</category>
      </categories>
      <tags>
        <tag>模型</tag>
        <tag>图像分割</tag>
      </tags>
  </entry>
  <entry>
    <title>DenseNet</title>
    <url>/2021/05/15/DenseNet/</url>
    <content><![CDATA[<h1 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a><a href="https://arxiv.org/pdf/1608.06993">DenseNet</a></h1><h2 id="1-模型介绍"><a href="#1-模型介绍" class="headerlink" title="1. 模型介绍"></a>1. 模型介绍</h2><p>如果在靠近输入和靠近输出层之间包含更短的连接，那么卷积神经网络可以很大程度上更深，更准确和高效地进行训练。根据这一结果，我们提出了DenseNet（密集卷积网络）: 对于每一层，所有前一层的特征图作为输入，而这一层地特征图用作所有后续层地输入。优势有：缓解了梯度消失问题，加强了特征传播，鼓励特征复用，并很大程度上减小了参数的数量。<br><span id="more"></span><br>为了确保网络中各层之间最大的信息流，我们将所有层直接连接到彼此。为了保持前馈性质，每一层都从所有前面的层获得额外的输入，并将自己的特性映射传递给所有后续层。与resnet相比，我们从来没有在将特性传递到一个层之前通过<strong>累加</strong>来组合它们，相反，我们通过特征<strong>拼接</strong>来组合它们</p>
<h2 id="2-模型结构及实现"><a href="#2-模型结构及实现" class="headerlink" title="2. 模型结构及实现"></a>2. 模型结构及实现</h2><h3 id="2-1-Compare-nets"><a href="#2-1-Compare-nets" class="headerlink" title="2.1 Compare nets"></a>2.1 Compare nets</h3><ul>
<li><strong>Traditional Nets:</strong>   <script type="math/tex">x_l=H_l(x_{l-1})</script></li>
<li><strong>ResNets</strong>：<script type="math/tex">x_l=H_l(x_{l-1}) + x_{l-1}</script></li>
<li><strong>DenseNets</strong>：<script type="math/tex">x_l=H_l([x_0,x_1,\dots,x_{l-1}])</script></li>
</ul>
<p>参数说明：</p>
<ul>
<li>$x_0$：卷积网络得输入的单个图像</li>
<li>此网络有$L$层，每一层实现一个非线性变换$H_l$，其中$l$表示第$l$层</li>
<li>$x_l$：第$l$层的输出</li>
</ul>
<h3 id="2-2-Dense-Block"><a href="#2-2-Dense-Block" class="headerlink" title="2.2 Dense Block"></a>2.2 Dense Block</h3><p><img src="https://pic.imgdb.cn/item/6311ae5916f2c2beb1ecacb9.png" alt=""></p>
<ul>
<li>每一个<code>Dense Layer</code>产生$k$个特征图</li>
<li>每一个<code>Dense Layer</code>的输入是之前<code>Dense Layer</code>输出特征图<strong>拼接</strong>后的结果，即第$l$层<code>Dense Layer</code>输入的特征图的个数为$l_0+k \cdot (l-1)$</li>
<li>每一个<code>Dense Layer</code>包含一个$1 × 1$卷积和一个$3 × 3$卷积，其中$1 × 1$卷积是为了降维，$3 × 3$卷积是为了提取特征</li>
</ul>
<p>代码实现<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">_DenseLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_input_features, growth_rate, bn_size, drop_rate=<span class="number">0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(_DenseLayer, self).__init__()</span><br><span class="line">        self.drop_rate = drop_rate</span><br><span class="line">        self.dense_layer = nn.Sequential(</span><br><span class="line">            nn.BatchNorm2d(num_input_features),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(in_channels=num_input_features, out_channels=bn_size * growth_rate, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(bn_size * growth_rate),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(in_channels=bn_size * growth_rate, out_channels=growth_rate, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">        self.dropout = nn.Dropout(p=self.drop_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = self.dense_layer(x)</span><br><span class="line">        <span class="keyword">if</span> self.drop_rate &gt; <span class="number">0</span>:</span><br><span class="line">            y = self.dropout(y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> torch.cat([x, y], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">_DenseBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_layers, num_input_features, bn_size, growth_rate, drop_rate=<span class="number">0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(_DenseBlock, self).__init__()</span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            layers.append(_DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate))</span><br><span class="line">        self.layers = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.layers(x)</span><br></pre></td></tr></table></figure></p>
<h3 id="2-2-TransitionLayer"><a href="#2-2-TransitionLayer" class="headerlink" title="2.2 TransitionLayer"></a>2.2 TransitionLayer</h3><p><code>TransitionLayer</code>模块包含一个$1 × 1$卷积用于降维，将特征图的通道数缩小一半和一个平均池化用于将特征图的尺寸缩小一半</p>
<p>代码实现<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">_TransitionLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_input_features, num_output_features</span>):</span><br><span class="line">        <span class="built_in">super</span>(_TransitionLayer, self).__init__()</span><br><span class="line">        self.transition_layer = nn.Sequential(</span><br><span class="line">            nn.BatchNorm2d(num_input_features),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(in_channels=num_input_features, out_channels=num_output_features, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.transition_layer(x)</span><br></pre></td></tr></table></figure></p>
<h3 id="2-2-DenseNet"><a href="#2-2-DenseNet" class="headerlink" title="2.2 DenseNet"></a>2.2 DenseNet</h3><p><code>DenseNet</code>的架构如下图所示:<br><img src="https://pic.imgdb.cn/item/6311b23816f2c2beb1ef397c.png" alt=""></p>
<p>代码实现：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">_DenseLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_input_features, growth_rate, bn_size, drop_rate=<span class="number">0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(_DenseLayer, self).__init__()</span><br><span class="line">        self.drop_rate = drop_rate</span><br><span class="line">        self.dense_layer = nn.Sequential(</span><br><span class="line">            nn.BatchNorm2d(num_input_features),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(in_channels=num_input_features, out_channels=bn_size * growth_rate, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(bn_size * growth_rate),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(in_channels=bn_size * growth_rate, out_channels=growth_rate, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">        self.dropout = nn.Dropout(p=self.drop_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = self.dense_layer(x)</span><br><span class="line">        <span class="keyword">if</span> self.drop_rate &gt; <span class="number">0</span>:</span><br><span class="line">            y = self.dropout(y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> torch.cat([x, y], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">_DenseBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_layers, num_input_features, bn_size, growth_rate, drop_rate=<span class="number">0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(_DenseBlock, self).__init__()</span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            layers.append(_DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate))</span><br><span class="line">        self.layers = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.layers(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">_TransitionLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_input_features, num_output_features</span>):</span><br><span class="line">        <span class="built_in">super</span>(_TransitionLayer, self).__init__()</span><br><span class="line">        self.transition_layer = nn.Sequential(</span><br><span class="line">            nn.BatchNorm2d(num_input_features),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(in_channels=num_input_features, out_channels=num_output_features, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.transition_layer(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DenseNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_init_features=<span class="number">64</span>, growth_rate=<span class="number">32</span>, blocks=(<span class="params"><span class="number">6</span>, <span class="number">12</span>, <span class="number">24</span>, <span class="number">16</span></span>), bn_size=<span class="number">4</span>, drop_rate=<span class="number">0</span>, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(DenseNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=num_init_features, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(num_init_features),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        num_features = num_init_features</span><br><span class="line">        self.layer1 = _DenseBlock(num_layers=blocks[<span class="number">0</span>], num_input_features=num_features, growth_rate=growth_rate, bn_size=bn_size, drop_rate=drop_rate)</span><br><span class="line">        num_features = num_features + blocks[<span class="number">0</span>] * growth_rate</span><br><span class="line">        self.transtion1 = _TransitionLayer(num_input_features=num_features, num_output_features=num_features // <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        num_features = num_features // <span class="number">2</span></span><br><span class="line">        self.layer2 = _DenseBlock(num_layers=blocks[<span class="number">1</span>], num_input_features=num_features, growth_rate=growth_rate, bn_size=bn_size, drop_rate=drop_rate)</span><br><span class="line">        num_features = num_features + blocks[<span class="number">1</span>] * growth_rate</span><br><span class="line">        self.transtion2 = _TransitionLayer(num_input_features=num_features, num_output_features=num_features // <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        num_features = num_features // <span class="number">2</span></span><br><span class="line">        self.layer3 = _DenseBlock(num_layers=blocks[<span class="number">2</span>], num_input_features=num_features, growth_rate=growth_rate, bn_size=bn_size, drop_rate=drop_rate)</span><br><span class="line">        num_features = num_features + blocks[<span class="number">2</span>] * growth_rate</span><br><span class="line">        self.transtion3 = _TransitionLayer(num_input_features=num_features, num_output_features=num_features // <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        num_features = num_features // <span class="number">2</span></span><br><span class="line">        self.layer4 = _DenseBlock(num_layers=blocks[<span class="number">3</span>], num_input_features=num_features, growth_rate=growth_rate, bn_size=bn_size, drop_rate=drop_rate)</span><br><span class="line">        num_features = num_features + blocks[<span class="number">3</span>] * growth_rate</span><br><span class="line"></span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.fc = nn.Linear(num_features, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.features(x)</span><br><span class="line"></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.transtion1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.transtion2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        x = self.transtion3(x)</span><br><span class="line">        x = self.layer4(x)</span><br><span class="line"></span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = torch.flatten(x, start_dim=<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p>
<h2 id="3-模型特点"><a href="#3-模型特点" class="headerlink" title="3. 模型特点"></a>3. 模型特点</h2><ol>
<li>改进了信息和梯度的流动，使得易于训练</li>
<li>密集连接具有正则化效益，减少了训练集规模较小的过拟合问题</li>
</ol>
]]></content>
      <categories>
        <category>图像分类</category>
      </categories>
      <tags>
        <tag>模型</tag>
        <tag>backbone</tag>
      </tags>
  </entry>
  <entry>
    <title>FCN</title>
    <url>/2021/12/30/FCN/</url>
    <content><![CDATA[<h1 id="FCN论文详解"><a href="#FCN论文详解" class="headerlink" title="FCN论文详解"></a>FCN论文详解</h1><blockquote>
<p>FCN全卷积网络是图像分割开山之作，其核心思想非常简单，用卷积层代替分类网络中的全连接层。</p>
</blockquote>
<span id="more"></span>
<h2 id="1-将全连接层替换为卷积层"><a href="#1-将全连接层替换为卷积层" class="headerlink" title="1. 将全连接层替换为卷积层"></a>1. 将全连接层替换为卷积层</h2><p>语义分割的目的是<strong>对图像中每一个像素点进行分类</strong>，与普通的分类任务只输出图像某个类别不同，<strong>语义分割任务输出的是与输入图像大小相同的图像，输出图像的每个像素对应输入图像每个像素的类别</strong>，这也就是论文中提到的<code>dense prediction</code>。</p>
<p>FCN全卷积网络是图像分割开山之作，其核心思想非常简单，<strong>用卷积层代替分类网络中的全连接层。</strong><br><img src="https://img-blog.csdnimg.cn/781c58979f904f4a8f9f2fcc1b8fd57e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5L2V5aaC5Y2D5rO3,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<p>用于分类的神经网络由卷积层、池化层和最后连接的全连接层组成，<strong>经过最后的全连接层后，二维的图像信息被映射为具体的一维类别信息进行输出，得到分类标签。</strong></p>
<p>对于语义分割问题，我们需要的不是具体的类别标签，而是一个二维的分割图，<strong>FCN方法丢弃全连接层，并将其换成卷积层，最后输出与原图相同大小的分割图</strong></p>
<p><strong>论文作者认为：全连接层让目标的位置信息消失了，只保留了语义信息，而将全连接层更换为卷积层可以同时保留位置信息和语义信息</strong><br><img src="https://img-blog.csdnimg.cn/21e775adbf0d437887fe31716099c2f4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5L2V5aaC5Y2D5rO3,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<h2 id="2-上采样"><a href="#2-上采样" class="headerlink" title="2. 上采样"></a>2. 上采样</h2><p>由于经过<strong>多次卷积之后图像的大小会缩小</strong>，需要通过<strong>上采样</strong>对其进行尺寸大小的恢复，<strong>使最后的分割图与原图尺寸一样</strong></p>
<h3 id="2-1-反卷积"><a href="#2-1-反卷积" class="headerlink" title="2.1 反卷积"></a>2.1 反卷积</h3><p>下图为<code>stride=1、paddding=0</code>的反卷积的工作过程<br><img src="https://img-blog.csdnimg.cn/c50c7c3bebd142d48d93551748d7fcf2.gif#pic_center" alt="在这里插入图片描述"></p>
<p>下图为<code>stride=2、padding=1</code>的反卷积的工作过程<br><img src="https://img-blog.csdnimg.cn/d241df84e06e4314a0ffe52a15eb4a88.gif#pic_center" alt="在这里插入图片描述"></p>
<p>根据<strong>卷积</strong>的尺寸计算公式</p>
<script type="math/tex; mode=display">o=\frac {i-k+2 \cdot p} {s} + 1</script><p>得<strong>反卷积</strong>的尺寸就算公式为：</p>
<script type="math/tex; mode=display">i = (o-1) \cdot s + k - 2 \cdot p</script><p><strong>Pytorch实现</strong>：<a href="https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html?highlight=nn%20convtranspose2d#torch.nn.ConvTranspose2d">nn.ConvTranspose2d</a></p>
<h3 id="2-2-插值"><a href="#2-2-插值" class="headerlink" title="2.2 插值"></a>2.2 <a href="https://blog.csdn.net/qq_42735631/article/details/117751529">插值</a></h3><h4 id="2-2-1-最近邻插值"><a href="#2-2-1-最近邻插值" class="headerlink" title="2.2.1 最近邻插值"></a>2.2.1 最近邻插值</h4><p><img src="https://img-blog.csdnimg.cn/c3b2878c047a49d599dc626a8b30ae6d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5L2V5aaC5Y2D5rO3,size_12,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<p>计算<strong>P</strong>与<strong>Q11、Q12、Q22、Q21</strong>的<strong>距离</strong>，<strong>可知P与Q11最近，所以P像素点的值与Q11像素点的值一样</strong></p>
<h4 id="2-2-2-双线性插值"><a href="#2-2-2-双线性插值" class="headerlink" title="2.2.2 双线性插值"></a>2.2.2 双线性插值</h4><p><img src="https://img-blog.csdnimg.cn/67e57649e93249488cb3b22d58a265d4.png#pic_center" alt="在这里插入图片描述"></p>
<p>双线性插值就是做两次线性变换，先在<strong>X</strong>轴上做一次线性变换，求出每一行的<strong>R</strong>点</p>
<script type="math/tex; mode=display">R_1=\frac {x_2-x} {x_2-x_1} Q_{11} + \frac {x-x_1} {x_2-x_1}Q_{21} \\ R_2=\frac {x_2-x} {x_2-x_1} Q_{12} + \frac {x-x_1} {x_2-x_1} Q_{22}</script><p>再在<strong>Y</strong>轴上做一次线性变换，求该区域的<strong>P</strong>点</p>
<script type="math/tex; mode=display">P=\frac {y_2-y} {y_2-y_1}R_1 + \frac {y-y_1} {y_2-y_1}R_2</script><h3 id="2-3-UpPooling"><a href="#2-3-UpPooling" class="headerlink" title="2.3 UpPooling"></a>2.3 <a href="https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html?highlight=nn%20unpool#torch.nn.MaxUnpool2d">UpPooling</a></h3><p><img src="https://img-blog.csdnimg.cn/ada957a8efea4989a4c96c3a57b5ef6f.png#pic_center" alt="在这里插入图片描述"></p>
<h3 id="2-4-Upsample"><a href="#2-4-Upsample" class="headerlink" title="2.4 Upsample"></a>2.4 <a href="https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html">Upsample</a></h3><h2 id="3-跳跃结构"><a href="#3-跳跃结构" class="headerlink" title="3. 跳跃结构"></a>3. 跳跃结构</h2><p>如果<strong>直接用全卷积后的层进行上采样的话，得到的结果往往不够精细</strong>，所以本文中采取了跳级结构的方法，<strong>将更靠前的卷积层和经过上采样的层相结合</strong>，如下图所示：<br><img src="https://img-blog.csdnimg.cn/fdd748ead141469b9837c157e3d89fe3.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5L2V5aaC5Y2D5rO3,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<p>采用这种方法，能够<strong>在保留全局特征的前提下，尽可能使得图像的划分更为精细</strong></p>
]]></content>
      <categories>
        <category>图像分割</category>
      </categories>
      <tags>
        <tag>模型</tag>
        <tag>图像分割</tag>
      </tags>
  </entry>
  <entry>
    <title>GoogLeNet</title>
    <url>/2021/05/04/GoogLeNet/</url>
    <content><![CDATA[<h1 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a><a href="https://arxiv.org/abs/1409.4842">GoogLeNet</a></h1><h2 id="1-模型介绍"><a href="#1-模型介绍" class="headerlink" title="1. 模型介绍"></a>1. 模型介绍</h2><p>GoogLeNet是2014年ImageNet比赛的冠军，它的主要特点是网络不仅有深度，还在横向上具有“宽度”。从名字GoogLeNet可以知道这是来自谷歌工程师所设计的网络结构，而名字中GoogLeNet更是致敬了LeNet。GoogLeNet中最核心的部分是其内部子网络结构<code>Inception</code>，该结构灵感来源于NIN(Network In Network)。<br><span id="more"></span></p>
<h2 id="2-模型结构及实现"><a href="#2-模型结构及实现" class="headerlink" title="2. 模型结构及实现"></a>2. 模型结构及实现</h2><h3 id="2-1-Inception"><a href="#2-1-Inception" class="headerlink" title="2.1 Inception"></a>2.1 Inception</h3><p>由于图像信息在空间尺寸上的巨大差异，如何选择合适的卷积核来提取特征就显得比较困难了。空间分布范围更广的图像信息适合用较大的卷积核来提取其特征；而空间分布范围较小的图像信息则适合用较小的卷积核来提取其特征。为了解决这个问题，GoogLeNet提出了一种被称为<code>Inception</code>模块的方案。如下图所示：</p>
<p><img src="https://pic.imgdb.cn/item/63106c3316f2c2beb1502f03.png" alt=""></p>
<ul>
<li>(a)是<code>Inception</code>模块的设计思想，使用3个不同大小的卷积核对输入图片进行卷积操作，并附加最大池化，将这4个操作的输出沿着通道这一维度进行拼接，构成的输出特征图将会包含经过不同大小的卷积核提取出来的特征，从而达到捕捉不同尺度信息的效果。<code>Inception</code>模块采用多通路(multi-path)的设计形式，每个支路使用不同大小的卷积核，最终输出特征图的通道数是每个支路输出通道数的总和，这将会导致输出通道数变得很大，尤其是使用多个<code>Inception</code>模块串联操作的时候，模型参数量会变得非常大。</li>
<li>为了减小参数量，<code>Inception</code>模块使用了(b)中的设计方式，在每个3x3和5x5的卷积层之前，增加1x1的卷积层来控制输出通道数；在最大池化层后面增加1x1卷积层减小输出通道数。</li>
</ul>
<p>代码实现如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BasicConv2d</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicConv2d, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.branch2 = nn.Sequential(</span><br><span class="line">            BasicConv2d(in_channels, ch3x3red, kernel_size=<span class="number">1</span>),</span><br><span class="line">            BasicConv2d(ch3x3red, ch3x3, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.branch3 = nn.Sequential(</span><br><span class="line">            BasicConv2d(in_channels, ch5x5red, kernel_size=<span class="number">1</span>),</span><br><span class="line">            BasicConv2d(ch5x5red, ch5x5, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.branch4 = nn.Sequential(</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            BasicConv2d(in_channels, pool_proj, kernel_size=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch1 = self.branch1(x)</span><br><span class="line">        branch2 = self.branch2(x)</span><br><span class="line">        branch3 = self.branch3(x)</span><br><span class="line">        branch4 = self.branch4(x)</span><br><span class="line"></span><br><span class="line">        outputs = [branch1, branch2, branch3, branch4]</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="2-2-GoogLeNet"><a href="#2-2-GoogLeNet" class="headerlink" title="2.2 GoogLeNet"></a>2.2 GoogLeNet</h3><p><code>GoogLeNet</code>的架构如下图所示，在主体卷积部分中使用5个模块，每个模块之间使用步幅为2的$3 ×3$最大池化层来减小输出高宽<br><img src="https://pic.imgdb.cn/item/63106de616f2c2beb1518569.png" alt=""></p>
<ul>
<li>第一模块使用一个64通道的$7 × 7$卷积层</li>
<li>第二模块使用2个卷积层:首先是64通道的$1 × 1$卷积层，然后是将通道增大3倍的$3 × 3$卷积层</li>
<li>第三模块串联2个<code>Inception</code>块</li>
<li>第四模块串联了5个Inception块</li>
<li>第五模块串联了2 个Inception块</li>
<li>第五模块的后面紧跟输出层，使用全局平均池化层来将每个通道的高和宽变成1，最后接上一个输出个数为标签类别数的全连接层</li>
</ul>
<p><strong>说明</strong>： 在原作者的论文中在<code>Inception(4a)</code>和<code>Inception(4e)</code>添加了图中所示的两个辅助分类器，如下图所示，训练时将三个分类器的损失函数进行加权求和，以缓解梯度消失现象</p>
<p>代码实现如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GoogLeNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span>, aux_logits=<span class="literal">True</span>, init_weights=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(GoogLeNet, self).__init__()</span><br><span class="line">        self.aux_logits = aux_logits</span><br><span class="line"></span><br><span class="line">        self.conv1 = BasicConv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>)</span><br><span class="line">        self.maxpool1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line">        self.conv2 = BasicConv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.conv3 = BasicConv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.maxpool2 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.inception3a = Inception(<span class="number">192</span>, <span class="number">64</span>, <span class="number">96</span>, <span class="number">128</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">        self.inception3b = Inception(<span class="number">256</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">192</span>, <span class="number">32</span>, <span class="number">96</span>, <span class="number">64</span>)</span><br><span class="line">        self.maxpool3 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.inception4a = Inception(<span class="number">480</span>, <span class="number">192</span>, <span class="number">96</span>, <span class="number">208</span>, <span class="number">16</span>, <span class="number">48</span>, <span class="number">64</span>)</span><br><span class="line">        self.inception4b = Inception(<span class="number">512</span>, <span class="number">160</span>, <span class="number">112</span>, <span class="number">224</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        self.inception4c = Inception(<span class="number">512</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        self.inception4d = Inception(<span class="number">512</span>, <span class="number">112</span>, <span class="number">144</span>, <span class="number">288</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        self.inception4e = Inception(<span class="number">528</span>, <span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">        self.maxpool4 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.inception5a = Inception(<span class="number">832</span>, <span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">        self.inception5b = Inception(<span class="number">832</span>, <span class="number">384</span>, <span class="number">192</span>, <span class="number">384</span>, <span class="number">48</span>, <span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.aux_logits:</span><br><span class="line">            self.aux1 = InceptionAux(<span class="number">512</span>, num_classes)</span><br><span class="line">            self.aux2 = InceptionAux(<span class="number">528</span>, num_classes)</span><br><span class="line"></span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.dropout = nn.Dropout(p=<span class="number">0.2</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">1024</span>, num_classes)</span><br><span class="line">        <span class="keyword">if</span> init_weights:</span><br><span class="line">            self._init_weight()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># N x 3 x 224 x 224</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="comment"># N x 64 x 112 x 112</span></span><br><span class="line">        x = self.maxpool1(x)</span><br><span class="line">        <span class="comment"># N x 64 x 56 x 56</span></span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="comment"># N x 64 x 56 x 56</span></span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        <span class="comment"># N x 192 x 56 x 56</span></span><br><span class="line">        x = self.maxpool2(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># N x 192 x 28 x 28</span></span><br><span class="line">        x = self.inception3a(x)</span><br><span class="line">        <span class="comment"># N x 256 x 28 x 28</span></span><br><span class="line">        x = self.inception3b(x)</span><br><span class="line">        <span class="comment"># N x 480 x 28 x 28</span></span><br><span class="line">        x = self.maxpool3(x)</span><br><span class="line">        <span class="comment"># N x 480 x 14 x 14</span></span><br><span class="line">        x = self.inception4a(x)</span><br><span class="line">        <span class="comment"># N x 512 x 14 x 14</span></span><br><span class="line">        <span class="keyword">if</span> self.aux_logits <span class="keyword">and</span> self.training:</span><br><span class="line">            aux1 = self.aux1(x)</span><br><span class="line"></span><br><span class="line">        x = self.inception4b(x)</span><br><span class="line">        <span class="comment"># N x 512 x 14 x 14</span></span><br><span class="line">        x = self.inception4c(x)</span><br><span class="line">        <span class="comment"># N x 512 x 14 x 14</span></span><br><span class="line">        x = self.inception4d(x)</span><br><span class="line">        <span class="comment"># N x 528 x 14 x 14</span></span><br><span class="line">        <span class="keyword">if</span> self.aux_logits <span class="keyword">and</span> self.training:</span><br><span class="line">            aux2 = self.aux2(x)</span><br><span class="line"></span><br><span class="line">        x = self.inception4e(x)</span><br><span class="line">        <span class="comment"># N x 832 x 14 x 14</span></span><br><span class="line">        x = self.maxpool4(x)</span><br><span class="line">        <span class="comment"># N x 832 x 7 x 7</span></span><br><span class="line">        x = self.inception5a(x)</span><br><span class="line">        <span class="comment"># N x 832 x 7 x 7</span></span><br><span class="line">        x = self.inception5b(x)</span><br><span class="line">        <span class="comment"># N x 1024 x 7 x 7</span></span><br><span class="line"></span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        <span class="comment"># N x 1024 x 1 x 1</span></span><br><span class="line">        x = torch.flatten(x, start_dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># N x 1024</span></span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.aux_logits <span class="keyword">and</span> self.training:</span><br><span class="line">            <span class="keyword">return</span> x, aux2, aux1</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_weight</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                nn.init.normal_(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.branch2 = nn.Sequential(</span><br><span class="line">            BasicConv2d(in_channels, ch3x3red, kernel_size=<span class="number">1</span>),</span><br><span class="line">            BasicConv2d(ch3x3red, ch3x3, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.branch3 = nn.Sequential(</span><br><span class="line">            BasicConv2d(in_channels, ch5x5red, kernel_size=<span class="number">1</span>),</span><br><span class="line">            BasicConv2d(ch5x5red, ch5x5, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.branch4 = nn.Sequential(</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            BasicConv2d(in_channels, pool_proj, kernel_size=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch1 = self.branch1(x)</span><br><span class="line">        branch2 = self.branch2(x)</span><br><span class="line">        branch3 = self.branch3(x)</span><br><span class="line">        branch4 = self.branch4(x)</span><br><span class="line"></span><br><span class="line">        outputs = [branch1, branch2, branch3, branch4]</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionAux</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionAux, self).__init__()</span><br><span class="line">        self.averagePool = nn.AdaptiveAvgPool2d((<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">        self.conv = BasicConv2d(in_channels, <span class="number">128</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.aux_classifier = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">128</span> * <span class="number">4</span> * <span class="number">4</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, num_classes)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.averagePool(x)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = torch.flatten(x, start_dim=<span class="number">1</span>)</span><br><span class="line">        x = self.aux_classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicConv2d</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicConv2d, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p>
<h2 id="3-模型特点"><a href="#3-模型特点" class="headerlink" title="3. 模型特点"></a>3. 模型特点</h2><ol>
<li>采用不同大小的卷积核意味着不同大小的感受野，最后通过拼接实现不同尺度特征的融合</li>
<li>之所以卷积核大小采用1、3和5，主要是为了方便对齐。设定卷积步长stride=1之后，只要分别设定pad=0、1、2，那么卷积之后便可以得到相同维度的特征，然后这些特征就可以直接拼接在一起了</li>
<li>网络越到后面，特征越抽象，而且每个特征所涉及的感受野也更大了，因此随着层数的增加，3x3和5x5卷积的比例也要增加。但是，使用5x5的卷积核仍然会带来巨大的计算量。 为此，文章采用1x1卷积核来进行降维。</li>
</ol>
]]></content>
      <categories>
        <category>图像分类</category>
      </categories>
      <tags>
        <tag>模型</tag>
        <tag>backbone</tag>
      </tags>
  </entry>
  <entry>
    <title>GhostNet</title>
    <url>/2021/10/31/GhostNet/</url>
    <content><![CDATA[<h1 id="Ghost-Net"><a href="#Ghost-Net" class="headerlink" title="Ghost Net"></a><a href="https://arxiv.org/pdf/1911.11907">Ghost Net</a></h1><h2 id="1-模型介绍"><a href="#1-模型介绍" class="headerlink" title="1. 模型介绍"></a>1. 模型介绍</h2><p>下图是由ResNet-50中的第一个残差块生成的某些中间特征图的可视化。从图中我们可以看出，这里面有很多特征图是具有高度相似性的（在图中分别用不同的颜色示意），换句话说，就是存在许多的冗余特征图。所以从另一个角度想，我们是不是可以利用一系列的线性变化，以很小的代价生成许多能从原始特征发掘所需信息的“幻影”特征图呢？这个便是整篇文章的核心思想。<br><span id="more"></span><br><img src="https://img-blog.csdnimg.cn/59beb0d06e2b405b8189693edb871e1c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5L2V5aaC5Y2D5rO3,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<h2 id="2-模型结构及实现"><a href="#2-模型结构及实现" class="headerlink" title="2. 模型结构及实现"></a>2. 模型结构及实现</h2><p><img src="https://img-blog.csdnimg.cn/b643dd29226143d3915daeb987d2927d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5L2V5aaC5Y2D5rO3,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<p><strong>Ghost module：</strong></p>
<ol>
<li>先通过<code>conv</code>生成一些特征图</li>
<li>然后对生成的特征图进行<code>cheap</code>操作（Dwise conv）生成冗余特征图</li>
<li>最后将<code>conv</code>生成的特征图与<code>cheap</code>操作生成的特征图进行<code>concat</code>操作</li>
</ol>
<h3 id="2-1-Ghost-Bottlenecks"><a href="#2-1-Ghost-Bottlenecks" class="headerlink" title="2.1 Ghost Bottlenecks"></a>2.1 Ghost Bottlenecks</h3><p><img src="https://img-blog.csdnimg.cn/74b247252f90447cbc1011bc6863b3d5.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5L2V5aaC5Y2D5rO3,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<p>第一层<code>Ghost module</code>充当<strong>扩展层</strong>用于增加通道的数目</p>
<p>第二层<code>Ghost module</code>减少通道的数目来满足<code>shortcut path</code></p>
<h3 id="2-2-网络结构"><a href="#2-2-网络结构" class="headerlink" title="2.2 网络结构"></a>2.2 网络结构</h3><p><img src="https://img-blog.csdnimg.cn/e674e7de20d44360b0d5da8ca51ed067.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5L2V5aaC5Y2D5rO3,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<p>代码实现：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DWConv3x3BNReLU</span>(nn.Sequential):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride, groups</span>):</span><br><span class="line">        <span class="built_in">super</span>(DWConv3x3BNReLU, self).__init__(</span><br><span class="line">            nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, groups=groups, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channel),</span><br><span class="line">            nn.ReLU6(inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SqueezeAndExcite</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, divide=<span class="number">4</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SqueezeAndExcite, self).__init__()</span><br><span class="line">        mid_channel = in_channel // divide</span><br><span class="line">        self.pool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.SEblock = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=in_channel, out_features=mid_channel),</span><br><span class="line">            nn.ReLU6(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(in_features=mid_channel, out_features=out_channel),</span><br><span class="line">            nn.ReLU6(inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        b, c, h, w = x.size()</span><br><span class="line">        out = self.pool(x)</span><br><span class="line">        out = torch.flatten(out, start_dim=<span class="number">1</span>)</span><br><span class="line">        out = self.SEblock(out)</span><br><span class="line">        out = out.view(b, c, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out * x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GhostModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, s=<span class="number">2</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, use_relu=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(GhostModule, self).__init__()</span><br><span class="line">        intrinsic_channel = out_channel // s</span><br><span class="line">        ghost_channel = intrinsic_channel * (s - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.primary_conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=in_channel, out_channels=intrinsic_channel, kernel_size=kernel_size, stride=stride, padding=(kernel_size-<span class="number">1</span>)//<span class="number">2</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(intrinsic_channel),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>) <span class="keyword">if</span> use_relu <span class="keyword">else</span> nn.Sequential()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.cheap_op = DWConv3x3BNReLU(in_channel=intrinsic_channel, out_channel=ghost_channel, stride=stride, groups=intrinsic_channel)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x1 = self.primary_conv(x)</span><br><span class="line">        x2 = self.cheap_op(x1)</span><br><span class="line">        out = torch.cat([x1, x2], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GhostBottleneck</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, mid_channel, out_channel, kernel_size, stride, use_se</span>):</span><br><span class="line">        <span class="built_in">super</span>(GhostBottleneck, self).__init__()</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">        self.bottleneck = nn.Sequential(</span><br><span class="line">            GhostModule(in_channel=in_channel, out_channel=mid_channel, use_relu=<span class="literal">True</span>),</span><br><span class="line">            DWConv3x3BNReLU(in_channel=mid_channel, out_channel=mid_channel, stride=stride, groups=mid_channel) <span class="keyword">if</span> self.stride &gt; <span class="number">1</span> <span class="keyword">else</span> nn.Sequential(),</span><br><span class="line">            SqueezeAndExcite(in_channel=mid_channel, out_channel=mid_channel) <span class="keyword">if</span> use_se <span class="keyword">else</span> nn.Sequential(),</span><br><span class="line">            GhostModule(in_channel=mid_channel, out_channel=out_channel, use_relu=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.stride &gt; <span class="number">1</span>:</span><br><span class="line">            self.shortcut = DWConv3x3BNReLU(in_channel=in_channel, out_channel=out_channel, stride=stride, groups=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.shortcut = nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = self.bottleneck(x)</span><br><span class="line">        residual = self.shortcut(x)</span><br><span class="line">        out += residual</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GhostNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(GhostNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.first_conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">            nn.ReLU6(inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        ghost_model_setting = [</span><br><span class="line">            <span class="comment"># in, mid, out, kernel, stride, use_se</span></span><br><span class="line">            [<span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="literal">False</span>],</span><br><span class="line">            [<span class="number">16</span>, <span class="number">48</span>, <span class="number">24</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="literal">False</span>],</span><br><span class="line">            [<span class="number">24</span>, <span class="number">72</span>, <span class="number">24</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="literal">False</span>],</span><br><span class="line">            [<span class="number">24</span>, <span class="number">72</span>, <span class="number">40</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="literal">True</span>],</span><br><span class="line">            [<span class="number">40</span>, <span class="number">120</span>, <span class="number">40</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="literal">True</span>],</span><br><span class="line">            [<span class="number">40</span>, <span class="number">240</span>, <span class="number">80</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="literal">False</span>],</span><br><span class="line">            [<span class="number">80</span>, <span class="number">200</span>, <span class="number">80</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="literal">False</span>],</span><br><span class="line">            [<span class="number">80</span>, <span class="number">184</span>, <span class="number">80</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="literal">False</span>],</span><br><span class="line">            [<span class="number">80</span>, <span class="number">184</span>, <span class="number">80</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="literal">False</span>],</span><br><span class="line">            [<span class="number">80</span>, <span class="number">480</span>, <span class="number">112</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="literal">True</span>],</span><br><span class="line">            [<span class="number">112</span>, <span class="number">672</span>, <span class="number">112</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="literal">True</span>],</span><br><span class="line">            [<span class="number">112</span>, <span class="number">672</span>, <span class="number">160</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="literal">True</span>],</span><br><span class="line">            [<span class="number">160</span>, <span class="number">960</span>, <span class="number">160</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="literal">False</span>],</span><br><span class="line">            [<span class="number">160</span>, <span class="number">960</span>, <span class="number">160</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="literal">True</span>],</span><br><span class="line">            [<span class="number">160</span>, <span class="number">960</span>, <span class="number">160</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="literal">False</span>],</span><br><span class="line">            [<span class="number">160</span>, <span class="number">960</span>, <span class="number">160</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="literal">True</span>],</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">for</span> in_channel, mid_channel, out_channel, kernel_size, stride, use_se <span class="keyword">in</span> ghost_model_setting:</span><br><span class="line">            layers.append(GhostBottleneck(in_channel=in_channel, mid_channel=mid_channel, out_channel=out_channel, kernel_size=kernel_size, stride=stride, use_se=use_se))</span><br><span class="line"></span><br><span class="line">        self.features = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">        self.last_stage = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">160</span>, out_channels=<span class="number">960</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">960</span>),</span><br><span class="line">            nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">960</span>, out_channels=<span class="number">1280</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU6(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.classifier = nn.Linear(in_features=<span class="number">1280</span>, out_features=num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.first_conv(x)</span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = self.last_stage(x)</span><br><span class="line">        x = torch.flatten(x, start_dim=<span class="number">1</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = GhostNet()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line">out = model(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(out.size())</span><br></pre></td></tr></table></figure></p>
<h2 id="3-模型特点"><a href="#3-模型特点" class="headerlink" title="3. 模型特点"></a>3. 模型特点</h2><ol>
<li>提出了<code>Ghost</code>模块</li>
</ol>
]]></content>
      <categories>
        <category>图像分类</category>
      </categories>
      <tags>
        <tag>模型</tag>
        <tag>backbone</tag>
        <tag>轻量级网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Happy</title>
    <url>/2022/09/01/Happy/</url>
    <content><![CDATA[<blockquote>
<p>好剧分享，快乐一下</p>
</blockquote>
<span id="more"></span>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=https://img1.baidu.com/it/u=3537231667,2941555870&fm=253&fmt=auto&app=138&f=JPEG?w=379&h=500 width = "30%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
    <a href="https://www.bilibili.com/bangumi/play/ep473306?from_spmid=666.19.0.0">机智的监狱生活</a>
    </div>
</center>

<hr>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fimg.huo720.com%2Fposter%2Fimdb%2Ftt9863766_500.jpg&refer=http%3A%2F%2Fimg.huo720.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1664795170&t=2501170f0dd63d11cfa24cb9deee7c99 width = "30%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
    <a href="https://www.kan.cc/view/759.html">囚犯医生</a>
    </div>
</center>

<hr>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fimg2.doubanio.com%2Fview%2Fthing_review%2Fl%2Fpublic%2Fp6419123.jpg&refer=http%3A%2F%2Fimg2.doubanio.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1666097507&t=b17291cc94296abb941f3ae35b6f5d05 width = "30%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
    <a href="https://www.hanjukankan.com/movie/index955.html">模范出租车</a>
    </div>
</center>

<hr>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201609%2F12%2F20160912212832_aJB4R.jpeg&refer=http%3A%2F%2Fb-ssl.duitang.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1666097758&t=6d1d5417d57a63bddb1a24886ed062c5 width = "30%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
    <a href="https://www.hanjukankan.com/movie/index184.html">W-两个世界</a>
    </div>
</center>

<hr>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=https://www.kan.cc/uploads/allimg/202207/37b1c03158542032.jpg width = "30%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
    <a href="https://www.kan.cc/view/3651.html">黑话律师</a>
    </div>
</center>

<hr>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201710%2F09%2F20171009104335_FHQ4N.thumb.700_0.jpeg&refer=http%3A%2F%2Fb-ssl.duitang.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1664793779&t=ad1fce1213d231341b1aa6b132911f43 width = "30%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
    <a href="https://www.kan.cc/view/764.html">当你沉睡时</a>
    </div>
</center>

<hr>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fimg3.doubanio.com%2Fview%2Fthing_review%2Fl%2Fpublic%2Fp1471230.jpg&refer=http%3A%2F%2Fimg3.doubanio.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1664794191&t=b9e22dc8084cab1a821b7e37e02abf97 width = "30%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
    <a href="https://www.kan.cc/view/2572.html">信号</a>
    </div>
</center>

<hr>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=https://b0.bdstatic.com/57856f0249542c0962c2c99da34d5900.jpg@h_1280 width = "30%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
    <a href="https://www.nunuyy2.org/dianshiju/99901.html">现在开始showtime</a>
    </div>
</center>

<hr>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fres.cngoldres.com%2Fupload%2Fnews%2F2017%2F9%2F30%2Faa5d17a14ace1f20a9ce718bdd5feea6.jpg&refer=http%3A%2F%2Fres.cngoldres.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1664794692&t=28f1813eee047ba4e1d2735162468c1c width = "30%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
    <a href="https://www.iqiyi.com/v_19rr8w99fw.html">无证之罪</a>
    </div>
</center>

<hr>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=https://img0.baidu.com/it/u=3186918077,702948581&fm=253&fmt=auto&app=138&f=JPEG?w=407&h=511 width = "30%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
    <a href="https://www.iqiyi.com/v_2g5a5i86730.html">沉默的真相</a>
    </div>
</center>

<hr>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=https://pic.rmb.bdstatic.com/bjh/news/0c78482db6d08fd4954fb76d3217f3b3.jpeg width = "30%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
    <a href="https://www.iqiyi.com/v_suvb5xrr2w.html">警察荣誉</a>
    </div>
</center>

<hr>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fp2.itc.cn%2Fimages01%2F20200617%2F313d50a545294762abd5811899047076.jpeg&refer=http%3A%2F%2Fp2.itc.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1664795464&t=ebf15de6c4180bc33c8b8ba220a9e2c1 width = "30%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
    <a href="https://www.iqiyi.com/v_2ffkws0bgr0.html">隐秘的角落</a>
    </div>
</center>

<hr>
<p>$To$ $Be$ $Continue$</p>
]]></content>
      <categories>
        <category>娱乐</category>
      </categories>
  </entry>
  <entry>
    <title>Optimizer</title>
    <url>/2022/08/30/Optimizer/</url>
    <content><![CDATA[<h1 id="各种优化器总结与比较"><a href="#各种优化器总结与比较" class="headerlink" title="各种优化器总结与比较"></a>各种优化器总结与比较</h1><p>优化器就是在深度学习反向传播过程中，指引损失函数（目标函数）的各个参数往正确的方向更新合适的大小，使得更新后的各个参数让损失函数（目标函数）值不断逼近全局最小。</p>
<span id="more"></span>
<h2 id="1-梯度下降法-Gradient-Descent"><a href="#1-梯度下降法-Gradient-Descent" class="headerlink" title="1. 梯度下降法(Gradient Descent)"></a>1. 梯度下降法(Gradient Descent)</h2><p>依据计算目标函数梯度使用的数据量的不同，有三种梯度下降的变体，即批量梯度下降，随机梯度下降，Mini-batch梯度下降。根据数据量的大小，在参数更新的准确性和执行更新所需时间之间做了一个权衡。</p>
<h3 id="1-1-批量梯度下降"><a href="#1-1-批量梯度下降" class="headerlink" title="1.1 批量梯度下降"></a>1.1 批量梯度下降</h3><p>标准的梯度下降，即批量梯度下降（batch gradient descent,BGD），在整个训练集上计算损失函数关于参数$\theta$的梯度。</p>
<script type="math/tex; mode=display">\theta=\theta-\eta \nabla_{\theta}J(\theta)</script><p>其中$\theta$是模型的参数，$\eta$是学习率，$\nabla_{\theta}J(\theta)$为损失函数对参数$\theta$的导数。由于为了一次参数更新我们需要在整个训练集上计算梯度，导致 BGD 可能会非常慢，而且在训练集太大而不能全部载入内存的时候会很棘手。BGD 也不允许我们在线更新模型参数，即实时增加新的训练样本。</p>
<p>BGD 对于凸误差曲面（convex error surface）保证收敛到全局最优点，而对于非凸曲面（non-convex surface）则是局部最优点。</p>
<h3 id="1-2-随机梯度下降"><a href="#1-2-随机梯度下降" class="headerlink" title="1.2 随机梯度下降"></a>1.2 随机梯度下降</h3><p>随机梯度下降（ stotastic gradient descent, SGD ）则是每次使用一个训练样本$x^{i}$和标签$y^{i}$进行一次参数更新。</p>
<script type="math/tex; mode=display">\theta=\theta -\eta \cdot \nabla_{\theta}J(\theta;x^i;y^i)</script><p>其中$\theta$是模型的参数，$\eta$是学习率，$\nabla_{\theta}J(\theta)$为损失函数对参数$\theta$的导数。BGD 对于大数据集来说执行了很多冗余的计算，因为在每一次参数更新前都要计算很多相似样本的梯度。SGD 通过一次执行一次更新解决了这种冗余。因此通常 SGD 的速度会非常快而且可以被用于在线学习。SGD以高方差的特点进行连续参数更新，导致目标函数严重震荡</p>
<p>BGD 能够收敛到（局部）最优点，然而 SGD 的震荡特点导致其可以跳到新的潜在的可能更好的局部最优点。已经有研究显示当我们慢慢的降低学习率时，SGD 拥有和 BGD 一样的收敛性能，对于非凸和凸曲面几乎同样能够达到局部或者全局最优点。</p>
<h3 id="1-3-Mini-batch梯度下降"><a href="#1-3-Mini-batch梯度下降" class="headerlink" title="1.3 Mini-batch梯度下降"></a>1.3 Mini-batch梯度下降</h3><p>Mini-batch gradient descent（ mini-batch gradient descent, MBGD ）则是在上面两种方法中采取了一个折中的办法：每次从训练集中取出$batch  size$个样本作为一个mini-batch，以此来进行一次参数更新。</p>
<script type="math/tex; mode=display">\theta=\theta -\eta \cdot \nabla_{\theta} J(\theta;x^{(i:i+n);y^{(i:i+n)}})</script><p>其中$\theta$是模型的参数，$\eta$是学习率，$\nabla_{\theta} J(\theta;x^{(i:i+n);y^{(i:i+n)}}$为损失函数对参数$\theta$的导数，n为Mini-bach的大小（batch size）。 batch size越大，批次越少，训练时间会更快一点，但可能造成数据的很大浪费；而batch size越小，对数据的利用越充分，浪费的数据量越少，但批次会很大，训练会更耗时。</p>
<p><strong>优点</strong></p>
<ul>
<li>减小参数更新的方差，这样可以有更稳定的收敛。</li>
<li>利用现在最先进的深度学习库对矩阵运算进行了高度优化的特点，这样可以使得计算 mini-batch 的梯度更高效。</li>
</ul>
<p>MBGD 是训练神经网络时的常用方法，而且通常即使实际上使用的是 MBGD，也会使用 SGD 这个词来代替。</p>
<h3 id="1-4-学习率的选择"><a href="#1-4-学习率的选择" class="headerlink" title="1.4 学习率的选择"></a>1.4 学习率的选择</h3><p>选择一个好的学习率是非常困难的。太小的学习率导致收敛非常缓慢，而太大的学习率则会阻碍收敛，导致损失函数在最优点附近震荡甚至发散。相同的学习率被应用到所有参数更新中。</p>
<h2 id="2-动量优化法"><a href="#2-动量优化法" class="headerlink" title="2. 动量优化法"></a>2. 动量优化法</h2><p>动量优化方法是在梯度下降法的基础上进行的改变，具有加速梯度下降的作用。一般有标准动量优化方法Momentum、NAG（Nesterov accelerated gradient）动量优化方法。</p>
<h3 id="2-1-Momentum"><a href="#2-1-Momentum" class="headerlink" title="2.1 Momentum"></a>2.1 Momentum</h3><p>为了抑制SGD的震荡，SGDM认为梯度下降过程可以加入惯性。可以简单理解为：当我们将一个小球从山上滚下来时，没有阻力的话，它的动量会越来越大，但是如果遇到了阻力，速度就会变小。SGDM全称是SGD with momentum，在SGD基础上引入了一阶动量：</p>
<script type="math/tex; mode=display">v_{t}=\gamma v_{t-1}+\eta \nabla J(\theta)</script><p>SGD-M参数更新公式如下，其中$\eta$是学习率，$\nabla J(\theta)$是当前参数的梯度</p>
<script type="math/tex; mode=display">\theta=\theta-v_{t}</script><p>一阶动量是各个时刻梯度方向的指数移动平均值，也就是说，t时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。$\gamma$的经验值为0.9，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向。</p>
<p><strong>特点</strong></p>
<ul>
<li>加入了动量因素，SGD-M缓解了SGD在局部最优点梯度为0，无法持续更新的问题和振荡幅度过大的问题。</li>
<li>当局部沟壑比较深，动量加持用完了，依然会困在局部最优里来回振荡</li>
</ul>
<h3 id="2-2-NAG"><a href="#2-2-NAG" class="headerlink" title="2.2 NAG"></a>2.2 NAG</h3><p>NAG全称Nesterov Accelerated Gradient，是在SGD、SGD-M的基础上的进一步改进，我们知道在时刻$t$的主要下降方向是由累积动量决定的，自己的梯度方向说了也不算，那与其看当前梯度方向，不如先看看如果跟着累积动量走了一步，那个时候再怎么走。因此，NAG不计算当前位置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的下降方向：</p>
<script type="math/tex; mode=display">v_{t}=\gamma v_{t-1}+\eta \nabla_{\theta}J(\theta-\gamma v_{t-1})</script><p>NAG参数更新公式如下，其中$\eta$是学习率， $\nabla_{\theta}J(\theta-\gamma v_{t-1})$是当前参数的梯度</p>
<script type="math/tex; mode=display">\theta=\theta-v_{t}</script><p>然后用下一个点的梯度方向，与历史累积动量相结合，计算当前时刻的累积动量。</p>
<p><strong>特点</strong></p>
<ul>
<li>有利于跳出当前局部最优的沟壑，寻找新的最优值，但收敛速度慢</li>
</ul>
<h2 id="3-自适应学习率优化算法"><a href="#3-自适应学习率优化算法" class="headerlink" title="3. 自适应学习率优化算法"></a>3. 自适应学习率优化算法</h2><p>自适应学习率优化算法针对于机器学习模型的学习率，传统的优化算法要么将学习率设置为常数要么根据训练次数调节学习率。极大忽视了学习率其他变化的可能性。然而，学习率对模型的性能有着显著的影响，因此需要采取一些策略来想办法更新学习率，从而提高训练速度。<br>目前的自适应学习率优化算法主要有：AdaGrad算法，AdaDelta算法, RMSProp算法以及Adam算法。</p>
<h3 id="3-1-AdaGrad"><a href="#3-1-AdaGrad" class="headerlink" title="3.1 AdaGrad"></a>3.1 AdaGrad</h3><p>AdaGrad算法就是将每一个参数的每一次迭代的梯度取平方累加后在开方，用全局学习率除以这个数，作为学习率的动态更新。对于不同的参数动态的采取不同的学习率，让目标函数更快的收敛。为了简洁，我们用$g_{t}$来表示t时刻的梯度，$g_{t,i}$就是目标函数的偏导数：</p>
<script type="math/tex; mode=display">g_{t,i}=\nabla_{\theta}J(\theta_{t,i})</script><p>SGD在在每个时刻t对参数$\theta_{i}$的更新为：</p>
<script type="math/tex; mode=display">\theta_{t+1,i}=\theta_{t,i}-\eta \cdot g_{t,i}</script><p>Adagrad修改了t时刻对于每个参数$\theta_{i}$的学习率$\eta$:</p>
<script type="math/tex; mode=display">\theta_{t+1,i}=\theta_{t,i}-\frac{\eta}{\sqrt{G_{t,ii}+\epsilon}} \cdot g_{t,i}</script><p>其中$G_{t}\in R^{d \times d}$是对角矩阵，其中每一个对角元素i，i是$\theta_{i}$在时刻t的梯度平方和，一般为了避免分母为0，会在分母上加一个小的平滑项，用符号$\epsilon$表示，通常为$10^{-8}$ 左右。因此$\sqrt{G_{t}+\epsilon} $恒大于0，而且参数更新越频繁，二阶动量越大，学习率就越小。有趣的是，如果去掉开方操作，算法性能会大幅下降。</p>
<p><strong>优点</strong></p>
<ul>
<li>在稀疏数据场景下表现非常好</li>
<li>此前的SGD及其变体的优化器主要聚焦在优化梯度前进的方向上，而AdaGrad首次使用二阶动量来关注学习率（步长），开启了自适应学习率算法的里程。大多数实现使用一个默认值 0.01 。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>$\sqrt{G_{t}+\epsilon}$是单调递增的，会使得学习率单调递减至0，可能会使得训练过程提前结束，即便后续还有数据也无法学到必要的知识。</li>
</ul>
<h3 id="3-2-AdaDelta"><a href="#3-2-AdaDelta" class="headerlink" title="3.2 AdaDelta"></a>3.2 AdaDelta</h3><p>由于AdaGrad单调递减的学习率变化过于激进，考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也就是AdaDelta名称中Delta的来历。Adadelta是 Adagrad 的扩展，旨在帮助缓解后者学习率单调下降的问题。</p>
<p>指数移动平均值大约就是过去一段时间的平均值，因此我们用这一方法来计算二阶累积动量：</p>
<script type="math/tex; mode=display">E[g^2]_{t}=\gamma E[g^2]_{t-1}+(1-\gamma) g_{t}^2</script><p>其中$\gamma$类似于冲量，大约是0.9.现在将SGD更新的参数变化向量$\Delta \theta_{t}$:</p>
<script type="math/tex; mode=display">\Delta \theta_{t}=-\eta \cdot g_{t,i}</script><script type="math/tex; mode=display">\theta_{t+1}=\theta_{t}+\Delta \theta_{t}</script><p>在Adagrad中，$\Delta \theta_{t}$是由：</p>
<script type="math/tex; mode=display">\Delta \theta_{t}=-\frac{\eta}{\sqrt{G_{t}+\epsilon}}\cdot g_{t,i}</script><p>表示的，现在用$E[g^2]_{t}$简单代替原来的对角矩阵$G_{t}$：</p>
<script type="math/tex; mode=display">\Delta \theta_{t}=-\frac{\eta}{\sqrt{E[g^2]_{t}+\epsilon}}\cdot g_{t,i}</script><p>将分母简记为RMS，表示梯度的均方根误差：</p>
<script type="math/tex; mode=display">\Delta \theta_{t}=-\frac{\eta}{RMS[g]_{t}}\cdot g_{t}</script><p>根据作者所说，更新中，定义指数衰减均值，代替梯度平方：</p>
<script type="math/tex; mode=display">E[\Delta \theta^2]_{t}=\gamma E[\Delta \theta^2]_{t-1}+(1-\gamma)\Delta \theta_{t}^2</script><p>均方根误差变为：</p>
<script type="math/tex; mode=display">RMS[\Delta \theta]_{t}=\sqrt{E[\Delta \theta^2]_{t}+\epsilon}</script><p>$RMS[\Delta \theta]_{t}$是未知的，我们近似用前一个时间步RMS值来估计：</p>
<script type="math/tex; mode=display">\Delta \theta_{t}=-\frac{RMS[\Delta \theta]_{t-1}}{RMS[g]_{t}}g_{t}</script><script type="math/tex; mode=display">\theta_{t+1}=\theta_{t}-\Delta \theta_{t}</script><p>Adadelta不用设置学习率，因为其更新规则已经把它消除了。</p>
<p><strong>优点</strong></p>
<ul>
<li>避免了二阶动量持续累积、导致训练过程提前结束的问题了</li>
</ul>
<h3 id="3-3-RMSProp"><a href="#3-3-RMSProp" class="headerlink" title="3.3 RMSProp"></a>3.3 RMSProp</h3><p>RMSProp 算法（Hinton，2012）修改 AdaGrad 以在非凸情况下表现更好，它改变梯度累积为指数加权的移动平均值，从而丢弃距离较远的历史梯度信息。RMSProp 与 Adadelta 的移动均值更新方式十分相似：</p>
<script type="math/tex; mode=display">E[g^2]_{t}=0.9 E[g^2]_{t-1}+0.1 g_{t}^2</script><p>RMSProp参数更新公式如下，其中$\eta$是学习率， $g_{t}$是当前参数的梯度</p>
<script type="math/tex; mode=display">\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{E[g^2]_{t}+\epsilon}}g_{t}</script><p>RMSprop将学习速率除以梯度平方的指数衰减平均值。Hinton建议$\gamma$设置为0.9，默认学习率$\eta$为0.001</p>
<h3 id="3-4-Adam"><a href="#3-4-Adam" class="headerlink" title="3.4 Adam"></a>3.4 Adam</h3><p>Adam最开始是由 OpenAI 的 Diederik Kingma 和多伦多大学的 Jimmy Ba提出的。Adam使用动量和自适应学习率来加快收敛速度。SGD-M在SGD基础上增加了一阶动量，AdaGrad和AdaDelta在SGD基础上增加了二阶动量（二阶矩估计）。把一阶动量和二阶动量都用起来，就是Adam了——Adaptive + Momentum。</p>
<p>SGD的一阶矩的估计，即mean均值：</p>
<script type="math/tex; mode=display">m_{t}=\beta_{1} \cdot m_{t-1}+(1-\beta_{1}) \cdot g_{t}</script><p>加上AdaDelta的二阶动量，二阶距的估计，即variance，和方差类似，都是二阶距的一种：</p>
<script type="math/tex; mode=display">v_{t}=\beta_{2} \cdot v_{t-1}+(1-\beta_{2})\cdot g_{t}^2</script><p>对mean和var进行校正，因为mean和var的初始值为0，所以它们会向0偏置，这样处理后会减少这种偏置影响。</p>
<script type="math/tex; mode=display">\hat m_{t}=\frac{m_{t}}{1-\beta_{1}^t}</script><script type="math/tex; mode=display">\hat v_{t}=\frac{v_{t}}{1-\beta_{2}^t}</script><p>Adam参数更新公式如下：</p>
<script type="math/tex; mode=display">\theta_{t+1}=\theta_{t}-\eta \cdot \hat m_{t}/(\sqrt{\hat v_{t}}+\epsilon)</script><p>其中$\eta$是学习率， $g_{t}$是当前参数的梯度，$\beta_{1}$为一阶矩估计的指数衰减率（如 0.9），$\beta_{2}$二阶矩估计的指数衰减率（如 0.999），前者控制一阶矩估计，后者控制二阶矩估计。该超参数在稀疏梯度（如在 NLP 或计算机视觉任务中）中应该设置为接近 1 的数，$\beta_{1}^t$和$\beta_{2}^t$是$\beta_{1}$和$\beta_{2}$的t次方</p>
<p><strong>优点</strong></p>
<ul>
<li>通过一阶动量和二阶动量，有效控制学习率步长和梯度方向，防止梯度的振荡和在鞍点的静止。</li>
<li>实现简单，计算高效，对内存需求少</li>
<li>参数的更新不受梯度的伸缩变换影响</li>
<li>超参数具有很好的解释性，且通常无需调整或仅需很少的微调</li>
<li>更新的步长能够被限制在大致的范围内（初始学习率）</li>
<li>能自然地实现步长退火过程（自动调整学习率）</li>
<li>很适合应用于大规模的数据及参数的场景</li>
<li>适用于不稳定目标函数</li>
<li>适用于梯度稀疏或梯度存在很大噪声的问题</li>
</ul>
<p>Adam在很多情况下算作默认工作性能比较优秀的优化器。</p>
<p><strong>缺点</strong></p>
<ul>
<li><p>可能不收敛：二阶动量是固定时间窗口内的累积，随着时间窗口的变化，遇到的数据可能发生巨变，使得$V_{t}$可能会时大时小，不是单调变化。这就可能在训练后期引起学习率的震荡，导致模型无法收敛。</p>
<p>  修正的方法。由于Adam中的学习率主要是由二阶动量控制的，为了保证算法的收敛，可以对二阶动量的变化进行控制，避免上下波动。</p>
<script type="math/tex; mode=display">v_{t}=max(\beta_{2} \cdot v_{t-1}+ (1-\beta_{2})g_{t}^2,v_{t-1})</script></li>
<li><p>可能错过全局最优解：自适应学习率算法可能会对前期出现的特征过拟合，后期才出现的特征很难纠正前期的拟合效果。后期Adam的学习率太低，影响了有效的收敛。</p>
</li>
</ul>
<h2 id="4-可视化比较"><a href="#4-可视化比较" class="headerlink" title="4. 可视化比较"></a>4. 可视化比较</h2><p><img src="https://img-blog.csdn.net/20180426130002689" alt=""></p>
]]></content>
      <categories>
        <category>深度学习基础</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>优化器</tag>
      </tags>
  </entry>
  <entry>
    <title>OCRNet</title>
    <url>/2022/10/26/OCRNet/</url>
    <content><![CDATA[<h1 id="OCRNet论文详解"><a href="#OCRNet论文详解" class="headerlink" title="OCRNet论文详解"></a>OCRNet论文详解</h1><p>针对语义分割中如何构建上下文信息，微软亚洲研究院和中科院计算所的研究员们提出了一种新的物体上下文信息——在构建上下文信息时显式地增强了来自于同一类物体的像素的贡献，这种新的上下文信息从语义分割的定义出发，符合第一性原理思维，在2019年7月和2020年1月的 <code>Cityscapes leaderboard</code>提交结果中都取得了语义分割任务第一名的成绩。</p>
<span id="more"></span>
<h2 id="1-模型架构"><a href="#1-模型架构" class="headerlink" title="1. 模型架构"></a>1. 模型架构</h2><p>文章的总体思路是：<strong>像素的标签是像素所在的对象的标签，并且通过用相应的对象区域表示来表征每个像素来加强像素表示。</strong></p>
<p><img src="https://pic1.imgdb.cn/item/6359363e16f2c2beb1a8f77a.png" alt=""><br>主要分为三个步骤</p>
<ul>
<li>根据网络中间层得特征表示估测一个粗略得语义分割结果，即论文中得<code>Soft Object Regions</code></li>
<li>根据粗略得语义分割结果和网络最深层的特征表示计算出K组向量，即论文中得<code>Object Regions Representations</code>，其中每一个向量对应一个语义类别得特征表示</li>
<li>将网络最深层的特征表示<code>Pixel Representations</code>作为$Query$，<code>Object Regions Representations</code>作为$Key-Value$，送入$Cross-Attention$得到最后的物体上下文特征表示<code>OCR(Object Contextual Representation)</code>；然后将<code>Object Contextual Representation</code>和<code>Pixel Representations</code>拼接在一起经过融合之后得到<code>Augmented Representations</code>，最后加一个分类器预测最后的语义分割结果</li>
</ul>
<h2 id="2-代码实现-Paddle"><a href="#2-代码实现-Paddle" class="headerlink" title="2. 代码实现(Paddle)"></a>2. 代码实现(Paddle)</h2><h3 id="2-1-Soft-Object-Regions-粉色框"><a href="#2-1-Soft-Object-Regions-粉色框" class="headerlink" title="2.1 Soft Object Regions (粉色框)"></a>2.1 Soft Object Regions (粉色框)</h3><p><img src="https://pic1.imgdb.cn/item/6359363e16f2c2beb1a8f77a.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Aux_head</span>(nn.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成soft object regions&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, num_classes</span>):</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2D(in_channels, in_channels, <span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2D(in_channels),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        self.cls = nn.Conv2D(in_channels, num_classes, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, feat</span>):</span><br><span class="line">        soft_regions = self.cls(self.conv(feat))</span><br><span class="line">        <span class="keyword">return</span> soft_regions</span><br></pre></td></tr></table></figure>
<h3 id="2-2-Object-Region-Representations-紫色框"><a href="#2-2-Object-Region-Representations-紫色框" class="headerlink" title="2.2 Object Region Representations (紫色框)"></a>2.2 Object Region Representations (紫色框)</h3><p><img src="https://pic1.imgdb.cn/item/6359363e16f2c2beb1a8f77a.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SpatialGatherBlock</span>(nn.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成object region representations&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, pixels_channels, regions_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.pixels_channels = pixels_channels</span><br><span class="line">        self.regions_channels = regions_channels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pixels, regions</span>):</span><br><span class="line">        <span class="comment"># pixels: from (n, c, h, w) to (n, h*w, c)</span></span><br><span class="line">        pixels = paddle.reshape(pixels, (<span class="number">0</span>, self.pixels_channels, -<span class="number">1</span>))</span><br><span class="line">        pixels = paddle.transpose(pixels, (<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># regions: from (n, k, h, w) to (n, k, h*w)</span></span><br><span class="line">        regions = paddle.reshape(regions, (<span class="number">0</span>, self.regions_channels, -<span class="number">1</span>))</span><br><span class="line">        regions = F.softmax(regions, axis=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># feats: from (n, k, c) to (n, c, k, 1)</span></span><br><span class="line">        feats = paddle.bmm(regions, pixels)</span><br><span class="line">        feats = paddle.transpose(feats, (<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">        feats = paddle.unsqueeze(feats, axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> feats</span><br></pre></td></tr></table></figure>
<h3 id="2-3-Augmented-Representations-黄色框"><a href="#2-3-Augmented-Representations-黄色框" class="headerlink" title="2.3 Augmented Representations (黄色框)"></a>2.3 Augmented Representations (黄色框)</h3><p><img src="https://pic1.imgdb.cn/item/6359363e16f2c2beb1a8f77a.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SpatialOCRModule</span>(nn.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成 augmented representations&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 in_channels,</span></span><br><span class="line"><span class="params">                 key_channels,</span></span><br><span class="line"><span class="params">                 out_channels,</span></span><br><span class="line"><span class="params">                 dropout_rate=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.attention_block = ObjectAttentionBlock(in_channels, key_channels)</span><br><span class="line">        self.conv1x1 = nn.Sequential(</span><br><span class="line">            layers.ConvBNReLU(<span class="number">2</span> * in_channels, out_channels, <span class="number">1</span>),</span><br><span class="line">            nn.Dropout2D(dropout_rate))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pixels, regions</span>):</span><br><span class="line">        context = self.attention_block(pixels, regions)</span><br><span class="line">        feats = paddle.concat([context, pixels], axis=<span class="number">1</span>)</span><br><span class="line">        feats = self.conv1x1(feats)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> feats</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ObjectAttentionBlock</span>(nn.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;self-attention 模块&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, key_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        self.key_channels = key_channels</span><br><span class="line"></span><br><span class="line">        self.f_pixel = nn.Sequential(</span><br><span class="line">            layers.ConvBNReLU(in_channels, key_channels, <span class="number">1</span>),</span><br><span class="line">            layers.ConvBNReLU(key_channels, key_channels, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        self.f_object = nn.Sequential(</span><br><span class="line">            layers.ConvBNReLU(in_channels, key_channels, <span class="number">1</span>),</span><br><span class="line">            layers.ConvBNReLU(key_channels, key_channels, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        self.f_down = layers.ConvBNReLU(in_channels, key_channels, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.f_up = layers.ConvBNReLU(key_channels, in_channels, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, proxy</span>):</span><br><span class="line">        x_shape = paddle.shape(x)</span><br><span class="line">        <span class="comment"># query : from (n, c1, h1, w1) to (n, h1*w1, key_channels)</span></span><br><span class="line">        query = self.f_pixel(x)</span><br><span class="line">        query = paddle.reshape(query, (<span class="number">0</span>, self.key_channels, -<span class="number">1</span>))</span><br><span class="line">        query = paddle.transpose(query, (<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># key : from (n, c2, h2, w2) to (n, key_channels, h2*w2)</span></span><br><span class="line">        key = self.f_object(proxy)</span><br><span class="line">        key = paddle.reshape(key, (<span class="number">0</span>, self.key_channels, -<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># value : from (n, c2, h2, w2) to (n, h2*w2, key_channels)</span></span><br><span class="line">        value = self.f_down(proxy)</span><br><span class="line">        value = paddle.reshape(value, (<span class="number">0</span>, self.key_channels, -<span class="number">1</span>))</span><br><span class="line">        value = paddle.transpose(value, (<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># sim_map (n, h1*w1, h2*w2)</span></span><br><span class="line">        sim_map = paddle.bmm(query, key)</span><br><span class="line">        sim_map = (self.key_channels**-<span class="number">.5</span>) * sim_map</span><br><span class="line">        sim_map = F.softmax(sim_map, axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># context from (n, h1*w1, key_channels) to (n , out_channels, h1, w1)</span></span><br><span class="line">        context = paddle.bmm(sim_map, value)</span><br><span class="line">        context = paddle.transpose(context, (<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">        context = paddle.reshape(context,</span><br><span class="line">                                 (<span class="number">0</span>, self.key_channels, x_shape[<span class="number">2</span>], x_shape[<span class="number">3</span>]))</span><br><span class="line">        context = self.f_up(context)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> context</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>完整代码：<a href="https://github.com/PaddlePaddle/PaddleSeg/blob/release%2F2.6/paddleseg/models/ocrnet.py">https://github.com/PaddlePaddle/PaddleSeg/blob/release%2F2.6/paddleseg/models/ocrnet.py</a></strong></p>
]]></content>
      <categories>
        <category>图像分割</category>
      </categories>
      <tags>
        <tag>模型</tag>
        <tag>图像分割</tag>
      </tags>
  </entry>
  <entry>
    <title>ResNet</title>
    <url>/2021/05/14/ResNet/</url>
    <content><![CDATA[<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a><a href="https://arxiv.org/pdf/1512.03385.pdf">ResNet</a></h1><h2 id="1-模型介绍"><a href="#1-模型介绍" class="headerlink" title="1. 模型介绍"></a>1. 模型介绍</h2><p>ResNet（Residual Neural Network）由微软研究院的Kaiming He等四名华人提出，通过使用ResNet Unit成功训练出了152层的神经网络，并在ILSVRC2015比赛中取得冠军，在top5上的错误率为3.57%，同时参数量比VGGNet低，效果非常突出。ResNet的结构可以极快的加速神经网络的训练，模型的准确率也有比较大的提升。同时ResNet的推广性非常好，甚至可以直接用到InceptionNet网络中。</p>
<span id="more"></span>
<p>一般网络越深，特征就越丰富，模型效果也就越好。在深度重要的驱动下，出现了2个问题：</p>
<h3 id="1-1-梯度消失和梯度爆炸"><a href="#1-1-梯度消失和梯度爆炸" class="headerlink" title="1.1 梯度消失和梯度爆炸"></a>1.1 梯度消失和梯度爆炸</h3><ul>
<li>梯度消失：误差梯度&lt;1，当网络层数增多时，最终求的梯度会以指数形式衰减</li>
<li>梯度爆炸：误差梯度&gt;1，当网络层数增多时，最终求的梯度会以指数形式增加</li>
</ul>
<h3 id="1-2-退化问题"><a href="#1-2-退化问题" class="headerlink" title="1.2 退化问题"></a>1.2 退化问题</h3><p>在适当深度的模型中添加更多的层会导致更高的训练误差，如下图：<br><img src="https://pic.imgdb.cn/item/631075ee16f2c2beb157919a.png" alt=""><br>退化问题表明：很难通过多个非线性层来逼近<strong>恒等映射</strong>，因为如果可以的话，那么更深的模型的训练误差应该不大于更浅层的对应模型。而ResNet就是提出一种方法让网络拥有<strong>恒等映射</strong>能力，即随着网络层数的增加，深层网络至少不会差于浅层网络。</p>
<h2 id="2-模型结构"><a href="#2-模型结构" class="headerlink" title="2. 模型结构"></a>2. 模型结构</h2><h3 id="2-1-残差块"><a href="#2-1-残差块" class="headerlink" title="2.1 残差块"></a>2.1 残差块</h3><p>恒等映射即为 $H(x)=x$，已有的神经网络结构很难做到这一点，但是如果我们将网络设计成 $H(x)=F(x)+x$，即 $F(x)=H(x)−x$，那么只需要使残差函数 $F(x)=0$，就构成了恒等映射 $H(x)=x$。</p>
<p><img src="https://pic.imgdb.cn/item/631076da16f2c2beb1581d27.png" alt=""></p>
<p>残差结构的目的是，随着网络的加深，使 $F(x)$ 逼近于0，使得深度网络的精度在最优浅层网络的基础上不会下降。但为什么不直接选取最优的浅层网络呢？这是因为最优的浅层网络结构并不易找寻，而ResNet可以通过增加深度，找到最优的浅层网络并保证深层网络不会因为层数的叠加而发生网络退化</p>
<p><img src="https://pic.imgdb.cn/item/6310773016f2c2beb1585460.png" alt=""></p>
<p>代码实现：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BasicBlock</span>(nn.Module):</span><br><span class="line">    expansion = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride=<span class="number">1</span>, downsample=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        identity = x</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(nn.Module):</span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride=<span class="number">1</span>, downsample=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Bottleneck, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(out_channel)</span><br><span class="line"></span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.Conv2d(out_channel)</span><br><span class="line"></span><br><span class="line">        self.conv3 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel*self.expansion, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(out_channel*self.expansion)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        identity = x</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.bn3(out)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure></p>
<h3 id="2-2-ResNet"><a href="#2-2-ResNet" class="headerlink" title="2.2 ResNet"></a>2.2 ResNet</h3><p><code>ResNet</code>的架构如下图所示，在主体卷积部分中使用5个模块<br><img src="https://pic.imgdb.cn/item/6310798516f2c2beb159e975.png" alt=""></p>
<ul>
<li>第1个模块：使用一个64通道的$7 × 7$卷积和一个步长为2的$3 × 3$的最大池化</li>
<li>第2个模块：残差块的堆叠</li>
<li>第3个模块：残差块的堆叠，其中第一个残差块中的$3×3$卷积的步长为2，从而达到降采样的目的</li>
<li>第4个模块：残差块的堆叠，其中第一个残差块中的$3×3$卷积的步长为2，从而达到降采样的目的</li>
<li>第5个模块：残差块的堆叠，其中第一个残差块中的$3×3$卷积的步长为2，从而达到降采样的目的</li>
<li>第五模块的后面紧跟输出层，使用全局平均池化层来将每个通道的高和宽变成1，最后接上一个输出个数为标签类别数的全连接层</li>
</ul>
<p>代码实现：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line">from torchsummary import summary</span><br><span class="line">import torchvision.models as models</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class BasicBlock(nn.Module):</span><br><span class="line">    expansion = 1</span><br><span class="line"></span><br><span class="line">    def __init__(self, in_channel, out_channel, stride=1, downsample=None):</span><br><span class="line">        super(BasicBlock, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=3, stride=stride, padding=1, bias=False)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        self.relu = nn.ReLU(inplace=True)</span><br><span class="line"></span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel, kernel_size=3, stride=1, padding=1, bias=False)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        identity = x</span><br><span class="line">        if self.downsample is not None:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        return out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Bottleneck(nn.Module):</span><br><span class="line">    expansion = 4</span><br><span class="line"></span><br><span class="line">    def __init__(self, in_channel, out_channel, stride=1, downsample=None):</span><br><span class="line">        super(Bottleneck, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=1, stride=1, bias=False)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(out_channel)</span><br><span class="line"></span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel, kernel_size=3, stride=stride, padding=1, bias=False)</span><br><span class="line">        self.bn2 = nn.Conv2d(out_channel)</span><br><span class="line"></span><br><span class="line">        self.conv3 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel*self.expansion, kernel_size=1, stride=1, bias=False)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(out_channel*self.expansion)</span><br><span class="line">        self.relu = nn.ReLU(inplace=True)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        identity = x</span><br><span class="line">        if self.downsample is not None:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.bn3(out)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        return out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class ResNet(nn.Module):</span><br><span class="line">    def __init__(self, block, block_num, num_classes=1000, include_top=True):</span><br><span class="line">        super(ResNet, self).__init__()</span><br><span class="line">        self.include_top = include_top</span><br><span class="line">        self.in_channel = 64</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(3, self.in_channel, kernel_size=7, stride=2, padding=3, bias=False)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(self.in_channel)</span><br><span class="line">        self.relu = nn.ReLU(inplace=True)</span><br><span class="line">        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)</span><br><span class="line"></span><br><span class="line">        self.layer1 = self._make_layer(block, 64, block_num[0])</span><br><span class="line">        self.layer2 = self._make_layer(block, 128, block_num[1], stride=2)</span><br><span class="line">        self.layer3 = self._make_layer(block, 256, block_num[2], stride=2)</span><br><span class="line">        self.layer4 = self._make_layer(block, 512, block_num[3], stride=2)</span><br><span class="line"></span><br><span class="line">        if self.include_top:</span><br><span class="line">            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))</span><br><span class="line">            self.fc = nn.Linear(512 * block.expansion, num_classes)</span><br><span class="line"></span><br><span class="line">        for m in self.modules():</span><br><span class="line">            if isinstance(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=&#x27;fan_out&#x27;, nonlinearity=&#x27;relu&#x27;)</span><br><span class="line"></span><br><span class="line">    def _make_layer(self, block, channel, block_num, stride=1):</span><br><span class="line">        downsample = None</span><br><span class="line"></span><br><span class="line">        if stride != 1 or self.in_channel != channel * block.expansion:</span><br><span class="line">            downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False),</span><br><span class="line">                nn.BatchNorm2d(channel*block.expansion)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line"></span><br><span class="line">        layers.append(block(self.in_channel, channel, downsample=downsample, stride=stride))</span><br><span class="line">        self.in_channel = channel * block.expansion</span><br><span class="line">        for _ in range(1, block_num):</span><br><span class="line">            layers.append(block(self.in_channel, channel))</span><br><span class="line"></span><br><span class="line">        return nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line"></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        x = self.layer4(x)</span><br><span class="line"></span><br><span class="line">        if self.include_top:</span><br><span class="line">            x = self.avgpool(x)</span><br><span class="line">            x = torch.flatten(x, 1)</span><br><span class="line">            x = self.fc(x)</span><br><span class="line"></span><br><span class="line">        return x</span><br></pre></td></tr></table></figure></p>
<h2 id="3-模型特点"><a href="#3-模型特点" class="headerlink" title="3. 模型特点"></a>3. 模型特点</h2><ol>
<li>使用了大量的残差连接，缓解了训练中梯度消失的问题，使得模型更容易收敛</li>
<li>使用步长为2的卷积层代替池化层实现降采样</li>
<li>每个卷积层之后都紧接着BN</li>
</ol>
]]></content>
      <categories>
        <category>图像分类</category>
      </categories>
      <tags>
        <tag>模型</tag>
        <tag>backbone</tag>
      </tags>
  </entry>
  <entry>
    <title>SegNet</title>
    <url>/2021/12/31/SegNet/</url>
    <content><![CDATA[<h1 id="SegNet论文详解"><a href="#SegNet论文详解" class="headerlink" title="SegNet论文详解"></a>SegNet论文详解</h1><p>本文提出了一种用于语义分割的深度全卷积神经网络结构SegNet，其核心<strong>由一个编码器网络和一个对应的解码器网络以及一个像素级分类层组成</strong>。<br><span id="more"></span></p>
<p>本文的创新在于：<br>解码器使用在对应编码器的最大池化步骤中计算的<strong>池化索引</strong>来执行非线性上采样，这与反卷积相比，减少了参数量和运算量，而且消除了学习上采样的需要。<br><img src="https://img-blog.csdnimg.cn/0ed48fd837404b7da916fc0ed8d4cbf8.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5L2V5aaC5Y2D5rO3,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<h2 id="1-网络结构"><a href="#1-网络结构" class="headerlink" title="1. 网络结构"></a>1. 网络结构</h2><p><img src="https://img-blog.csdnimg.cn/28ab3c4e856447e4b981ecfdcad3055c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5L2V5aaC5Y2D5rO3,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<h3 id="1-1-编码器"><a href="#1-1-编码器" class="headerlink" title="1.1 编码器"></a>1.1 编码器</h3><ol>
<li>Conv层<ul>
<li>通过卷积提取特征，其中使用的是<code>same padding</code>的卷积，不会改变特征图的尺寸</li>
</ul>
</li>
<li>BN层<ul>
<li>起到归一化的作用</li>
</ul>
</li>
<li>ReLU层<ul>
<li>起到激活函数的作用</li>
</ul>
</li>
<li>Pooling层<ul>
<li><code>max pooling</code>层，同时会<strong>记录最大值的索引位置</strong></li>
</ul>
</li>
</ol>
<h3 id="1-2-解码器"><a href="#1-2-解码器" class="headerlink" title="1.2 解码器"></a>1.2 解码器</h3><ol>
<li><p>Upsampling层<br><img src="https://img-blog.csdnimg.cn/0afcb64febd349909cc66e0ca58966fc.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5L2V5aaC5Y2D5rO3,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<ul>
<li>对输入的特征图放大两倍，然后把输入特征图的数据根据编码器<code>pooling</code>层的<strong>索引位置</strong>放入，<strong>其他位置为0</strong></li>
</ul>
</li>
<li>Conv层<ul>
<li>通过卷积提取特征，其中使用的是<code>same padding</code>的卷积，不会改变特征图的尺寸</li>
</ul>
</li>
<li>BN层<ul>
<li>起到归一化的作用</li>
</ul>
</li>
<li>ReLU层<ul>
<li>起到激活函数的作用</li>
</ul>
</li>
</ol>
<h3 id="1-3-像素级分类层"><a href="#1-3-像素级分类层" class="headerlink" title="1.3 像素级分类层"></a>1.3 像素级分类层</h3><p>输出每一个像素点在所有类别概率，其中<strong>最大的概率类别为该像素的预测值</strong></p>
<h2 id="2-Pytorch实现"><a href="#2-Pytorch实现" class="headerlink" title="2. Pytorch实现"></a>2. Pytorch实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        batchNorm_momentum = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">        self.encode1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>, momentum=batchNorm_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>, momentum=batchNorm_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.encode2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>, momentum=batchNorm_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>, momentum=batchNorm_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.encode3 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">256</span>, momentum=batchNorm_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">256</span>, momentum=batchNorm_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.encode4 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">512</span>, momentum=batchNorm_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">512</span>, momentum=batchNorm_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.encode5 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">512</span>, momentum=batchNorm_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">512</span>, momentum=batchNorm_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        idx = []</span><br><span class="line"></span><br><span class="line">        x = self.encode1(x)</span><br><span class="line">        x, id1 = F.max_pool2d_with_indices(x, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, return_indices=<span class="literal">True</span>)</span><br><span class="line">        idx.append(id1)</span><br><span class="line"></span><br><span class="line">        x = self.encode2(x)</span><br><span class="line">        x, id2 = F.max_pool2d_with_indices(x, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, return_indices=<span class="literal">True</span>)</span><br><span class="line">        idx.append(id2)</span><br><span class="line"></span><br><span class="line">        x = self.encode3(x)</span><br><span class="line">        x, id3 = F.max_pool2d_with_indices(x, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, return_indices=<span class="literal">True</span>)</span><br><span class="line">        idx.append(id3)</span><br><span class="line"></span><br><span class="line">        x = self.encode4(x)</span><br><span class="line">        x, id4 = F.max_pool2d_with_indices(x, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, return_indices=<span class="literal">True</span>)</span><br><span class="line">        idx.append(id4)</span><br><span class="line"></span><br><span class="line">        x = self.encode5(x)</span><br><span class="line">        x, id5 = F.max_pool2d_with_indices(x, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, return_indices=<span class="literal">True</span>)</span><br><span class="line">        idx.append(id5)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x, idx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        batchNorm_momentum = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">        self.decode1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">512</span>, momentum=batchNorm_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">512</span>, momentum=batchNorm_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.decode2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">512</span>, momentum=batchNorm_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">512</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">256</span>, momentum=batchNorm_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.decode3 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">256</span>, momentum=batchNorm_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>, momentum=batchNorm_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.decode4 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>, momentum=batchNorm_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">128</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>, momentum=batchNorm_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.decode5 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>, momentum=batchNorm_momentum),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, out_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, idx</span>):</span><br><span class="line">        x = F.max_unpool2d(x, idx[<span class="number">4</span>], kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        x = self.decode1(x)</span><br><span class="line">        x = F.max_unpool2d(x, idx[<span class="number">3</span>], kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        x = self.decode2(x)</span><br><span class="line">        x = F.max_unpool2d(x, idx[<span class="number">2</span>], kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        x = self.decode3(x)</span><br><span class="line">        x = F.max_unpool2d(x, idx[<span class="number">1</span>], kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        x = self.decode4(x)</span><br><span class="line">        x = F.max_unpool2d(x, idx[<span class="number">0</span>], kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        x = self.decode5(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SegNet</span>(nn.Module):</span><br><span class="line">    <span class="comment"># https://arxiv.org/abs/1511.00561</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(SegNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.encode = Encoder(in_channels=<span class="number">3</span>)</span><br><span class="line">        self.decode = Decoder(out_channels=num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x, idx = self.encode(x)</span><br><span class="line">        x = self.decode(x, idx)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">384</span>, <span class="number">544</span>)</span><br><span class="line">    model = SegNet(num_classes=<span class="number">2</span>)</span><br><span class="line">    output = model(<span class="built_in">input</span>)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>图像分割</category>
      </categories>
      <tags>
        <tag>模型</tag>
        <tag>图像分割</tag>
      </tags>
  </entry>
  <entry>
    <title>ShuffleNet</title>
    <url>/2021/10/31/ShuffleNet/</url>
    <content><![CDATA[<h1 id="ShuffleNet"><a href="#ShuffleNet" class="headerlink" title="ShuffleNet"></a><a href="https://arxiv.org/pdf/1707.01083">ShuffleNet</a></h1><h2 id="1-模型介绍"><a href="#1-模型介绍" class="headerlink" title="1. 模型介绍"></a>1. 模型介绍</h2><p>我们提出了一个极其效率的CNN架构——<code>ShuffleNet</code>，其专为计算能力非常有限的移动设备设计。这个新的架构利用了两个新的操作：<code>pointwise group conv</code>和<code>channel shuffle</code>，并大大降低计算成本，同时确保准确性。<br><span id="more"></span></p>
<h2 id="2-模型结构及实现"><a href="#2-模型结构及实现" class="headerlink" title="2. 模型结构及实现"></a>2. 模型结构及实现</h2><h3 id="2-1-Channel-Shuffle-for-Group-Convolutions"><a href="#2-1-Channel-Shuffle-for-Group-Convolutions" class="headerlink" title="2.1 Channel Shuffle for Group Convolutions"></a>2.1 Channel Shuffle for Group Convolutions</h3><p>在微型网络结构中， 由于 <code>1×1</code> 卷积计算代价很高，在计算资源有限的情况下特征图的通道数就会受限，这会极大地降低模型的准确率。为了解决这个问题，一个简单的方案就是通道之间进行稀疏连接，也就是对 <code>1×1</code> 卷积也进行分组。</p>
<p>如下图图(a)所示，输出特征图只与一部分输入特征图相连接。但这样就会带来一个副作用，叠加几个卷积层后，输出的特征图都只由输入特征的其中一部分产生，比如图中红色部分的特征就只由输入的红色部分特征得来，而蓝色部分的特征就只由输入的蓝色部分特征得来。这阻止了不同组之间特征的信息流动因此会减弱网络的表示能力。</p>
<p><img src="https://img-blog.csdnimg.cn/6270bbe24603458284fc42d44421bdb9.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5L2V5aaC5Y2D5rO3,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<p>如果我们允许组卷积可以从不同组获取输入数据，那么输入通道和输出通道将会完全相关联。如上图图(b)所示，先把每个组内的特征分为几个子组特征，再把每个子组特征分别送到下一层的每个组中去卷积。假设一个卷积层有$g$个组，其输出具有$g \times n$个通道，我们首先将输出通道<code>reshape</code>为$(g, n)$，然后再转置，最后将其<code>flatten</code>（展平）作为下一层的输入。具体见下图，其中$g=3，n=2$</p>
<p><img src="https://img-blog.csdnimg.cn/e8e5c28945094cceb02059c03c9cee08.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5L2V5aaC5Y2D5rO3,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<h3 id="2-2-ShuffleNet-Unit"><a href="#2-2-ShuffleNet-Unit" class="headerlink" title="2.2 ShuffleNet Unit"></a>2.2 ShuffleNet Unit</h3><p><img src="https://img-blog.csdnimg.cn/d9f0f1bdc7394707add48298fce93af4.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5L2V5aaC5Y2D5rO3,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<p>接下来分析一下，<code>ShuffleNet</code>的<code>FLOPs</code>的变化。假设输出尺寸为$c∗h∗w$，和<code>bottleneck</code>中的通道数为$m$，$g$是分组的组数。其中<code>ResNet</code>（左）和<code>ResNeXt</code>（右）的结构单元如下图所示：<br><img src="https://img-blog.csdnimg.cn/93a4adc51b3c438f82a0dd7ad09ec911.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5L2V5aaC5Y2D5rO3,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<ul>
<li><code>ResNet</code>的FLOPs为：$(c \times 1 \times 1)hwm+(m \times 3 \times 3)hwm+(m \times 1 \times 1)hwc=9hwm^2+2hwcm=hw(2cm+9m^2)$</li>
<li><code>ResNeXt</code>的FLOPs为：$(c/g \times 1 \times 1)hwm + (m/g \times 3 \times 3)hwm+(m/g \times 1 \times 1)hwc=hw(2cm/g+9m^2/g)$（论文中是$hw(2cm+9m^2/g)$，应该是将ResNeXt中bottleneck中的1x1卷积看作为常规卷积，而非组卷积）</li>
<li><code>ShuffleNet</code>的FLOPs为：$(c/g \times 1 \times 1)hwm+(m/m \times 3 \times 3)hwm+(m/g \times 1 \times 1)hwc=hw(2cm/g+9m)$</li>
</ul>
<p>可以看出，<code>ShuffleNet</code>相对的<code>FLOPs</code>较小</p>
<h3 id="2-3-网络结构"><a href="#2-3-网络结构" class="headerlink" title="2.3 网络结构"></a>2.3 网络结构</h3><p><code>ShuffleNet</code>网络结构如下图所示：<br><img src="https://img-blog.csdnimg.cn/8359ce7ecfc2480ebd6a576907599d3c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5L2V5aaC5Y2D5rO3,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<p>网络中的 ShuffleNet 单元可以划分为三个阶段，每个阶段的第一个单元步长为 <code>2</code>，每经过一个阶段特征图通道数翻倍，瓶颈层的特征图通道数为输出通道数的<code>1/4</code></p>
<p>分组卷积的组数 $g$控制着点卷积的稀疏性，在同一个复杂度下，组数越多，特征图的通道数就可以越大。</p>
<p>代码实现：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBNRelU</span>(nn.Sequential):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, kernel_size, stride, groups</span>):</span><br><span class="line">        padding = (kernel_size - <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">        <span class="built_in">super</span>(ConvBNRelU, self).__init__(</span><br><span class="line">            nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups),</span><br><span class="line">            nn.BatchNorm2d(out_channel),</span><br><span class="line">            nn.ReLU6(inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBN</span>(nn.Sequential):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, groups</span>):</span><br><span class="line">        <span class="built_in">super</span>(ConvBN, self).__init__(</span><br><span class="line">            nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, groups=groups),</span><br><span class="line">            nn.BatchNorm2d(out_channel),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChannelShuffle</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, groups</span>):</span><br><span class="line">        <span class="built_in">super</span>(ChannelShuffle, self).__init__()</span><br><span class="line">        self.groups = groups</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># Channel shuffle: [N,C,H,W] -&gt; [N,g,C/g,H,W] -&gt; [N,C/g,g,H,w] -&gt; [N,C,H,W]</span></span><br><span class="line">        bacth_size, num_channels, height, width = x.size()</span><br><span class="line">        channels_per_group = num_channels // self.groups</span><br><span class="line"></span><br><span class="line">        x = x.view(bacth_size, self.groups, channels_per_group, height, width)</span><br><span class="line">        x = torch.transpose(x, dim0=<span class="number">1</span>, dim1=<span class="number">2</span>).contiguous()</span><br><span class="line">        x = x.view(bacth_size, -<span class="number">1</span>, height, width)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ShuffleNetUnits</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride, groups</span>):</span><br><span class="line">        <span class="built_in">super</span>(ShuffleNetUnits, self).__init__()</span><br><span class="line">        self.stride = stride</span><br><span class="line">        out_channel = out_channel - in_channel <span class="keyword">if</span> self.stride &gt; <span class="number">1</span> <span class="keyword">else</span> out_channel</span><br><span class="line">        mid_channel = out_channel // <span class="number">4</span></span><br><span class="line"></span><br><span class="line">        self.bottleneck = nn.Sequential(</span><br><span class="line">            ConvBNRelU(in_channel=in_channel, out_channel=mid_channel, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, groups=groups),</span><br><span class="line">            ChannelShuffle(groups=groups),</span><br><span class="line">            ConvBNRelU(in_channel=mid_channel, out_channel=mid_channel, kernel_size=<span class="number">3</span>, stride=stride, groups=groups),</span><br><span class="line">            ConvBN(in_channel=mid_channel, out_channel=out_channel, groups=groups),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.stride &gt; <span class="number">1</span>:</span><br><span class="line">            self.shortcut = nn.AvgPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.relu = nn.ReLU6(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = self.bottleneck(x)</span><br><span class="line">        <span class="keyword">if</span> self.stride &gt; <span class="number">1</span>:</span><br><span class="line">            out = torch.cat([self.shortcut(x), out], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            out += x</span><br><span class="line">        <span class="keyword">return</span> self.relu(out)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ShuffleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, planes, layers, groups, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ShuffleNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.stage1 = nn.Sequential(</span><br><span class="line">            ConvBNRelU(in_channel=<span class="number">3</span>, out_channel=<span class="number">24</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, groups=<span class="number">1</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.stage2 = self._make_layer(in_channel=<span class="number">24</span>, out_channel=planes[<span class="number">0</span>], groups=groups, block_num=layers[<span class="number">0</span>], is_stage2=<span class="literal">True</span>)</span><br><span class="line">        self.stage3 = self._make_layer(in_channel=planes[<span class="number">0</span>], out_channel=planes[<span class="number">1</span>], groups=groups, block_num=layers[<span class="number">1</span>], is_stage2=<span class="literal">False</span>)</span><br><span class="line">        self.stage4 = self._make_layer(in_channel=planes[<span class="number">1</span>], out_channel=planes[<span class="number">2</span>], groups=groups, block_num=layers[<span class="number">2</span>], is_stage2=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.globalpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Dropout(p=<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(in_features=planes[<span class="number">2</span>], out_features=num_classes)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># weight init</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    nn.init.zeros_(m.bias)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                nn.init.ones_(m.weight)</span><br><span class="line">                nn.init.zeros_(m.bias)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                nn.init.normal_(m.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">                nn.init.zeros_(m.bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, in_channel, out_channel, groups, block_num, is_stage2</span>):</span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(ShuffleNetUnits(in_channel=in_channel, out_channel=out_channel, stride=<span class="number">2</span>, groups=<span class="number">1</span> <span class="keyword">if</span> is_stage2 <span class="keyword">else</span> groups))</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, block_num):</span><br><span class="line">            layers.append(ShuffleNetUnits(in_channel=out_channel, out_channel=out_channel, stride=<span class="number">1</span>, groups=groups))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.stage1(x)</span><br><span class="line">        x = self.stage2(x)</span><br><span class="line">        x = self.stage3(x)</span><br><span class="line">        x = self.stage4(x)</span><br><span class="line"></span><br><span class="line">        x = self.globalpool(x)</span><br><span class="line">        x = torch.flatten(x, start_dim=<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">shufflenet_g1</span>(<span class="params">**kwargs</span>):</span><br><span class="line">    planes = [<span class="number">144</span>, <span class="number">288</span>, <span class="number">576</span>]</span><br><span class="line">    layers = [<span class="number">4</span>, <span class="number">8</span>, <span class="number">4</span>]</span><br><span class="line">    model = ShuffleNet(planes=planes, layers=layers, groups=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">shufflenet_g2</span>(<span class="params">**kwargs</span>):</span><br><span class="line">    planes = [<span class="number">200</span>, <span class="number">400</span>, <span class="number">800</span>]</span><br><span class="line">    layers = [<span class="number">4</span>, <span class="number">8</span>, <span class="number">4</span>]</span><br><span class="line">    model = ShuffleNet(planes=planes, layers=layers, groups=<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = shufflenet_g2()</span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line">out = model(x)</span><br><span class="line"><span class="built_in">print</span>(out.size())</span><br></pre></td></tr></table></figure></p>
<h2 id="3-模型特点"><a href="#3-模型特点" class="headerlink" title="3. 模型特点"></a>3. 模型特点</h2><ol>
<li>提出了<code>pointwise group conv</code></li>
<li>提出了<code>channel shuffle</code>操作</li>
</ol>
]]></content>
      <categories>
        <category>图像分类</category>
      </categories>
      <tags>
        <tag>模型</tag>
        <tag>backbone</tag>
        <tag>轻量级网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Transfomer</title>
    <url>/2022/09/09/Transfomer/</url>
    <content><![CDATA[<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a><a href="https://arxiv.org/abs/1706.03762">Transformer</a></h1><h2 id="1-模型介绍"><a href="#1-模型介绍" class="headerlink" title="1.模型介绍"></a>1.模型介绍</h2><p>Transformer 网络架构架构由 Ashish Vaswani 等人在 Attention Is All You Need一文中提出，并用于机器翻译任务，和以往网络架构有所区别的是，该网络架构中，编码器和解码器没有采用 RNN 或 CNN 等网络架构，而是采用完全依赖于注意力机制的架构。<br><span id="more"></span></p>
<h2 id="2-模型结构及实现"><a href="#2-模型结构及实现" class="headerlink" title="2. 模型结构及实现"></a>2. 模型结构及实现</h2><p>Transformer的结构包括<code>Input Embedding</code>, <code>Position Embedding</code>, <code>Encoder</code>, <code>Decoder</code>。</p>
<h3 id="2-1-Embedding"><a href="#2-1-Embedding" class="headerlink" title="2.1 Embedding"></a>2.1 Embedding</h3><p>字向量与位置编码的公式表示如下：</p>
<script type="math/tex; mode=display">X=Embedding Lookup(X)+Position Encoding</script><h4 id="2-1-1-Input-Embedding"><a href="#2-1-1-Input-Embedding" class="headerlink" title="2.1.1 Input Embedding"></a>2.1.1 Input Embedding</h4><p>可以将Input Embedding看作是一个 lookup table，对于每个 word，进行 word embedding 就相当于一个lookup操作，查出一个对应结果。</p>
<h4 id="2-1-2-Position-Encoding"><a href="#2-1-2-Position-Encoding" class="headerlink" title="2.1.2 Position Encoding"></a>2.1.2 Position Encoding</h4><p>Transformer模型中还缺少一种解释输入序列中单词顺序的方法。为了处理这个问题，transformer给encoder层和decoder层的输入添加了一个额外的向量Positional Encoding，维度和embedding的维度一样，这个向量采用了一种很独特的方法来让模型学习到这个值，这个向量能决定当前词的位置，或者说在一个句子中不同的词之间的距离。这个位置向量的具体计算方法有很多种，论文中的计算方法如下</p>
<script type="math/tex; mode=display">PE(pos,2i)=sin(pos/10000^{2i}/d_{model})</script><script type="math/tex; mode=display">PE(pos,2i+1)=cos(pos/10000^{2i}/d_{model})</script><p>其中pos是指当前词在句子中的位置，i是指向量中每个值的index，可以看出，在偶数位置，使用正弦编码，在奇数位置，使用余弦编码.</p>
<h3 id="2-2-Encoder"><a href="#2-2-Encoder" class="headerlink" title="2.2 Encoder"></a>2.2 Encoder</h3><p><img src="https://pic.imgdb.cn/item/631b010d16f2c2beb19eb181.png" alt=""></p>
<p>用公式把一个Transformer Encoder block 的计算过程整理一下</p>
<ul>
<li>自注意力机制</li>
</ul>
<script type="math/tex; mode=display">Q=XW_{Q}</script><script type="math/tex; mode=display">K=XW_{K}</script><script type="math/tex; mode=display">V=XW_{V}</script><script type="math/tex; mode=display">X_{attention}=selfAttention(Q,K,V)</script><ul>
<li>self-attention 残差连接与 Layer Normalization</li>
</ul>
<script type="math/tex; mode=display">X_{attention}=LayerNorm(X_{attention})</script><ul>
<li>FeedForward，其实就是两层线性映射并用激活函数激活，比如说RELU</li>
</ul>
<script type="math/tex; mode=display">X_{hidden}=Linear(RELU(Linear(X_{attention})))</script><ul>
<li>FeedForward 残差连接与 Layer Normalization</li>
</ul>
<script type="math/tex; mode=display">X_{hidden}=X_{attention}+X_{hidden}</script><script type="math/tex; mode=display">X_{hidden}=LayerNorm(X_{hidden})</script><p>其中：$X_{hidden} \in R^{batch_size<em>seq_len</em>embed_dim}$</p>
<h4 id="2-2-1-自注意力机制"><a href="#2-2-1-自注意力机制" class="headerlink" title="2.2.1 自注意力机制"></a>2.2.1 自注意力机制</h4><ul>
<li>首先，自注意力机制（self-attention）会计算出三个新的向量，在论文中，向量的维度是512维，我们把这三个向量分别称为Query、Key、Value，这三个向量是用embedding向量与一个矩阵相乘得到的结果，这个矩阵是随机初始化的，维度为（64，512）注意第二个维度需要和embedding的维度一样，其值在反向传播的过程中会一直进行更新，得到的这三个向量的维度是64低于embedding维度的。</li>
</ul>
<p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/8f322d81d9c3491d9b714e39986e482926755e9c86dd4266805cceb4add145c7"  width="600px" /></center> </p>
<p><center><br>Query Key Value </br></center><br><br></br></p>
<p>2、计算self-attention的分数值，该分数值决定了当我们在某个位置encode一个词时，对输入句子的其他部分的关注程度。这个分数值的计算方法是Query与Key做点乘，以下图为例，首先我们需要针对Thinking这个词，计算出其他词对于该词的一个分数值，首先是针对于自己本身即q1·k1，然后是针对于第二个词即q1·k2</p>
<p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/04a8c9a33598465c80e63736e54c70b2c480a4feec9141dcb3487cf5a1f90f7a"  width="600px" /></center> </p>
<p><center><br>Query Key Value</br></center><br><br></br></p>
<p>3、接下来，把点乘的结果除以一个常数，这里我们除以8，这个值一般是采用上文提到的矩阵的第一个维度的开方即64的开方8，当然也可以选择其他的值，然后把得到的结果做一个softmax的计算。得到的结果即是每个词对于当前位置的词的相关性大小，当然，当前位置的词相关性肯定会会很大</p>
<p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/ed1176ae195145fa8abf6635d4d6aaeb938d2d803b6d458b9cc41a9ea7e1914f"  width="600px" /></center> </p>
<p><center><br>softmax </br></center><br><br></br></p>
<p>4、下一步就是把Value和softmax得到的值进行相乘，并相加，得到的结果即是self-attetion在当前节点的值。</p>
<p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/5be4009ebf3f43ce9a9947785bb6058ba7289b9adf9b4aba8f6682d949cd4b4f"  width="600px" /></center> </p>
<p><center><br>dot product </br></center><br><br></br></p>
<p>在实际的应用场景，为了提高计算速度，我们采用的是矩阵的方式，直接计算出Query, Key, Value的矩阵，然后把embedding的值与三个矩阵直接相乘，把得到的新矩阵Q与K相乘，乘以一个常数，做softmax操作，最后乘上V矩阵</p>
<p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/82e73917080f4f64917ae7a45bfa39596c2229f3dd0e4e489ff76e78fa627c93"  width="600px" /></center> </p>
<p><center><br> scaled dot product attention </br></center><br><br></br></p>
<p>这种通过 query 和 key 的相似性程度来确定 value 的权重分布的方法被称为scaled dot-product attention。</p>
<p>用公式表达如下：</p>
<script type="math/tex; mode=display">Q=XW_{Q}</script><script type="math/tex; mode=display">K=XW_{K}</script><script type="math/tex; mode=display">V=XW_{V}</script><script type="math/tex; mode=display">X_{attention}=selfAttention(Q,K,V)</script><h4 id="2-2-2-Multi-head-Attention"><a href="#2-2-2-Multi-head-Attention" class="headerlink" title="2.2.2 Multi-head Attention"></a>2.2.2 Multi-head Attention</h4><p>不仅仅只初始化一组Q、K、V的矩阵，而是初始化多组，tranformer是使用了8组，所以最后得到的结果是8个矩阵。</p>
<p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/11e365dd4a4145b79e6258c0d77ec77cac1a1f9a5dab4b31830459b2b7cb347a"  width="600px" /></center> </p>
<p><center><br> multi-head attention </br></center><br><br></br></p>
<p>multi-head注意力的全过程如下，首先输入句子，“Thinking Machines”,在embedding模块把句子中的每个单词变成向量X，在encoder层中，除了第0层有embedding操作外，其他的层没有embedding操作；接着把X分成8个head，</p>
<p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/4d634e35dea1472d9d6946b75c14f121fd274ec19e474f86bc4a903620d87f65"  width="600px" /></center> </p>
<p><center><br> multi-head attention总体结构 </br></center><br><br></br></p>
<h4 id="2-2-3-残差连接"><a href="#2-2-3-残差连接" class="headerlink" title="2.2.3 残差连接"></a>2.2.3 残差连接</h4><p>经过 self-attention 加权之后输出，也就是Attention(Q,K,V) ，然后把他们加起来做残差连接</p>
<script type="math/tex; mode=display">X_{hidden}=X_{embedding}+self Attention(Q,K,V)</script><p>除了self-attention这里做残差连接外，feed forward那个地方也需要残差连接，公式类似：</p>
<script type="math/tex; mode=display">X_{hidden}=X_{feed_forward}+X_{hidden}</script><h4 id="2-2-4-Layer-Normalization"><a href="#2-2-4-Layer-Normalization" class="headerlink" title="2.2.4 Layer Normalization"></a>2.2.4 Layer Normalization</h4><p>Layer Normalization 的作用是把神经网络中隐藏层归一为标准正态分布，也就是独立同分布，以起到加快训练速度，加速收敛的作用</p>
<script type="math/tex; mode=display">X_{hidden}=LayerNorm(X_{hidden})</script><p>其中：$X_{hidden} \in R^{batch_size<em>seq_len</em>embed_dim}$</p>
<h4 id="2-2-5-Feed-Forward"><a href="#2-2-5-Feed-Forward" class="headerlink" title="2.2.5 Feed Forward"></a>2.2.5 Feed Forward</h4><p>将Multi-Head Attention得到的向量再投影到一个更大的空间（论文里将空间放大了4倍）在那个大空间里可以更方便地提取需要的信息（使用Relu激活函数），最后再投影回token向量原来的空间</p>
<script type="math/tex; mode=display">FFN(x)=ReLU(W_{1}x+b_{1})W_{2}+b_{2}</script><h3 id="2-3-Decoder"><a href="#2-3-Decoder" class="headerlink" title="2.3 Decoder"></a>2.3 Decoder</h3><p><img src="https://pic.imgdb.cn/item/631b040816f2c2beb1a28bf5.png" alt=""></p>
<p>和 Encoder 一样，上面三个部分的每一个部分，都有一个残差连接，后接一个 Layer Normalization。Decoder 的中间部件并不复杂，大部分在前面 Encoder 里我们已经介绍过了，但是 Decoder 由于其特殊的功能，因此在训练时会涉及到一些细节，下面会介绍Decoder的Masked Self-Attention和Encoder-Decoder Attention两部分，其结构图如下图所示</p>
<p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/08f1c94cb40f4d76aba52e7d026897177f7b6f5f69804560bbed9e576094679f"  width="600px" /></center> </p>
<p><center><br> decoder self attention </br></center><br><br></br></p>
<h4 id="2-3-1-Masked-Self-Attention"><a href="#2-3-1-Masked-Self-Attention" class="headerlink" title="2.3.1 Masked Self-Attention"></a>2.3.1 Masked Self-Attention</h4><p>传统 Seq2Seq 中 Decoder 使用的是 RNN 模型，因此在训练过程中输入因此在训练过程中输入t时刻的词，模型无论如何也看不到未来时刻的词，因为循环神经网络是时间驱动的，只有当t时刻运算结束了，才能看到t+1时刻的词。而 Transformer Decoder 抛弃了 RNN，改为 Self-Attention，由此就产生了一个问题，在训练过程中，整个 ground truth 都暴露在 Decoder 中，这显然是不对的，我们需要对 Decoder 的输入进行一些处理，该处理被称为 Mask。</p>
<p>Mask 非常简单，首先生成一个下三角全 0，上三角全为负无穷的矩阵，然后将其与 Scaled Scores 相加即可，之后再做 softmax，就能将 -inf 变为 0，得到的这个矩阵即为每个字之间的权重。</p>
<h4 id="2-3-2-Masked-Encoder-Decoder-Attention"><a href="#2-3-2-Masked-Encoder-Decoder-Attention" class="headerlink" title="2.3.2 Masked Encoder-Decoder Attention"></a>2.3.2 Masked Encoder-Decoder Attention</h4><p>其实这一部分的计算流程和前面 Masked Self-Attention 很相似，结构也一摸一样，唯一不同的是这里的K,V为 Encoder 的输出，Q为 Decoder 中 Masked Self-Attention 的输出</p>
<p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/eeb0db3260814772afdc9c1566a4afaa7133168766f34670bdecb26875edcd3f"  width="600px" /></center> </p>
<p><center><br> Masked Encoder-Decoder Attention </br></center><br><br></br></p>
<h4 id="2-3-3-Decoder的解码"><a href="#2-3-3-Decoder的解码" class="headerlink" title="2.3.3 Decoder的解码"></a>2.3.3 Decoder的解码</h4><p>下图展示了Decoder的解码过程，Decoder中的字符预测完之后，会当成输入预测下一个字符，知道遇见终止符号为止。</p>
<p><img src="../../images/pretrain_model/Transformer/transformer_decoding_2.gif" alt=""></p>
<h3 id="2-4-Transformer的最后一层和Softmax"><a href="#2-4-Transformer的最后一层和Softmax" class="headerlink" title="2.4 Transformer的最后一层和Softmax"></a>2.4 Transformer的最后一层和Softmax</h3><p>线性层是一个简单的全连接的神经网络，它将解码器堆栈生成的向量投影到一个更大的向量，称为logits向量。如图linear的输出</p>
<p>softmax层将这些分数转换为概率（全部为正值，总和为1.0）。选择概率最高的单元，并生成与其关联的单词作为此时间步的输出。</p>
<h2 id="3-模型特点——权重共享"><a href="#3-模型特点——权重共享" class="headerlink" title="3 模型特点——权重共享"></a>3 模型特点——权重共享</h2><p>Transformer在两个地方进行了权重共享：</p>
<ul>
<li>（1）Encoder和Decoder间的Embedding层权重共享；</li>
</ul>
<p>《Attention is all you need》中Transformer被应用在机器翻译任务中，源语言和目标语言是不一样的，但它们可以共用一张大词表，对于两种语言中共同出现的词（比如：数字，标点等等）可以得到更好的表示，而且对于Encoder和Decoder，嵌入时都只有对应语言的embedding会被激活，因此是可以共用一张词表做权重共享的。</p>
<p>论文中，Transformer词表用了bpe来处理，所以最小的单元是subword。英语和德语同属日耳曼语族，有很多相同的subword，可以共享类似的语义。而像中英这样相差较大的语系，语义共享作用可能不会很大。</p>
<p>但是，共用词表会使得词表数量增大，增加softmax的计算时间，因此实际使用中是否共享可能要根据情况权衡。</p>
<ul>
<li>（2）Decoder中Embedding层和FC层权重共享；</li>
</ul>
<p>Embedding层可以说是通过onehot去取到对应的embedding向量，FC层可以说是相反的，通过向量（定义为 x）去得到它可能是某个词的softmax概率，取概率最大（贪婪情况下）的作为预测值。</p>
<p>那哪一个会是概率最大的呢？在FC层的每一行量级相同的前提下，理论上和 x 相同的那一行对应的点积和softmax概率会是最大的（可类比本文问题1）。</p>
<p>因此，Embedding层和FC层权重共享，Embedding层中和向量 x 最接近的那一行对应的词，会获得更大的预测概率。实际上，Decoder中的Embedding层和FC层有点像互为逆过程。</p>
<p>通过这样的权重共享可以减少参数的数量，加快收敛。</p>
]]></content>
      <categories>
        <category>机器翻译</category>
      </categories>
      <tags>
        <tag>Attention</tag>
        <tag>Transfomer</tag>
      </tags>
  </entry>
  <entry>
    <title>VGG</title>
    <url>/2021/04/23/VGG/</url>
    <content><![CDATA[<h1 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a><a href="https://arxiv.org/pdf/1409.1556.pdf">VGG</a></h1><h2 id="1-模型介绍"><a href="#1-模型介绍" class="headerlink" title="1. 模型介绍"></a>1. 模型介绍</h2><p>随着AlexNet在2012年的ImageNet大赛上大放异彩后，卷积神经网络进入了飞速发展的阶段。2014年，由Simonyan和Zisserman提出的VGG网络在ImageNet上取得了亚军的成绩。VGG的命名来源于论文作者所在的实验室Visual Geometry Group，其对卷积神经网络进行了改良，探索了网络深度与性能的关系，用更小的卷积核和更深的网络结构，取得了较好的效果，成为了CNN发展史上较为重要的一个网络。VGG中使用了一系列大小为3x3的小尺寸卷积核和池化层构造深度卷积神经网络，因为其结构简单、应用性极强而广受研究者欢迎，尤其是它的网络结构设计方法，为构建深度神经网络提供了方向。</p>
<span id="more"></span>
<h2 id="2-模型结构"><a href="#2-模型结构" class="headerlink" title="2. 模型结构"></a>2. 模型结构</h2><p><img src="https://pic.imgdb.cn/item/6310669a16f2c2beb14bad1d.png" alt=""></p>
<p>VGG网络的设计严格使用$3×3$的卷积层和池化层来提取特征，并在网络的最后面使用三层全连接层，将最后一层全连接层的输出作为分类的预测。</p>
<p>VGG中还有一个显著特点：每次经过池化层<code>maxpooling</code>后特征图的尺寸减小一倍，而通道数增加一倍（最后一个池化层除外）。</p>
<ul>
<li>特征图的尺寸减小一倍是通过<strong>最大池化</strong></li>
<li>特征图的通道数增加一倍是通过<strong>池化后的卷积层</strong></li>
</ul>
<p>在VGG中每层卷积将使用<code>ReLU</code>作为激活函数，在全连接层之后添加<code>dropout</code>来抑制过拟合。使用小的卷积核能够有效地减少参数的个数，使得训练和测试变得更加有效。比如使用两层$3×3$ 卷积层，可以得到感受野为5的特征图，而比使用$5×5$的卷积层需要更少的参数。由于卷积核比较小，可以堆叠更多的卷积层，加深网络的深度，这对于图像分类任务来说是有利的。VGG模型的成功证明了增加网络的深度，可以更好的学习图像中的特征模式。</p>
<h2 id="3-模型实现"><a href="#3-模型实现" class="headerlink" title="3. 模型实现"></a>3. 模型实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VGG</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, num_classes=<span class="number">1000</span>, init_weight=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(VGG, self).__init__()</span><br><span class="line">        self.features = features</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">512</span>*<span class="number">7</span>*<span class="number">7</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, num_classes)</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> init_weight:</span><br><span class="line">            self._init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = torch.flatten(x, start_dim=<span class="number">1</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.xavier_normal(m.weight)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                nn.init.xavier_normal(m.weight)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_features</span>(<span class="params">cfgs: <span class="built_in">list</span></span>):</span><br><span class="line">    layers = []</span><br><span class="line">    in_channels = <span class="number">3</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> cfgs:</span><br><span class="line">        <span class="keyword">if</span> v == <span class="string">&#x27;M&#x27;</span>:</span><br><span class="line">            layers += [nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            conv2d = nn.Conv2d(in_channels=in_channels, out_channels=v, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">            layers += [conv2d, nn.ReLU(inplace=<span class="literal">True</span>)]</span><br><span class="line">            in_channels = v</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># key对应于论文的模型</span></span><br><span class="line"><span class="comment"># A -&gt; vgg11</span></span><br><span class="line"><span class="comment"># B -&gt; vgg13</span></span><br><span class="line"><span class="comment"># D -&gt; vgg16</span></span><br><span class="line"><span class="comment"># E -&gt; vgg19</span></span><br><span class="line">cfgs = &#123;</span><br><span class="line">    <span class="string">&quot;A&quot;</span>: [<span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&quot;B&quot;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">156</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&quot;D&quot;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&quot;E&quot;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">model_name=<span class="string">&#x27;D&#x27;</span>, **kwargs</span>):</span><br><span class="line">    <span class="keyword">assert</span> model_name <span class="keyword">in</span> cfgs, <span class="string">&quot;Warning: model &#123;&#125; not in cfgs dict!!!&quot;</span>.<span class="built_in">format</span>(model_name)</span><br><span class="line">    cfg = cfgs[model_name]</span><br><span class="line"></span><br><span class="line">    model = VGG(make_features(cfg), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h2 id="4-模型特点"><a href="#4-模型特点" class="headerlink" title="4. 模型特点"></a>4. 模型特点</h2><ol>
<li>整个网络都使用了同样大小的卷积核尺寸$3×3$和最大池化尺寸$2×2$</li>
<li>两个$3×3$的卷积层串联相当于1个$5×5$的卷积层，感受野大小为$5×5$。同样地，3个$3×3$的卷积层串联的效果则相当于1个$7×7$的卷积层。这样的连接方式使得网络参数量更小，而且多层的激活函数令网络对特征的学习能力更强</li>
</ol>
]]></content>
      <categories>
        <category>图像分类</category>
      </categories>
      <tags>
        <tag>模型</tag>
        <tag>backbone</tag>
      </tags>
  </entry>
  <entry>
    <title>U-Net</title>
    <url>/2021/12/27/U-Net/</url>
    <content><![CDATA[<h1 id="U-Net论文详解"><a href="#U-Net论文详解" class="headerlink" title="U-Net论文详解"></a>U-Net论文详解</h1><blockquote>
<p>U-Net结构由一个用于捕获上下文信息的压缩路径和一个支持精确定位的对称扩展路径构成。实验结果表明可以从很少的图像进行端到端的训练，并在ISBI挑战上优于先前最优的方法(滑动窗口卷积网络)，并获得了冠军</p>
</blockquote>
<span id="more"></span>
<h2 id="1-背景介绍"><a href="#1-背景介绍" class="headerlink" title="1. 背景介绍"></a>1. 背景介绍</h2><p>卷积网络的典型应用是分类任务，其中图像的输出是一个单一的类标签。然而在许多视觉任务中，特别是生物医学图像处理中，期望的输出应该包含定位，即给每一个像素点分配一个类标签。</p>
<p>于是滑动窗口卷积网络通过提供像素点周围的局部区域来预测每个像素的类别标签。但是这样的方法存在两个缺点：</p>
<ol>
<li>速度特别慢，网络必须为每一个窗口单元单独运行，并且窗口单元重合而导致大量冗余</li>
<li>在定位精度和上下文信息之间的权衡。大的窗口单元需要更多的max pooling层，这会降低精度；而小的窗口单元捕获的上下文信息较少。</li>
</ol>
<p>于是本文提出了U-Net网络</p>
<h2 id="2-U-Net网络架构"><a href="#2-U-Net网络架构" class="headerlink" title="2. U-Net网络架构"></a>2. U-Net网络架构</h2><p><img src="https://img-blog.csdnimg.cn/ee65efa0cb8e4ceabf581b6ab7a9281e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5L2V5aaC5Y2D5rO3,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<p><strong>网络是一个经典的全卷积网络。网络的输入是一张572x572经过镜像操作的图像。为了使得每次下采样后特征图的尺寸为偶数。</strong><br><img src="https://img-blog.csdnimg.cn/ab4fe06a1a854b6ebd19b575a37c0c3e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5L2V5aaC5Y2D5rO3,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<p>网络的左侧为<strong>压缩路径</strong>，由<strong>4个block</strong>构成，<strong>每个block由2个未padding的卷积和一个最大池化构成，其中每次卷积特征图的尺寸为减小2，最大池化后会缩小一半。</strong></p>
<p><strong>现在大部分采用same padding的卷积，这样就不用对输入进行镜像操作，而且在拼接压缩路径与对应的扩展路径也不用进行裁剪，而且裁剪会使得特征图不对称</strong></p>
<p>网络的右侧为<strong>扩展路径</strong>，同样由<strong>4个bloc</strong>k构成，每个block开始之前通过<strong>反卷积将特征图的尺寸扩大一倍</strong>，然后与压缩路径对应的特征图拼接，<strong>由于采用未padding的卷积，左侧压缩路径的特征图的尺寸比右侧扩展路径的特征图的大，所以需要先进行裁剪，使其大小相同，然后拼接</strong>，然后<strong>经过两次未padding的卷积</strong>进一步提取特征</p>
<p>最后根据自己的任务，输出对应大小的预测特征图</p>
<p>现在大部分采用<strong>双线性插值代替反卷积</strong>，而且效果会更好</p>
<h2 id="3-数据增强"><a href="#3-数据增强" class="headerlink" title="3. 数据增强"></a>3. 数据增强</h2><p>我们主要通过平移和旋转不变性以及灰度值的变化来增强模型的鲁棒性，特别地，<strong>任意的弹性形变对训练非常有帮助</strong>。</p>
<h2 id="4-Pytorch实现"><a href="#4-Pytorch实现" class="headerlink" title="4. Pytorch实现"></a>4. Pytorch实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.block1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        self.block2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        self.pool = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.block1(x)</span><br><span class="line">        x = self.block2(x)</span><br><span class="line">        x_pooled = self.pool(x)</span><br><span class="line">        <span class="keyword">return</span> x, x_pooled</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.up_sample = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.block1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        self.block2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x_prev, x</span>):</span><br><span class="line">        x = self.up_sample(x)</span><br><span class="line">        x_shape = x.shape[<span class="number">2</span>:]</span><br><span class="line">        x_prev_shape = x.shape[<span class="number">2</span>:]</span><br><span class="line">        h_diff = x_prev_shape[<span class="number">0</span>] - x_shape[<span class="number">0</span>]</span><br><span class="line">        w_diff = x_prev_shape[<span class="number">1</span>] - x_shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># padding</span></span><br><span class="line">        x_tmp = torch.zeros(x_prev.shape).to(x.device)</span><br><span class="line">        x_tmp[:, :, h_diff//<span class="number">2</span>: h_diff+x_shape[<span class="number">0</span>], w_diff//<span class="number">2</span>: x_shape[<span class="number">1</span>]] = x</span><br><span class="line">        x = torch.cat([x_prev, x_tmp], dim=<span class="number">1</span>)</span><br><span class="line">        x = self.block1(x)</span><br><span class="line">        x = self.block2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">UNet</span>(nn.Module):</span><br><span class="line">    <span class="comment"># https://arxiv.org/abs/1505.04597</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(UNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.down_sample1 = Encoder(in_channels=<span class="number">3</span>, out_channels=<span class="number">64</span>)</span><br><span class="line">        self.down_sample2 = Encoder(in_channels=<span class="number">64</span>, out_channels=<span class="number">128</span>)</span><br><span class="line">        self.down_sample3 = Encoder(in_channels=<span class="number">128</span>, out_channels=<span class="number">256</span>)</span><br><span class="line">        self.down_sample4 = Encoder(in_channels=<span class="number">256</span>, out_channels=<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">        self.mid1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">512</span>, <span class="number">1024</span>, <span class="number">3</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        self.mid2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1024</span>, <span class="number">1024</span>, <span class="number">3</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.up_sample1 = Decoder(in_channels=<span class="number">1024</span>, out_channels=<span class="number">512</span>)</span><br><span class="line">        self.up_sample2 = Decoder(in_channels=<span class="number">512</span>, out_channels=<span class="number">256</span>)</span><br><span class="line">        self.up_sample3 = Decoder(in_channels=<span class="number">256</span>, out_channels=<span class="number">128</span>)</span><br><span class="line">        self.up_sample4 = Decoder(in_channels=<span class="number">128</span>, out_channels=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">        self.classifier = nn.Conv2d(<span class="number">64</span>, num_classes, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x1, x = self.down_sample1(x)</span><br><span class="line">        x2, x = self.down_sample2(x)</span><br><span class="line">        x3, x = self.down_sample3(x)</span><br><span class="line">        x4, x = self.down_sample4(x)</span><br><span class="line"></span><br><span class="line">        x = self.mid1(x)</span><br><span class="line">        x = self.mid2(x)</span><br><span class="line"></span><br><span class="line">        x = self.up_sample1(x4, x)</span><br><span class="line">        x = self.up_sample2(x3, x)</span><br><span class="line">        x = self.up_sample3(x2, x)</span><br><span class="line">        x = self.up_sample4(x1, x)</span><br><span class="line"></span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="built_in">input</span> = torch.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">384</span>, <span class="number">384</span>)</span><br><span class="line">    model = UNet(<span class="number">2</span>)</span><br><span class="line">    out = model(<span class="built_in">input</span>)</span><br><span class="line">    <span class="built_in">print</span>(out.shape)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>图像分割</category>
      </categories>
      <tags>
        <tag>模型</tag>
        <tag>图像分割</tag>
      </tags>
  </entry>
  <entry>
    <title>向量距离与相似度</title>
    <url>/2022/08/30/%E5%90%91%E9%87%8F%E8%B7%9D%E7%A6%BB%E4%B8%8E%E7%9B%B8%E4%BC%BC%E5%BA%A6/</url>
    <content><![CDATA[<h1 id="向量距离与相似度"><a href="#向量距离与相似度" class="headerlink" title="向量距离与相似度"></a>向量距离与相似度</h1><p>假设当前有两个$n$维向量$x$和$y$ (除非特别说明，本文默认依此写法表示向量)，可以通过两个向量之间的距离或者相似度来判定这两个向量的相近程度，显然两个向量之间距离越小，相似度越高；两个向量之间距离越大，相似度越低。</p>
<span id="more"></span>
<h2 id="1-常见的距离计算方式"><a href="#1-常见的距离计算方式" class="headerlink" title="1. 常见的距离计算方式"></a>1. 常见的距离计算方式</h2><h3 id="1-1-闵可夫斯基距离（Minkowski-Distance）"><a href="#1-1-闵可夫斯基距离（Minkowski-Distance）" class="headerlink" title="1.1 闵可夫斯基距离（Minkowski Distance）"></a>1.1 闵可夫斯基距离（Minkowski Distance）</h3><script type="math/tex; mode=display">
Minkowski \; Distance = (\sum_{i=1}^n {|x_i - y_i|}^{p})^{\frac{1}{p}}</script><p>Minkowski Distane 是对多个距离度量公式概括性的表述，当$p=1$时，Minkowski Distane 便是曼哈顿距离；当$p=2$时，Minkowski Distane 便是欧式距离；Minkowski Distane 取极限的形式便是切比雪夫距离。</p>
<h3 id="1-2-曼哈顿距离（Manhattan-Distance）"><a href="#1-2-曼哈顿距离（Manhattan-Distance）" class="headerlink" title="1.2 曼哈顿距离（Manhattan Distance）"></a>1.2 曼哈顿距离（Manhattan Distance）</h3><script type="math/tex; mode=display">
Manhattan \; Distance = (\sum_{i=1}^n |x_i - y_i|)</script><h3 id="1-3-欧式距离-欧几里得距离（Euclidean-distance）"><a href="#1-3-欧式距离-欧几里得距离（Euclidean-distance）" class="headerlink" title="1.3 欧式距离/欧几里得距离（Euclidean distance）"></a>1.3 欧式距离/欧几里得距离（Euclidean distance）</h3><script type="math/tex; mode=display">
Euclidean \; Distance = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}</script><h3 id="1-4-切比雪夫距离（Chebyshev-Distance）"><a href="#1-4-切比雪夫距离（Chebyshev-Distance）" class="headerlink" title="1.4 切比雪夫距离（Chebyshev Distance）"></a>1.4 切比雪夫距离（Chebyshev Distance）</h3><script type="math/tex; mode=display">
\underset{p \rightarrow \infty}{\text{lim}} (\sum_{i=1}^n {|x_i - y_i|}^{p})^{\frac{1}{p}} = \text{max} \; (|x_i-y_i|)</script><h3 id="1-5-海明距离（Hamming-Distance）"><a href="#1-5-海明距离（Hamming-Distance）" class="headerlink" title="1.5 海明距离（Hamming Distance）"></a>1.5 海明距离（Hamming Distance）</h3><p> 在信息论中，两个等长字符串之间的海明距离是两个字符串对应位置的不同字符的个数。假设有两个字符串分别是：$x=[x_1,x_2,…,x_n]$和$y=[y_1,y_2,…,y_n]$，则两者的距离为：</p>
<script type="math/tex; mode=display">
Hamming \; Distance  = \sum_{i=1}^{n} {\text{II}}(x_i=y_i)</script><p>其中$\text{II}$表示指示函数，两者相同为1，否则为0。</p>
<h3 id="1-6-KL散度"><a href="#1-6-KL散度" class="headerlink" title="1.6 KL散度"></a>1.6 KL散度</h3><p>在信息论中，随机变量$X$的熵表示如下：</p>
<script type="math/tex; mode=display">H=-\sum_{i=1}^Np(x_i)\cdot logp(x_i)</script><p>给定随机变量$X$和两个概率分布$P$和$Q$，KL散度可以用来衡量两个分布之间的差异性，又叫相对熵。其公式如下：</p>
<script type="math/tex; mode=display">
KL(P||Q)= \sum_{x \in X} p(x)log\,\frac{P(x)}{Q(x)}</script><h2 id="2-常见的相似度函数"><a href="#2-常见的相似度函数" class="headerlink" title="2. 常见的相似度函数"></a>2. 常见的相似度函数</h2><h3 id="2-1-余弦相似度（Cosine-Similarity）"><a href="#2-1-余弦相似度（Cosine-Similarity）" class="headerlink" title="2.1 余弦相似度（Cosine Similarity）"></a>2.1 余弦相似度（Cosine Similarity）</h3><script type="math/tex; mode=display">
\begin{align}
Cosine \; Similarity = \frac{x \cdot y}{|x|\cdot |y|} = \frac{\sum_{i=1}^n x_iy_i}{\sqrt{\sum_{i=1}^n x_i^2}\sqrt{\sum_{i=1}^n y_i^2}}
\end{align}</script><h3 id="2-2-皮尔逊相关系数-（Pearson-Correlation-Coefficient）"><a href="#2-2-皮尔逊相关系数-（Pearson-Correlation-Coefficient）" class="headerlink" title="2.2 皮尔逊相关系数 （Pearson Correlation Coefficient）"></a>2.2 皮尔逊相关系数 （Pearson Correlation Coefficient）</h3><p>给定两个随机变量$X$和$Y$，皮尔逊相关系数可以用来衡量两者的相关程度，公式如下:</p>
<script type="math/tex; mode=display">
\begin{align}
\rho_{x,y} &= \frac{cov(X,Y)}{\sigma_X \sigma_Y} = \frac{E[(X-\mu_X)(Y-\mu_Y)]}{\sigma_X \sigma_Y} \\
& = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i-\bar{Y})}{\sqrt{\sum_{i=1}^n(X_i-\bar{X})^2}\sqrt{\sum_{i=1}^n(Y_i-\bar{Y})^2}}
\end{align}</script><p>其中$\mu_X$和$\mu_Y$分别表示向量$X$和$Y$的均值，$\sigma_X$和$\sigma_Y$分别表示向量$X$和$Y$的标准差。</p>
<h3 id="2-3-Jaccard-相似系数（Jaccard-Coefficient）"><a href="#2-3-Jaccard-相似系数（Jaccard-Coefficient）" class="headerlink" title="2.3 Jaccard 相似系数（Jaccard Coefficient）"></a>2.3 Jaccard 相似系数（Jaccard Coefficient）</h3><p>假设有两个集合$X$和$Y$(注意这里的两者不是向量)，则其计算公式为：</p>
<script type="math/tex; mode=display">
Jaccard(X,Y)=\frac{X\cap Y}{X\cup Y}</script>]]></content>
      <categories>
        <category>深度学习基础</category>
      </categories>
      <tags>
        <tag>向量</tag>
        <tag>相似度</tag>
      </tags>
  </entry>
  <entry>
    <title>图像分割损失函数汇总</title>
    <url>/2022/08/29/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<h1 id="图像分割损失函数汇总"><a href="#图像分割损失函数汇总" class="headerlink" title="图像分割损失函数汇总"></a>图像分割损失函数汇总</h1><p>在本文中，总结了大多数广泛应用于图像分割的损失函数，并通过<code>Pytorch</code>框架进行了实现。<br><span id="more"></span></p>
<h2 id="1-BCELoss："><a href="#1-BCELoss：" class="headerlink" title="1. BCELoss："></a>1. BCELoss：</h2><script type="math/tex; mode=display">l(x,y)=L=\{l_1,...,l_N\}^T</script><script type="math/tex; mode=display">l_n=-w_n[y_n \cdot logx_n + (1-y_n) \cdot log(1-x_n)]</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BCELoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 weight=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 pos_weight=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 ignore_index=<span class="number">255</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param weight: A manual rescaling weight given to the loss of each batch element</span></span><br><span class="line"><span class="string">        :param pos_weight: A weight of positive examples</span></span><br><span class="line"><span class="string">        :param ignore_index: Specifies a target value that is ignored and does not contribute to the input gradient</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(BCELoss, self).__init__()</span><br><span class="line">        self.weight = weight</span><br><span class="line">        self.pos_weight = pos_weight</span><br><span class="line">        self.ignore_index = ignore_index</span><br><span class="line">        self.EPS = <span class="number">1e-10</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, logit, label</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param logit: Logit tensor, the data type is float32, float64. Shape is</span></span><br><span class="line"><span class="string">                (N, C), where C is number of classes, and if shape is more than 2D, this</span></span><br><span class="line"><span class="string">                is (N, C, D1, D2,..., Dk), k &gt;= 1.</span></span><br><span class="line"><span class="string">        :param label: Label tensor, the data type is int64. Shape is (N, C), where each</span></span><br><span class="line"><span class="string">                value is 0 or 1, and if shape is more than 2D, this is</span></span><br><span class="line"><span class="string">                (N, C, D1, D2,..., Dk), k &gt;= 1.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(label.shape) != <span class="built_in">len</span>(logit.shape):</span><br><span class="line">            label = torch.unsqueeze(label, dim=<span class="number">1</span>)</span><br><span class="line">        mask = (label != self.ignore_index)</span><br><span class="line">        mask = mask.to(dtype=torch.float32)</span><br><span class="line">        <span class="keyword">if</span> label.shape[<span class="number">1</span>] != logit.shape[<span class="number">1</span>]:</span><br><span class="line">            label = label.squeeze(<span class="number">1</span>)</span><br><span class="line">            label = F.one_hot(label, logit.shape[<span class="number">1</span>])</span><br><span class="line">            label = torch.permute(label, dims=(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        label = label.to(dtype=torch.float32)</span><br><span class="line">        loss = F.binary_cross_entropy_with_logits(</span><br><span class="line">            <span class="built_in">input</span>=logit,</span><br><span class="line">            target=label,</span><br><span class="line">            weight=self.weight,</span><br><span class="line">            pos_weight=self.pos_weight,</span><br><span class="line">            reduction=<span class="string">&#x27;none&#x27;</span></span><br><span class="line">        )</span><br><span class="line">        loss = loss * mask</span><br><span class="line">        loss = torch.mean(loss) / (torch.mean(mask) + self.EPS)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    inp = torch.randn(<span class="number">4</span>, <span class="number">21</span>, <span class="number">448</span>, <span class="number">448</span>)</span><br><span class="line">    tgt = torch.randint(low=<span class="number">0</span>, high=<span class="number">20</span>, size=(<span class="number">4</span>, <span class="number">448</span>, <span class="number">448</span>))</span><br><span class="line">    loss = BCELoss()</span><br><span class="line">    <span class="built_in">print</span>(loss(inp, tgt))</span><br></pre></td></tr></table></figure>
<h2 id="2-CrossEntropyLoss"><a href="#2-CrossEntropyLoss" class="headerlink" title="2.CrossEntropyLoss:"></a>2.CrossEntropyLoss:</h2><script type="math/tex; mode=display">l(x, y)= L = \{l_1, ..., l_N\}^T</script><script type="math/tex; mode=display">l_n=-w_{y_n}log \frac {exp(x_{n, y_n})} {\sum_{c=1}^C exp(x_{n,c})} \cdot 1\{y_n \not = ignore\_index\}</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CELoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 weight=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 ignore_index=<span class="number">255</span>,</span></span><br><span class="line"><span class="params">                 top_k_percent_pixels=<span class="number">1.0</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param weight: (tuple|list|ndarray|Tensor, optional): A manual rescaling weight</span></span><br><span class="line"><span class="string">            given to each class. Its length must be equal to the number of classes.</span></span><br><span class="line"><span class="string">        :param ignore_index: (int64, optional): Specifies a target value that is ignored</span></span><br><span class="line"><span class="string">            and does not contribute to the input gradient. Default ``255``.</span></span><br><span class="line"><span class="string">        :param top_k_percent_pixels: (float, optional): the value lies in [0.0, 1.0].</span></span><br><span class="line"><span class="string">            When its value &lt; 1.0, only compute the loss for the top k percent pixels</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(CELoss, self).__init__()</span><br><span class="line">        self.weight = weight</span><br><span class="line">        self.ignore_index = ignore_index</span><br><span class="line">        self.top_k_percent_pixels = top_k_percent_pixels</span><br><span class="line">        self.EPS = <span class="number">1e-8</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, logit, label, semantic_weights=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param logit: Logit tensor, the data type is float32, float64. Shape is</span></span><br><span class="line"><span class="string">                (N, C), where C is number of classes, and if shape is more than 2D, this</span></span><br><span class="line"><span class="string">                is (N, C, D1, D2,..., Dk), k &gt;= 1.</span></span><br><span class="line"><span class="string">        :param label: Label tensor, the data type is int64. Shape is (N), where each</span></span><br><span class="line"><span class="string">                value is 0 &lt;= label[i] &lt;= C-1, and if shape is more than 2D, this is</span></span><br><span class="line"><span class="string">                (N, D1, D2,..., Dk), k &gt;= 1.</span></span><br><span class="line"><span class="string">        :param semantic_weights: Weights about loss for each pixels,</span></span><br><span class="line"><span class="string">                shape is the same as label. Default: None.</span></span><br><span class="line"><span class="string">        :return: (Tensor): The average loss.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> self.weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.weight = torch.tensor(self.weight, dtype=torch.float32, device=logit.device)</span><br><span class="line">            <span class="keyword">if</span> logit.shape[<span class="number">1</span>] != <span class="built_in">len</span>(self.weight):</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">&quot;The number of weights = &#123;&#125; must be the same as the number of classes = &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(self.weight), logit.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">        label = label.to(dtype=torch.int64)</span><br><span class="line">        loss = F.cross_entropy(</span><br><span class="line">            <span class="built_in">input</span>=logit,</span><br><span class="line">            target=label,</span><br><span class="line">            ignore_index=self.ignore_index,</span><br><span class="line">            reduction=<span class="string">&#x27;none&#x27;</span>,</span><br><span class="line">            weight=self.weight</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> self._post_process_loss(logit, label, semantic_weights, loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_post_process_loss</span>(<span class="params">self, logit, label, semantic_weights, loss</span>):</span><br><span class="line">        mask = label != self.ignore_index</span><br><span class="line">        mask = mask.to(dtype=torch.float32)</span><br><span class="line">        <span class="keyword">if</span> loss.ndim &gt; mask.ndim:</span><br><span class="line">            loss = torch.squeeze(loss, dim=-<span class="number">1</span>)</span><br><span class="line">        loss = loss * mask</span><br><span class="line">        <span class="keyword">if</span> semantic_weights <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            loss = loss * semantic_weights</span><br><span class="line">        <span class="keyword">if</span> self.weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            _one_hot = F.one_hot(label * mask, logit.shape[<span class="number">1</span>])</span><br><span class="line">            coef = torch.<span class="built_in">sum</span>(_one_hot * self.weight, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            coef = torch.ones_like(label)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.top_k_percent_pixels == <span class="number">1.0</span>:</span><br><span class="line">            avg_loss = torch.mean(loss) / (torch.mean(mask * coef) + self.EPS)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            loss = loss.reshape((-<span class="number">1</span>, ))</span><br><span class="line">            top_k_pixels = <span class="built_in">int</span>(self.top_k_percent_pixels * loss.numel())</span><br><span class="line">            loss, indices = torch.topk(loss, top_k_pixels)</span><br><span class="line">            coef = coef.reshape((-<span class="number">1</span>, ))</span><br><span class="line">            coef = torch.gather(coef, dim=<span class="number">0</span>, index=indices)</span><br><span class="line">            coef = coef.to(dtype=torch.float32)</span><br><span class="line">            coef.requires_grad = <span class="literal">True</span></span><br><span class="line">            avg_loss = loss.mean() / (torch.mean(coef) + self.EPS)</span><br><span class="line">        <span class="keyword">return</span> avg_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    inp = torch.randn(<span class="number">4</span>, <span class="number">21</span>, <span class="number">448</span>, <span class="number">448</span>)</span><br><span class="line">    tgt = torch.randint(low=<span class="number">0</span>, high=<span class="number">20</span>, size=(<span class="number">4</span>, <span class="number">448</span>, <span class="number">448</span>))</span><br><span class="line">    loss = CELoss(top_k_percent_pixels=<span class="number">0.8</span>)</span><br><span class="line">    <span class="built_in">print</span>(loss(inp, tgt))</span><br></pre></td></tr></table></figure>
<h2 id="3-Focal-Loss"><a href="#3-Focal-Loss" class="headerlink" title="3. Focal Loss:"></a>3. Focal Loss:</h2><p><img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-07_at_4.45.06_PM_leJm2yh.png" alt=""></p>
<script type="math/tex; mode=display">CE(p_t) = -log(p_t)</script><script type="math/tex; mode=display">FL(p_t)=-\alpha(1-p_t)^\gamma log(p_t)</script><p>其中：$p_t=\frac {exp(x_{n, t})} {\sum_{c=1}^C exp(x_{n,c})}$表示样本属于<code>true class</code>的概率，$\alpha , \gamma$是超参数。</p>
<p>通过添加$(1-p_t)^\gamma$的目的是：<strong>减少易分类样本的权重，从而使得模型在训练时更专注于难分类的样本。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FocalLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, alpha=<span class="number">1.0</span>, gamma=<span class="number">2.0</span>, ignore_index=<span class="number">255</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param alpha: The alpha of Focal Loss</span></span><br><span class="line"><span class="string">        :param gamma: The gamma of Focal Loss</span></span><br><span class="line"><span class="string">        :param ignore_index: Specifies a target value that is ignored and does not contribute to the input gradient</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(FocalLoss, self).__init__()</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.ignore_index = ignore_index</span><br><span class="line">        self.EPS = <span class="number">1e-10</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, logit, label</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param logit: Logit tensor, the data type is float32, float64. Shape is</span></span><br><span class="line"><span class="string">                (N, C, H, W), where C is number of classes.</span></span><br><span class="line"><span class="string">        :param label: Label tensor, the data type is int64. Shape is (N, H, W),</span></span><br><span class="line"><span class="string">                where each value is 0 &lt;= label[i] &lt;= C-1.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> logit.ndim == <span class="number">4</span>, <span class="string">&quot;The ndim of logit should be 4&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> label.ndim == <span class="number">3</span>, <span class="string">&quot;The ndim of label should be 3&quot;</span></span><br><span class="line">        label = label.to(dtype=torch.int64)</span><br><span class="line">        ce_loss = F.cross_entropy(</span><br><span class="line">            <span class="built_in">input</span>=logit,</span><br><span class="line">            target=label,</span><br><span class="line">            ignore_index=self.ignore_index,</span><br><span class="line">            reduction=<span class="string">&#x27;none&#x27;</span></span><br><span class="line">        )</span><br><span class="line">        pt = torch.exp(-ce_loss)</span><br><span class="line">        focal_loss = self.alpha * ((<span class="number">1</span> - pt) ** self.gamma) * ce_loss</span><br><span class="line">        mask = label != self.ignore_index</span><br><span class="line">        mask = mask.to(dtype=torch.float32)</span><br><span class="line">        focal_loss *= mask</span><br><span class="line">        avg_loss = torch.mean(focal_loss) / (torch.mean(mask) + self.EPS)</span><br><span class="line">        <span class="keyword">return</span> avg_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    inp = torch.randn(<span class="number">4</span>, <span class="number">21</span>, <span class="number">448</span>, <span class="number">448</span>)</span><br><span class="line">    tgt = torch.randint(low=<span class="number">0</span>, high=<span class="number">20</span>, size=(<span class="number">4</span>, <span class="number">448</span>, <span class="number">448</span>))</span><br><span class="line">    loss = FocalLoss()</span><br><span class="line">    <span class="built_in">print</span>(loss(inp, tgt))</span><br></pre></td></tr></table></figure>
<h2 id="4-Dice-Loss"><a href="#4-Dice-Loss" class="headerlink" title="4. Dice Loss"></a>4. Dice Loss</h2><script type="math/tex; mode=display">L_{dice}=1-\frac {2|X ∩ Y|} {|X|+|Y|}</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DiceLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, weight=<span class="literal">None</span>, ignore_index=<span class="number">255</span>, smooth=<span class="number">1.0</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param weight: The weight for each class</span></span><br><span class="line"><span class="string">        :param ignore_index: pecifies a target value that is ignored and does not contribute to the input gradient</span></span><br><span class="line"><span class="string">        :param smooth: Laplace smoothing to smooth dice loss and accelerate convergence</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(DiceLoss, self).__init__()</span><br><span class="line">        self.weight = weight</span><br><span class="line">        self.ignore_index = ignore_index</span><br><span class="line">        self.smooth = smooth</span><br><span class="line">        self.EPS = <span class="number">1e-8</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, logits, labels</span>):</span><br><span class="line">        num_classes = logits.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> self.weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.weight = torch.tensor(self.weight, dtype=torch.float32, device=logits.device)</span><br><span class="line">            <span class="keyword">if</span> num_classes != <span class="built_in">len</span>(self.weight):</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">&quot;The length of weight = &#123;&#125; should be same as the length of num_classes = &#123;&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">                    <span class="built_in">len</span>(self.weight), num_classes</span><br><span class="line">                ))</span><br><span class="line">        mask = labels != self.ignore_index</span><br><span class="line">        mask = mask.to(dtype=torch.long)</span><br><span class="line"></span><br><span class="line">        labels[labels == self.ignore_index] = <span class="number">0</span></span><br><span class="line">        labels_one_hot = F.one_hot(labels, num_classes)</span><br><span class="line">        labels_one_hot = torch.permute(labels_one_hot, dims=(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        logits = F.softmax(logits, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        dice_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_classes):</span><br><span class="line">            dice_loss_i = dice_loss_helper(logits[:, i], labels_one_hot[:, i], mask, self.smooth, self.EPS)</span><br><span class="line">            <span class="keyword">if</span> self.weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                dice_loss_i *= self.weight[i]</span><br><span class="line">            dice_loss += dice_loss_i</span><br><span class="line">        dice_loss /= num_classes</span><br><span class="line">        <span class="keyword">return</span> dice_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dice_loss_helper</span>(<span class="params">logit, label, mask, smooth, eps</span>):</span><br><span class="line">    <span class="keyword">assert</span> logit.shape == label.shape, <span class="string">&quot;The shape of logit and label should be the same&quot;</span></span><br><span class="line">    num = logit.shape[<span class="number">0</span>]</span><br><span class="line">    logit = torch.reshape(logit, shape=(num, -<span class="number">1</span>))</span><br><span class="line">    label = torch.reshape(label, shape=(num, -<span class="number">1</span>))</span><br><span class="line">    mask = torch.reshape(mask, shape=(num, -<span class="number">1</span>))</span><br><span class="line">    logit *= mask</span><br><span class="line">    label *= mask</span><br><span class="line">    intersection = torch.<span class="built_in">sum</span>(logit * label, dim=<span class="number">1</span>)</span><br><span class="line">    union = torch.<span class="built_in">sum</span>(logit + label, dim=<span class="number">1</span>)</span><br><span class="line">    dice_loss = <span class="number">1</span> - (<span class="number">2</span> * intersection + smooth) / (union + smooth + eps)</span><br><span class="line">    dice_loss = dice_loss.mean()</span><br><span class="line">    <span class="keyword">return</span> dice_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    inp = torch.randn(<span class="number">4</span>, <span class="number">20</span>, <span class="number">448</span>, <span class="number">448</span>)</span><br><span class="line">    tgt = torch.randint(low=<span class="number">0</span>, high=<span class="number">19</span>, size=(<span class="number">4</span>, <span class="number">448</span>, <span class="number">448</span>))</span><br><span class="line">    loss = DiceLoss()</span><br><span class="line">    <span class="built_in">print</span>(loss(inp, tgt))</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>图像分割</category>
      </categories>
      <tags>
        <tag>图像分割</tag>
        <tag>损失函数</tag>
      </tags>
  </entry>
  <entry>
    <title>评估指标</title>
    <url>/2022/08/30/%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/</url>
    <content><![CDATA[<h1 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h1><p>常见评价指标有精度、精确率、召回率、P-R曲线、F1 值、TPR、FPR、ROC、AUC等指标，还有在生物领域常用的敏感性、特异性等指标。</p>
<span id="more"></span>
<h2 id="1-图像分类评估指标"><a href="#1-图像分类评估指标" class="headerlink" title="1. 图像分类评估指标"></a>1. 图像分类评估指标</h2><p>在分类任务中，各指标的计算基础都来自于对正负样本的分类结果，用混淆矩阵表示，如 <strong>图1</strong> 所示：</p>
<center><img src="https://raw.githubusercontent.com/w5688414/paddleImage/main/metrics_img/confusion_metric.png" width="500" hegiht="" ></center>
<center><br>图1 混淆矩阵 </br></center>

<h3 id="1-1-精度"><a href="#1-1-精度" class="headerlink" title="1.1 精度"></a>1.1 精度</h3><script type="math/tex; mode=display">Accuracy=\frac{TP+TN}{TP+FN+FP+TN}</script><p>即所有分类正确的样本占全部样本的比例。</p>
<h3 id="1-2-精确率"><a href="#1-2-精确率" class="headerlink" title="1.2 精确率"></a>1.2 精确率</h3><p>精准率又叫做：Precision、查准率</p>
<script type="math/tex; mode=display">Precision=\frac{TP}{TP+FP}</script><p>即预测是正例的结果中，确实是正例的比例。</p>
<h3 id="1-3-召回率"><a href="#1-3-召回率" class="headerlink" title="1.3 召回率"></a>1.3 召回率</h3><p>召回率又叫：Recall、查全率</p>
<script type="math/tex; mode=display">Recall=\frac{TP}{TP+FN}</script><p>即所有正例的样本中，被找出的比例</p>
<h3 id="1-4-P-R曲线"><a href="#1-4-P-R曲线" class="headerlink" title="1.4 P-R曲线"></a>1.4 P-R曲线</h3><p>P-R曲线又叫做：PRC</p>
<center><img src="https://raw.githubusercontent.com/w5688414/paddleImage/main/metrics_img/PRC.png" width="500" hegiht="" ></center>
<center><br>图2 PRC曲线图</br></center>

<p>根据预测结果将预测样本排序，最有可能为正样本的在前，最不可能的在后，依次将样本预测为正样本，分别计算当前的精确率和召回率，绘制P-R曲线。</p>
<h3 id="1-5-F1-值"><a href="#1-5-F1-值" class="headerlink" title="1.5 F1 值"></a>1.5 F1 值</h3><script type="math/tex; mode=display">F1=\frac{2 * P * R}{P + R}</script><h3 id="1-6-TPR"><a href="#1-6-TPR" class="headerlink" title="1.6 TPR"></a>1.6 TPR</h3><p>真正例率，与召回率相同</p>
<script type="math/tex; mode=display">TPR=\frac{TP}{TP+FN}</script><h3 id="1-7-FPR"><a href="#1-7-FPR" class="headerlink" title="1.7 FPR"></a>1.7 FPR</h3><p>假正例率</p>
<script type="math/tex; mode=display">FPR=\frac{FP}{TN+FP}</script><h3 id="1-8-ROC"><a href="#1-8-ROC" class="headerlink" title="1.8 ROC"></a>1.8 ROC</h3><p>根据预测结果将预测样本排序，最有可能为正样本的在前，最不可能的在后，依次将样本预测为正样本，分别计算当前的TPR和FPR，绘制ROC曲线。</p>
<h3 id="1-9-AUC"><a href="#1-9-AUC" class="headerlink" title="1.9 AUC"></a>1.9 AUC</h3><p>Area Under ROC Curve，ROC曲线下的面积：</p>
<center><img src="https://raw.githubusercontent.com/w5688414/paddleImage/main/metrics_img/AUC.png" width="500" hegiht="" ></center>
<center><br>图3 ROC曲线图</br></center>

<h3 id="1-10-敏感性"><a href="#1-10-敏感性" class="headerlink" title="1.10 敏感性"></a>1.10 敏感性</h3><p>敏感性或者灵敏度（Sensitivity，也称为真阳性率）是指实际为阳性的样本中，判断为阳性的比例（例如真正有生病的人中，被医院判断为有生病者的比例），计算方式是真阳性除以真阳性+假阴性（实际为阳性，但判断为阴性）的比值（能将实际患病的病例正确地判断为患病的能力，即患者被判为阳性的概率）。公式如下：</p>
<script type="math/tex; mode=display">sensitivity =\frac{TP}{TP + FN}</script><p>即有病（阳性）人群中，检测出阳性的几率。（检测出确实有病的能力）</p>
<h3 id="1-11-特异性"><a href="#1-11-特异性" class="headerlink" title="1.11 特异性"></a>1.11 特异性</h3><p>特异性或特异度（Specificity，也称为真阴性率）是指实际为阴性的样本中，判断为阴性的比例（例如真正未生病的人中，被医院判断为未生病者的比例），计算方式是真阴性除以真阴性+假阳性（实际为阴性，但判断为阳性）的比值（能正确判断实际未患病的病例的能力，即试验结果为阴性的比例）。公式如下：</p>
<script type="math/tex; mode=display">specificity =\frac{TN}{TN + FP}</script><p>即无病（阴性）人群中，检测出阴性的几率。（检测出确实没病的能力）</p>
<h2 id="2-目标检测评估指标"><a href="#2-目标检测评估指标" class="headerlink" title="2. 目标检测评估指标"></a>2. 目标检测评估指标</h2><h2 id="3-图像分割评估指标"><a href="#3-图像分割评估指标" class="headerlink" title="3. 图像分割评估指标"></a>3. 图像分割评估指标</h2><h2 id="4-OCR评估指标"><a href="#4-OCR评估指标" class="headerlink" title="4. OCR评估指标"></a>4. OCR评估指标</h2><h2 id="5-GAN评估指标"><a href="#5-GAN评估指标" class="headerlink" title="5. GAN评估指标"></a>5. GAN评估指标</h2>]]></content>
      <categories>
        <category>深度学习基础</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>模型评估</tag>
      </tags>
  </entry>
  <entry>
    <title>MobileNet</title>
    <url>/2021/10/30/MobileNet/</url>
    <content><![CDATA[<h1 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a>MobileNet</h1><h2 id="1-模型介绍"><a href="#1-模型介绍" class="headerlink" title="1. 模型介绍"></a>1. 模型介绍</h2><p>移动模型已经建立在越来越高效的构建块上。 MobileNetV1引入了深度方向可分离卷积作为传统卷积层的有效替代。 深度可分离卷积通过将空间滤波与特征生成机制分离，有效地分解了传统卷积。深度可分离卷积由两个单独的层定义：用于空间滤波的轻量级深度卷积和用于特征生成的1x1点卷积。<br><span id="more"></span><br>MobileNetV2引入了线性瓶颈和倒置残差结构，以通过利用问题的低秩性质来提高层结构的效率。由1x1扩展卷积、深度卷积和1x1投影卷积层定义。当且仅当输入和输出具有相同数量的通道时，才通过残差连接进行连接。这种结构在输入和输出处保持紧凑的表示形式，同时在内部扩展到更高维度的特征空间以提高非线性每通道变换的表达能力。</p>
<p>对于MobileNetV3，我们使用这些层的组合作为构建块，以构建最有效的模型。层也通过修改的swish非线性进行了升级。SE以及swish非线性都使用Sigmoid形，这在定点算术中计算效率低下，而且难以保持精度，因此我们将其替换hard sigmoid</p>
<h2 id="2-模型结构及实现"><a href="#2-模型结构及实现" class="headerlink" title="2. 模型结构及实现"></a>2. 模型结构及实现</h2><h3 id="2-1-Mobile-Net-V1"><a href="#2-1-Mobile-Net-V1" class="headerlink" title="2.1 Mobile Net V1"></a>2.1 <a href="https://arxiv.org/pdf/1704.04861">Mobile Net V1</a></h3><p>本文主要提出了<strong>深度可分离卷积</strong>，即一个标准卷积分解为一个<code>Depthwise Conv</code>和一个<code>Pointwise Conv</code></p>
<ul>
<li><code>Depthwise Conv</code>：本质为分组个数为卷积核个数的分组卷积</li>
<li><code>Pointwise Conv</code>: 1 × 1 卷积<br>如下图：<br><img src="https://pic.imgdb.cn/item/6311b8a516f2c2beb1f4692f.png" alt=""></li>
</ul>
<p>标准卷积与深度可分离卷积计算量比较：</p>
<p><code>input feature map</code>：$D_F \times D_F \times M$</p>
<p><code>output feature map</code>：$D_G \times D_G \times N$</p>
<p><code>conv kernel size</code>：$D_K \times D_k \times M \times N$</p>
<ul>
<li>$D_F$ ：输入特征图的宽度和高度的大小</li>
<li>$D_G$ ：输入特征图的宽度和高度的大小</li>
<li>M：输入特征图的个数</li>
<li>N：输出特征图的个数</li>
<li>$D_K$：卷积核的大小</li>
</ul>
<p>标准卷积的计算量：</p>
<p>对于每一个像素点的计算量为：$D_K \times D_K \times M$，共有$D_G \times D_G \times N$个像素点，所以总计算量为：$D_K \times D_K \times M \times N \times D_G \times D_G$</p>
<p>深度可分离卷积的计算量：</p>
<ol>
<li><code>depthwise conv</code>：对于每一个像素点的计算量为：$D_K \times D_K$，共有$D_G \times D_G \times M$个像素点，所以总计算量为：$D_K \times D_K \times M \times D_G \times D_G$</li>
<li><code>pointwise conv</code>：计算量为：$1 \times 1 \times M \times N \times D_G \times D_G$</li>
</ol>
<script type="math/tex; mode=display">\frac {深度可分离卷积计算量} {标准卷积计算量} = \frac {D_K \times D_K \times M \times D_G \times D_G + M \times N \times D_G \times D_G } {D_K \times D_K \times M \times N \times D_G \times D_G} = \frac {1} {N} + \frac {1} {D_K^2}</script><p>代码实现<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Depthwise conv + Pointwise conv&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_planes, out_planes, stride=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Block, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size=<span class="number">3</span>, stride=stride, </span><br><span class="line">             padding=<span class="number">1</span>, groups=in_planes, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(in_planes)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=<span class="number">1</span>, </span><br><span class="line">            stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_planes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        out = F.relu(self.bn2(self.conv2(out)))</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure></p>
<h3 id="2-2-Mobile-Net-V2"><a href="#2-2-Mobile-Net-V2" class="headerlink" title="2.2 Mobile Net V2"></a>2.2 <a href="https://arxiv.org/pdf/1801.04381">Mobile Net V2</a></h3><p>本文针对于<code>ReLU</code>在低维度是会对特征有损失，于是提出了<code>inverted residual block</code>模块：先升维再降至低维度且不在使用非线性的RELU作为激活函数<br><img src="https://pic.imgdb.cn/item/6311ce9416f2c2beb1035f00.png" alt=""><br><img src="https://pic.imgdb.cn/item/6311d00016f2c2beb10457b9.png" alt=""><br><img src="https://pic.imgdb.cn/item/6311d05816f2c2beb1048443.png" alt=""><br>代码实现：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBNReLu</span>(nn.Sequential):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, groups=<span class="number">1</span></span>):</span><br><span class="line">        padding = (kernel_size - <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">        <span class="built_in">super</span>(ConvBNReLu, self).__init__(</span><br><span class="line">            nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channel),</span><br><span class="line">            nn.ReLU6(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride, expand_ratio</span>):</span><br><span class="line">        <span class="built_in">super</span>(InvertedResidual, self).__init__()</span><br><span class="line">        hidden_channel = in_channel * expand_ratio</span><br><span class="line">        self.use_shortcut = stride == <span class="number">1</span> <span class="keyword">and</span> in_channel == out_channel</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">if</span> expand_ratio != <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 1x1 pointWise conv</span></span><br><span class="line">            layers.append(ConvBNReLu(in_channel, hidden_channel, kernel_size=<span class="number">1</span>))</span><br><span class="line">        layers.extend([</span><br><span class="line">            <span class="comment"># 3x3 depthWise conv</span></span><br><span class="line">            ConvBNReLu(hidden_channel, hidden_channel, stride=stride, groups=hidden_channel),</span><br><span class="line">            <span class="comment"># 1x1 pointWise conv(linear)</span></span><br><span class="line">            nn.Conv2d(in_channels=hidden_channel, out_channels=out_channel, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channel),</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">        self.conv = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> self.use_shortcut:</span><br><span class="line">            <span class="keyword">return</span> x + self.conv(x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.conv(x)</span><br></pre></td></tr></table></figure></p>
<h3 id="2-3-Mobile-Net-V3"><a href="#2-3-Mobile-Net-V3" class="headerlink" title="2.3 Mobile Net V3"></a>2.3 <a href="https://arxiv.org/pdf/1905.02244">Mobile Net V3</a></h3><p>本文在此前基础上增加了SE注意力机制，以及<code>h-swish</code>激活函数：$h-swish[x]=x \frac {ReLU6(x+3)} {6}$</p>
<p><img src="https://pic.imgdb.cn/item/6311d17016f2c2beb10515aa.png" alt=""></p>
<p>模块代码实现：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">HardSwish</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplace=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(HardSwish, self).__init__()</span><br><span class="line">        self.relu6 = nn.ReLU6(inplace=inplace)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x * self.relu6(x+<span class="number">3</span>)/<span class="number">6</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBNActivation</span>(nn.Sequential):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, kernel_size, stride, groups, activate</span>):</span><br><span class="line">        padding = (kernel_size - <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">        <span class="built_in">super</span>(ConvBNActivation, self).__init__(</span><br><span class="line">            nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channel),</span><br><span class="line">            nn.ReLU6(inplace=<span class="literal">True</span>) <span class="keyword">if</span> activate == <span class="string">&#x27;relu&#x27;</span> <span class="keyword">else</span> HardSwish()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SqueezeAndExcite</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, divide=<span class="number">4</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SqueezeAndExcite, self).__init__()</span><br><span class="line">        mid_channel = in_channel // divide</span><br><span class="line">        self.pool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.SEblock = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=in_channel, out_features=mid_channel),</span><br><span class="line">            nn.ReLU6(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(in_features=mid_channel, out_features=out_channel),</span><br><span class="line">            HardSwish(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        b, c, h, w = x.size()</span><br><span class="line">        out = self.pool(x)</span><br><span class="line">        out = torch.flatten(out, start_dim=<span class="number">1</span>)</span><br><span class="line">        out = self.SEblock(out)</span><br><span class="line">        out = out.view(b, c, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out * x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SEInverteBottleneck</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, mid_channel, out_channel, kernel_size, use_se, activate, stride</span>):</span><br><span class="line">        <span class="built_in">super</span>(SEInverteBottleneck, self).__init__()</span><br><span class="line">        self.use_shortcut = stride == <span class="number">1</span> <span class="keyword">and</span> in_channel == out_channel</span><br><span class="line">        self.use_se = use_se</span><br><span class="line"></span><br><span class="line">        self.conv = ConvBNActivation(in_channel=in_channel, out_channel=mid_channel, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, groups=<span class="number">1</span>, activate=activate)</span><br><span class="line">        self.depth_conv = ConvBNActivation(in_channel=mid_channel, out_channel=mid_channel, kernel_size=kernel_size, stride=stride, groups=mid_channel, activate=activate)</span><br><span class="line">        <span class="keyword">if</span> self.use_se:</span><br><span class="line">            self.SEblock = SqueezeAndExcite(in_channel=mid_channel, out_channel=mid_channel)</span><br><span class="line"></span><br><span class="line">        self.point_conv = ConvBNActivation(in_channel=mid_channel, out_channel=out_channel, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, groups=<span class="number">1</span>, activate=activate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = self.conv(x)</span><br><span class="line">        out = self.depth_conv(out)</span><br><span class="line">        <span class="keyword">if</span> self.use_se:</span><br><span class="line">            out = self.SEblock(out)</span><br><span class="line">        out = self.point_conv(out)</span><br><span class="line">        <span class="keyword">if</span> self.use_shortcut:</span><br><span class="line">            <span class="keyword">return</span> x + out</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure></p>
<p><code>MobileNetV3</code>模型结构如下图：<br><img src="https://pic.imgdb.cn/item/6311d20216f2c2beb1055f67.png" alt=""></p>
<p>代码实现：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HardSwish</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplace=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(HardSwish, self).__init__()</span><br><span class="line">        self.relu6 = nn.ReLU6(inplace=inplace)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x * self.relu6(x+<span class="number">3</span>)/<span class="number">6</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBNActivation</span>(nn.Sequential):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, kernel_size, stride, groups, activate</span>):</span><br><span class="line">        padding = (kernel_size - <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">        <span class="built_in">super</span>(ConvBNActivation, self).__init__(</span><br><span class="line">            nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channel),</span><br><span class="line">            nn.ReLU6(inplace=<span class="literal">True</span>) <span class="keyword">if</span> activate == <span class="string">&#x27;relu&#x27;</span> <span class="keyword">else</span> HardSwish()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SqueezeAndExcite</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, divide=<span class="number">4</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SqueezeAndExcite, self).__init__()</span><br><span class="line">        mid_channel = in_channel // divide</span><br><span class="line">        self.pool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.SEblock = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=in_channel, out_features=mid_channel),</span><br><span class="line">            nn.ReLU6(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(in_features=mid_channel, out_features=out_channel),</span><br><span class="line">            HardSwish(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        b, c, h, w = x.size()</span><br><span class="line">        out = self.pool(x)</span><br><span class="line">        out = torch.flatten(out, start_dim=<span class="number">1</span>)</span><br><span class="line">        out = self.SEblock(out)</span><br><span class="line">        out = out.view(b, c, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out * x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SEInverteBottleneck</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, mid_channel, out_channel, kernel_size, use_se, activate, stride</span>):</span><br><span class="line">        <span class="built_in">super</span>(SEInverteBottleneck, self).__init__()</span><br><span class="line">        self.use_shortcut = stride == <span class="number">1</span> <span class="keyword">and</span> in_channel == out_channel</span><br><span class="line">        self.use_se = use_se</span><br><span class="line"></span><br><span class="line">        self.conv = ConvBNActivation(in_channel=in_channel, out_channel=mid_channel, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, groups=<span class="number">1</span>, activate=activate)</span><br><span class="line">        self.depth_conv = ConvBNActivation(in_channel=mid_channel, out_channel=mid_channel, kernel_size=kernel_size, stride=stride, groups=mid_channel, activate=activate)</span><br><span class="line">        <span class="keyword">if</span> self.use_se:</span><br><span class="line">            self.SEblock = SqueezeAndExcite(in_channel=mid_channel, out_channel=mid_channel)</span><br><span class="line"></span><br><span class="line">        self.point_conv = ConvBNActivation(in_channel=mid_channel, out_channel=out_channel, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, groups=<span class="number">1</span>, activate=activate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = self.conv(x)</span><br><span class="line">        out = self.depth_conv(out)</span><br><span class="line">        <span class="keyword">if</span> self.use_se:</span><br><span class="line">            out = self.SEblock(out)</span><br><span class="line">        out = self.point_conv(out)</span><br><span class="line">        <span class="keyword">if</span> self.use_shortcut:</span><br><span class="line">            <span class="keyword">return</span> x + out</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MobileNetV3</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span>, <span class="built_in">type</span>=<span class="string">&#x27;large&#x27;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MobileNetV3, self).__init__()</span><br><span class="line">        self.<span class="built_in">type</span> = <span class="built_in">type</span></span><br><span class="line"></span><br><span class="line">        self.first_conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">            HardSwish(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.<span class="built_in">type</span> == <span class="string">&#x27;large&#x27;</span>:</span><br><span class="line">            self.large_bottleneck = nn.Sequential(</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">16</span>, mid_channel=<span class="number">16</span>, out_channel=<span class="number">16</span>, kernel_size=<span class="number">3</span>, use_se=<span class="literal">False</span>, activate=<span class="string">&#x27;relu&#x27;</span>, stride=<span class="number">1</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">16</span>, mid_channel=<span class="number">64</span>, out_channel=<span class="number">24</span>, kernel_size=<span class="number">3</span>, use_se=<span class="literal">False</span>, activate=<span class="string">&#x27;relu&#x27;</span>, stride=<span class="number">2</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">24</span>, mid_channel=<span class="number">72</span>, out_channel=<span class="number">24</span>, kernel_size=<span class="number">3</span>, use_se=<span class="literal">False</span>, activate=<span class="string">&#x27;relu&#x27;</span>, stride=<span class="number">1</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">24</span>, mid_channel=<span class="number">72</span>, out_channel=<span class="number">40</span>, kernel_size=<span class="number">5</span>, use_se=<span class="literal">True</span>, activate=<span class="string">&#x27;relu&#x27;</span>, stride=<span class="number">2</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">40</span>, mid_channel=<span class="number">120</span>, out_channel=<span class="number">40</span>, kernel_size=<span class="number">5</span>, use_se=<span class="literal">True</span>, activate=<span class="string">&#x27;relu&#x27;</span>, stride=<span class="number">1</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">40</span>, mid_channel=<span class="number">120</span>, out_channel=<span class="number">40</span>, kernel_size=<span class="number">5</span>, use_se=<span class="literal">True</span>, activate=<span class="string">&#x27;relu&#x27;</span>, stride=<span class="number">1</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">40</span>, mid_channel=<span class="number">240</span>, out_channel=<span class="number">80</span>, kernel_size=<span class="number">3</span>, use_se=<span class="literal">False</span>, activate=<span class="string">&#x27;hswish&#x27;</span>, stride=<span class="number">2</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">80</span>, mid_channel=<span class="number">200</span>, out_channel=<span class="number">80</span>, kernel_size=<span class="number">3</span>, use_se=<span class="literal">False</span>, activate=<span class="string">&#x27;hswish&#x27;</span>, stride=<span class="number">1</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">80</span>, mid_channel=<span class="number">184</span>, out_channel=<span class="number">80</span>, kernel_size=<span class="number">3</span>, use_se=<span class="literal">False</span>, activate=<span class="string">&#x27;hswish&#x27;</span>, stride=<span class="number">1</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">80</span>, mid_channel=<span class="number">184</span>, out_channel=<span class="number">80</span>, kernel_size=<span class="number">3</span>, use_se=<span class="literal">False</span>, activate=<span class="string">&#x27;hswish&#x27;</span>, stride=<span class="number">1</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">80</span>, mid_channel=<span class="number">480</span>, out_channel=<span class="number">112</span>, kernel_size=<span class="number">3</span>, use_se=<span class="literal">True</span>, activate=<span class="string">&#x27;hswish&#x27;</span>, stride=<span class="number">1</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">112</span>, mid_channel=<span class="number">672</span>, out_channel=<span class="number">112</span>, kernel_size=<span class="number">3</span>, use_se=<span class="literal">True</span>, activate=<span class="string">&#x27;hswish&#x27;</span>, stride=<span class="number">1</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">112</span>, mid_channel=<span class="number">672</span>, out_channel=<span class="number">160</span>, kernel_size=<span class="number">5</span>, use_se=<span class="literal">True</span>, activate=<span class="string">&#x27;hswish&#x27;</span>, stride=<span class="number">2</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">160</span>, mid_channel=<span class="number">960</span>, out_channel=<span class="number">160</span>, kernel_size=<span class="number">5</span>, use_se=<span class="literal">True</span>, activate=<span class="string">&#x27;hswish&#x27;</span>, stride=<span class="number">1</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">160</span>, mid_channel=<span class="number">960</span>, out_channel=<span class="number">160</span>, kernel_size=<span class="number">5</span>, use_se=<span class="literal">True</span>, activate=<span class="string">&#x27;hswish&#x27;</span>, stride=<span class="number">1</span>),</span><br><span class="line">            )</span><br><span class="line">            self.large_last_stage = nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_channels=<span class="number">160</span>, out_channels=<span class="number">960</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(<span class="number">960</span>),</span><br><span class="line">                HardSwish(),</span><br><span class="line">                nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">                nn.Conv2d(in_channels=<span class="number">960</span>, out_channels=<span class="number">1280</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                HardSwish(),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.small_bottleneck = nn.Sequential(</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">16</span>, mid_channel=<span class="number">16</span>, out_channel=<span class="number">16</span>, kernel_size=<span class="number">3</span>, use_se=<span class="literal">True</span>, activate=<span class="string">&#x27;relu&#x27;</span>, stride=<span class="number">2</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">16</span>, mid_channel=<span class="number">72</span>, out_channel=<span class="number">24</span>, kernel_size=<span class="number">3</span>, use_se=<span class="literal">False</span>, activate=<span class="string">&#x27;relu&#x27;</span>, stride=<span class="number">2</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">24</span>, mid_channel=<span class="number">88</span>, out_channel=<span class="number">24</span>, kernel_size=<span class="number">3</span>, use_se=<span class="literal">False</span>, activate=<span class="string">&#x27;relu&#x27;</span>, stride=<span class="number">1</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">24</span>, mid_channel=<span class="number">96</span>, out_channel=<span class="number">40</span>, kernel_size=<span class="number">5</span>, use_se=<span class="literal">True</span>, activate=<span class="string">&#x27;hswish&#x27;</span>, stride=<span class="number">2</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">40</span>, mid_channel=<span class="number">240</span>, out_channel=<span class="number">40</span>, kernel_size=<span class="number">5</span>, use_se=<span class="literal">True</span>, activate=<span class="string">&#x27;hswish&#x27;</span>, stride=<span class="number">1</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">40</span>, mid_channel=<span class="number">240</span>, out_channel=<span class="number">40</span>, kernel_size=<span class="number">5</span>, use_se=<span class="literal">True</span>, activate=<span class="string">&#x27;hswish&#x27;</span>, stride=<span class="number">1</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">40</span>, mid_channel=<span class="number">120</span>, out_channel=<span class="number">48</span>, kernel_size=<span class="number">5</span>, use_se=<span class="literal">True</span>, activate=<span class="string">&#x27;hswish&#x27;</span>, stride=<span class="number">1</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">48</span>, mid_channel=<span class="number">144</span>, out_channel=<span class="number">48</span>, kernel_size=<span class="number">5</span>, use_se=<span class="literal">True</span>, activate=<span class="string">&#x27;hswish&#x27;</span>, stride=<span class="number">1</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">48</span>, mid_channel=<span class="number">288</span>, out_channel=<span class="number">96</span>, kernel_size=<span class="number">5</span>, use_se=<span class="literal">True</span>, activate=<span class="string">&#x27;hswish&#x27;</span>, stride=<span class="number">2</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">96</span>, mid_channel=<span class="number">576</span>, out_channel=<span class="number">96</span>, kernel_size=<span class="number">5</span>, use_se=<span class="literal">True</span>, activate=<span class="string">&#x27;hswish&#x27;</span>, stride=<span class="number">1</span>),</span><br><span class="line">                SEInverteBottleneck(in_channel=<span class="number">96</span>, mid_channel=<span class="number">576</span>, out_channel=<span class="number">96</span>, kernel_size=<span class="number">5</span>, use_se=<span class="literal">True</span>, activate=<span class="string">&#x27;hswish&#x27;</span>, stride=<span class="number">1</span>),</span><br><span class="line">            )</span><br><span class="line">            self.small_last_stage = nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_channels=<span class="number">96</span>, out_channels=<span class="number">576</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(<span class="number">576</span>),</span><br><span class="line">                HardSwish(),</span><br><span class="line">                nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">                nn.Conv2d(in_channels=<span class="number">576</span>, out_channels=<span class="number">1280</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                HardSwish(),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Dropout(p=<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(in_features=<span class="number">1280</span>, out_features=num_classes),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># weight init</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    nn.init.zeros_(m.bias)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                nn.init.ones_(m.weight)</span><br><span class="line">                nn.init.zeros_(m.bias)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                nn.init.normal_(m.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">                nn.init.zeros_(m.bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.first_conv(x)</span><br><span class="line">        <span class="keyword">if</span> self.<span class="built_in">type</span> == <span class="string">&#x27;large&#x27;</span>:</span><br><span class="line">            x = self.large_bottleneck(x)</span><br><span class="line">            x = self.large_last_stage(x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = self.small_bottleneck(x)</span><br><span class="line">            x = self.small_last_stage(x)</span><br><span class="line"></span><br><span class="line">        x = torch.flatten(x, start_dim=<span class="number">1</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p>
<h2 id="3-模型特点"><a href="#3-模型特点" class="headerlink" title="3. 模型特点"></a>3. 模型特点</h2><ol>
<li>提出深度可分离卷积</li>
<li><code>ReLU</code>在低维度是会对特征有损失，于是提出了倒残差模块</li>
<li>引入了SE注意力机制和<code>h-swish</code>激活函数</li>
</ol>
]]></content>
      <categories>
        <category>图像分类</category>
      </categories>
      <tags>
        <tag>模型</tag>
        <tag>backbone</tag>
        <tag>轻量级网络</tag>
      </tags>
  </entry>
  <entry>
    <title>ViT</title>
    <url>/2022/09/05/ViT/</url>
    <content><![CDATA[<h1 id="Vision-Transformer"><a href="#Vision-Transformer" class="headerlink" title="Vision Transformer"></a><a href="https://arxiv.org/abs/2010.11929">Vision Transformer</a></h1><h2 id="1-模型介绍"><a href="#1-模型介绍" class="headerlink" title="1. 模型介绍"></a>1. 模型介绍</h2><p>在计算机视觉领域中，多数算法都是保持CNN整体结构不变，在CNN中增加attention模块或者使用attention模块替换CNN中的某些部分。有研究者提出，没有必要总是依赖于CNN。因此，作者提出ViT算法，仅仅使用Transformer结构也能够在图像分类任务中表现很好。<br><span id="more"></span><br>受到NLP领域中Transformer成功应用的启发，ViT算法中尝试将标准的Transformer结构直接应用于图像，并对整个图像分类流程进行最少的修改。具体来讲，ViT算法中，会将整幅图像拆分成小图像块，然后把这些小图像块的线性嵌入序列作为Transformer的输入送入网络，然后使用监督学习的方式进行图像分类的训练。</p>
<p>该算法在中等规模（例如ImageNet）以及大规模（例如ImageNet-21K、JFT-300M）数据集上进行了实验验证，发现：</p>
<ul>
<li>Transformer相较于CNN结构，缺少一定的平移不变性和局部感知性，因此在数据量不充分时，很难达到同等的效果。具体表现为使用中等规模的ImageNet训练的Transformer会比ResNet在精度上低几个百分点。</li>
<li>当有大量的训练样本时，结果则会发生改变。使用大规模数据集进行预训练后，再使用迁移学习的方式应用到其他数据集上，可以达到或超越当前的SOTA水平。</li>
</ul>
<h2 id="2-模型结构与实现"><a href="#2-模型结构与实现" class="headerlink" title="2. 模型结构与实现"></a>2. 模型结构与实现</h2><p>ViT算法的整体结构如 <strong>图1</strong> 所示。</p>
<p><img src="https://pic.imgdb.cn/item/6315acd016f2c2beb158cf3a.png" alt="图1 ViT算法结构示意图"></p>
<p><center>图1 ViT算法结构示意图</center><br></br></p>
<h3 id="2-1-图像分块嵌入"><a href="#2-1-图像分块嵌入" class="headerlink" title="2.1. 图像分块嵌入"></a>2.1. 图像分块嵌入</h3><p>考虑到在Transformer结构中，输入是一个二维的矩阵，矩阵的形状可以表示为 $(N,D)$，其中 $N$ 是sequence的长度，而 $D$ 是sequence中每个向量的维度。因此，在ViT算法中，首先需要设法将 $H \times W \times C$ 的三维图像转化为 $(N,D)$ 的二维输入。</p>
<p>ViT中的具体实现方式为：将 $H \times W \times C$ 的图像，变为一个 $N \times (P^2 \times C)$ 的序列。这个序列可以看作是一系列展平的图像块，也就是将图像切分成小块后，再将其展平。该序列中一共包含了 $N=HW/P^2$ 个图像块，每个图像块的维度则是 $(P^2 \times C)$。其中  $P$ 是图像块的大小，$C$ 是通道数量。经过如上变换，就可以将 $N$ 视为sequence的长度了。</p>
<p>但是，此时每个图像块的维度是 $(P^2 \times C)$，而我们实际需要的向量维度是 $D$，因此我们还需要对图像块进行 Embedding。这里 Embedding 的方式非常简单，只需要对每个 $(P^2 \times C)$ 的图像块做一个线性变换，将维度压缩为 $D$ 即可。</p>
<p>上述对图像进行分块以及 Embedding 的具体方式如 <strong>图2</strong> 所示。</p>
<p><img src="https://pic.imgdb.cn/item/6315acf116f2c2beb15944c8.png" alt="图2 图像分块嵌入示意图"></p>
<p><center>图2 图像分块嵌入示意图</center><br></br></p>
<p>具体代码实现如下所示:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    2D Image to Patch Embedding</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_c=<span class="number">3</span>, embed_dim=<span class="number">768</span>, norm_layer=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        img_size = (img_size, img_size)</span><br><span class="line">        patch_size = (patch_size, patch_size)</span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.grid_size = (img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>])</span><br><span class="line">        self.num_patches = self.grid_size[<span class="number">0</span>] * self.grid_size[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line">        self.norm = norm_layer(embed_dim) <span class="keyword">if</span> norm_layer <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        <span class="keyword">assert</span> H == self.img_size[<span class="number">0</span>] <span class="keyword">and</span> W == self.img_size[<span class="number">1</span>], \</span><br><span class="line">            <span class="string">f&quot;Input image size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) doesn&#x27;t match model (<span class="subst">&#123;self.img_size[<span class="number">0</span>]&#125;</span>*<span class="subst">&#123;self.img_size[<span class="number">1</span>]&#125;</span>).&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># flatten: [B, C, H, W] -&gt; [B, C, HW]</span></span><br><span class="line">        <span class="comment"># transpose: [B, C, HW] -&gt; [B, HW, C]</span></span><br><span class="line">        x = self.proj(x).flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="2-2-多头注意力"><a href="#2-2-多头注意力" class="headerlink" title="2.2. 多头注意力"></a>2.2. 多头注意力</h3><p>将图像转化为 $N \times (P^2 \times C)$ 的序列后，就可以将其输入到 Transformer 结构中进行特征提取了，如 <strong>图3</strong> 所示。</p>
<p><img src="https://pic.imgdb.cn/item/6315ad7b16f2c2beb159fc6a.png" alt="图3 多头注意力示意图"></p>
<p><center>图3 多头注意力示意图</center><br></br></p>
<p>Transformer 结构中最重要的结构就是 Multi-head Attention，即多头注意力结构。具有2个head的 Multi-head Attention 结构如 <strong>图4</strong> 所示。输入 $a^i$ 经过转移矩阵，并切分生成 $q^{(i,1)}$、$q^{(i,2)}$、$k^{(i,1)}$、$k^{(i,2)}$、$v^{(i,1)}$、$v^{(i,2)}$，然后 $q^{(i,1)}$ 与 $k^{(i,1)}$ 做 attention，得到权重向量 $\alpha$，将 $\alpha$ 与 $v^{(i,1)}$ 进行加权求和，得到最终的 $b^{(i,1)}(i=1,2,…,N)$，同理可以得到 $b^{(i,2)}(i=1,2,…,N)$。接着将它们拼接起来，通过一个线性层进行处理，得到最终的结果。</p>
<p><img src="https://pic.imgdb.cn/item/6315adac16f2c2beb15a315d.png" alt="图4 多头注意力"></p>
<p><center>图4 多头注意力</center><br></br></p>
<p>其中，使用 $q^{(i,j)}$、$k^{(i,j)}$ 与 $v^{(i,j)}$ 计算 $b^{(i,j)}(i=1,2,…,N)$ 的方法是缩放点积注意力 (Scaled Dot-Product Attention)。 结构如 <strong>图5</strong> 所示。首先使用每个 $q^{(i,j)}$ 去与 $k^{(i,j)}$ 做 attention，这里说的 attention 就是匹配这两个向量有多接近，具体的方式就是计算向量的加权内积，得到 $\alpha_{(i,j)}$。这里的加权内积计算方式如下所示：</p>
<script type="math/tex; mode=display">\alpha_{(1,i)} =  q^1 * k^i / \sqrt{d}</script><p>其中，$d$ 是 $q$ 和 $k$ 的维度，因为 $q*k$ 的数值会随着维度的增大而增大，因此除以 $\sqrt{d}$ 的值也就相当于归一化的效果。</p>
<p>接下来，把计算得到的 $\alpha_{(i,j)}$ 取 softmax 操作，再将其与 $v^{(i,j)}$ 相乘。</p>
<p><img src="https://pic.imgdb.cn/item/6315adda16f2c2beb15a5dbd.png" alt="图5 缩放点积注意力"></p>
<p><center>图5 缩放点积注意力</center><br></br></p>
<p>具体代码实现如下所示:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 dim,   <span class="comment"># 输入token的dim</span></span></span><br><span class="line"><span class="params">                 num_heads=<span class="number">8</span>,</span></span><br><span class="line"><span class="params">                 qkv_bias=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 qk_scale=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 attn_drop_ratio=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 proj_drop_ratio=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Attention, self).__init__()</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        head_dim = dim // num_heads</span><br><span class="line">        self.scale = qk_scale <span class="keyword">or</span> head_dim ** -<span class="number">0.5</span></span><br><span class="line">        self.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop_ratio)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.proj_drop = nn.Dropout(proj_drop_ratio)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># [batch_size, num_patches + 1, total_embed_dim]</span></span><br><span class="line">        B, N, C = x.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># qkv(): -&gt; [batch_size, num_patches + 1, 3 * total_embed_dim]</span></span><br><span class="line">        <span class="comment"># reshape: -&gt; [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]</span></span><br><span class="line">        <span class="comment"># permute: -&gt; [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]</span></span><br><span class="line">        qkv = self.qkv(x).reshape(B, N, <span class="number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="comment"># [batch_size, num_heads, num_patches + 1, embed_dim_per_head]</span></span><br><span class="line">        q, k, v = qkv[<span class="number">0</span>], qkv[<span class="number">1</span>], qkv[<span class="number">2</span>]  <span class="comment"># make torchscript happy (cannot use tensor as tuple)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># transpose: -&gt; [batch_size, num_heads, embed_dim_per_head, num_patches + 1]</span></span><br><span class="line">        <span class="comment"># @: multiply -&gt; [batch_size, num_heads, num_patches + 1, num_patches + 1]</span></span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) * self.scale</span><br><span class="line">        attn = attn.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        attn = self.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># @: multiply -&gt; [batch_size, num_heads, num_patches + 1, embed_dim_per_head]</span></span><br><span class="line">        <span class="comment"># transpose: -&gt; [batch_size, num_patches + 1, num_heads, embed_dim_per_head]</span></span><br><span class="line">        <span class="comment"># reshape: -&gt; [batch_size, num_patches + 1, total_embed_dim]</span></span><br><span class="line">        x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B, N, C)</span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="2-3-多层感知机（MLP）"><a href="#2-3-多层感知机（MLP）" class="headerlink" title="2.3. 多层感知机（MLP）"></a>2.3. 多层感知机（MLP）</h3><p> Transformer 结构中还有一个重要的结构就是 MLP，即多层感知机，如 <strong>图6</strong> 所示。</p>
<p><img src="https://pic.imgdb.cn/item/6315ae2b16f2c2beb15aa73d.png" alt="图6 MLP多层感知机的结构"></p>
<p>具体代码实现如下所示:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Mlp</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    MLP as used in Vision Transformer, MLP-Mixer and related networks</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, hidden_features=<span class="literal">None</span>, out_features=<span class="literal">None</span>, act_layer=nn.GELU, drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        out_features = out_features <span class="keyword">or</span> in_features</span><br><span class="line">        hidden_features = hidden_features <span class="keyword">or</span> in_features</span><br><span class="line">        self.fc1 = nn.Linear(in_features, hidden_features)</span><br><span class="line">        self.act = act_layer()</span><br><span class="line">        self.fc2 = nn.Linear(hidden_features, out_features)</span><br><span class="line">        self.drop = nn.Dropout(drop)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.act(x)</span><br><span class="line">        x = self.drop(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="2-4-DropPath"><a href="#2-4-DropPath" class="headerlink" title="2.4. DropPath"></a>2.4. DropPath</h3><p>除了以上重要模块意外，代码实现过程中还使用了DropPath（Stochastic Depth）来代替传统的Dropout结构，DropPath可以理解为一种特殊的 Dropout。其作用为：<strong>若x为输入的张量，其通道为[B,C,H,W]，那么drop_path的含义为在一个Batch_size中，随机有drop_prob的样本，不经过主干，而直接由分支进行恒等映射。</strong></p>
<p>具体实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">drop_path</span>(<span class="params">x, drop_prob: <span class="built_in">float</span> = <span class="number">0.</span>, training: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">if</span> drop_prob == <span class="number">0.</span> <span class="keyword">or</span> <span class="keyword">not</span> training:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    keep_prob = <span class="number">1</span> - drop_prob</span><br><span class="line">    shape = (x.shape[<span class="number">0</span>],) + (<span class="number">1</span>,) * (x.ndim - <span class="number">1</span>)  <span class="comment"># work with diff dim tensors, not just 2D ConvNets</span></span><br><span class="line">    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)</span><br><span class="line">    random_tensor.floor_()  <span class="comment"># binarize</span></span><br><span class="line">    output = x.div(keep_prob) * random_tensor</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DropPath</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, drop_prob=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(DropPath, self).__init__()</span><br><span class="line">        self.drop_prob = drop_prob</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> drop_path(x, self.drop_prob, self.training)</span><br></pre></td></tr></table></figure>
<h3 id="2-5-基础模块"><a href="#2-5-基础模块" class="headerlink" title="2.5. 基础模块"></a>2.5. 基础模块</h3><p>基于上面实现的 Attention、MLP、DropPath模块就可以组合出 Vision Transformer 模型的一个基础模块，如 <strong>图8</strong> 所示。</p>
<p><img src="https://pic.imgdb.cn/item/6315aef316f2c2beb15b72bd.png" alt="图8 基础模块示意图"></p>
<p><center>图8 基础模块示意图</center><br></br></p>
<p>基础模块的具体实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 dim,</span></span><br><span class="line"><span class="params">                 num_heads,</span></span><br><span class="line"><span class="params">                 mlp_ratio=<span class="number">4.</span>,</span></span><br><span class="line"><span class="params">                 qkv_bias=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 qk_scale=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 drop_ratio=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 attn_drop_ratio=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 drop_path_ratio=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 act_layer=nn.GELU,</span></span><br><span class="line"><span class="params">                 norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>(Block, self).__init__()</span><br><span class="line">        self.norm1 = norm_layer(dim)</span><br><span class="line">        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,</span><br><span class="line">                              attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio)</span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> drop path for stochastic depth, we shall see if this is better than dropout here</span></span><br><span class="line">        self.drop_path = DropPath(drop_path_ratio) <span class="keyword">if</span> drop_path_ratio &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.norm2 = norm_layer(dim)</span><br><span class="line">        mlp_hidden_dim = <span class="built_in">int</span>(dim * mlp_ratio)</span><br><span class="line">        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + self.drop_path(self.attn(self.norm1(x)))</span><br><span class="line">        x = x + self.drop_path(self.mlp(self.norm2(x)))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="2-6-定义ViT网络"><a href="#2-6-定义ViT网络" class="headerlink" title="2.6. 定义ViT网络"></a>2.6. 定义ViT网络</h3><p>基础模块构建好后，就可以构建完整的ViT网络了。在构建完整网络结构之前，还需要给大家介绍几个模块：</p>
<ul>
<li>Class Token</li>
</ul>
<p>假设我们将原始图像切分成 $3 \times 3$ 共9个小图像块，最终的输入序列长度却是10，也就是说我们这里人为的增加了一个向量进行输入，我们通常将人为增加的这个向量称为 Class Token。那么这个 Class Token 有什么作用呢？</p>
<p>我们可以想象，如果没有这个向量，也就是将 $N=9$ 个向量输入 Transformer 结构中进行编码，我们最终会得到9个编码向量，可对于图像分类任务而言，我们应该选择哪个输出向量进行后续分类呢？因此，ViT算法提出了一个可学习的嵌入向量 Class Token，将它与9个向量一起输入到 Transformer 结构中，输出10个编码向量，然后用这个 Class Token 进行分类预测即可。</p>
<p>其实这里也可以理解为：ViT 其实只用到了 Transformer 中的 Encoder，而并没有用到 Decoder，而 Class Token 的作用就是寻找其他9个输入向量对应的类别。</p>
<ul>
<li>Positional Encoding</li>
</ul>
<p>按照 Transformer 结构中的位置编码习惯，这个工作也使用了位置编码。不同的是，ViT 中的位置编码没有采用原版 Transformer 中的 $sincos$ 编码，而是直接设置为可学习的 Positional Encoding。</p>
<ul>
<li>MLP Head</li>
</ul>
<p>得到输出后，ViT中使用了 MLP Head对输出进行分类处理，这里的 MLP Head 由 LayerNorm 和两层全连接层组成，并且采用了 GELU 激活函数。</p>
<p>具体代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VisionTransformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_c=<span class="number">3</span>, num_classes=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 embed_dim=<span class="number">768</span>, depth=<span class="number">12</span>, num_heads=<span class="number">12</span>, mlp_ratio=<span class="number">4.0</span>, qkv_bias=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 qk_scale=<span class="literal">None</span>, drop_ratio=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 attn_drop_ratio=<span class="number">0.</span>, drop_path_ratio=<span class="number">0.</span>, embed_layer=PatchEmbed, norm_layer=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 act_layer=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            img_size (int, tuple): input image size</span></span><br><span class="line"><span class="string">            patch_size (int, tuple): patch size</span></span><br><span class="line"><span class="string">            in_c (int): number of input channels</span></span><br><span class="line"><span class="string">            num_classes (int): number of classes for classification head</span></span><br><span class="line"><span class="string">            embed_dim (int): embedding dimension</span></span><br><span class="line"><span class="string">            depth (int): depth of transformer</span></span><br><span class="line"><span class="string">            num_heads (int): number of attention heads</span></span><br><span class="line"><span class="string">            mlp_ratio (int): ratio of mlp hidden dim to embedding dim</span></span><br><span class="line"><span class="string">            qkv_bias (bool): enable bias for qkv if True</span></span><br><span class="line"><span class="string">            qk_scale (float): override default qk scale of head_dim ** -0.5 if set</span></span><br><span class="line"><span class="string">            drop_ratio (float): dropout rate</span></span><br><span class="line"><span class="string">            attn_drop_ratio (float): attention dropout rate</span></span><br><span class="line"><span class="string">            drop_path_ratio (float): stochastic depth rate</span></span><br><span class="line"><span class="string">            embed_layer (nn.Module): patch embedding layer</span></span><br><span class="line"><span class="string">            norm_layer: (nn.Module): normalization layer</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(VisionTransformer, self).__init__()</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.num_features = self.embed_dim = embed_dim  <span class="comment"># num_features for consistency with other models</span></span><br><span class="line">        self.num_tokens = <span class="number">2</span> <span class="keyword">if</span> distilled <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        norm_layer = norm_layer <span class="keyword">or</span> partial(nn.LayerNorm, eps=<span class="number">1e-6</span>)</span><br><span class="line">        act_layer = act_layer <span class="keyword">or</span> nn.GELU</span><br><span class="line"></span><br><span class="line">        self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim)</span><br><span class="line">        num_patches = self.patch_embed.num_patches</span><br><span class="line"></span><br><span class="line">        self.cls_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, embed_dim))</span><br><span class="line">        self.pos_drop = nn.Dropout(p=drop_ratio)</span><br><span class="line"></span><br><span class="line">        dpr = [x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_ratio, depth)]  <span class="comment"># stochastic depth decay rule</span></span><br><span class="line">        self.blocks = nn.Sequential(*[</span><br><span class="line">            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,</span><br><span class="line">                  drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i],</span><br><span class="line">                  norm_layer=norm_layer, act_layer=act_layer)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)</span><br><span class="line">        ])</span><br><span class="line">        self.norm = norm_layer(embed_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Classifier head(s)</span></span><br><span class="line">        self.head = nn.Linear(self.num_features, num_classes) <span class="keyword">if</span> num_classes &gt; <span class="number">0</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Weight init</span></span><br><span class="line">        nn.init.trunc_normal_(self.pos_embed, std=<span class="number">0.02</span>)</span><br><span class="line">        <span class="keyword">if</span> self.dist_token <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            nn.init.trunc_normal_(self.dist_token, std=<span class="number">0.02</span>)</span><br><span class="line"></span><br><span class="line">        nn.init.trunc_normal_(self.cls_token, std=<span class="number">0.02</span>)</span><br><span class="line">        self.apply(_init_vit_weights)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># [B, C, H, W] -&gt; [B, num_patches, embed_dim]</span></span><br><span class="line">        x = self.patch_embed(x)  <span class="comment"># [B, 196, 768]</span></span><br><span class="line">        <span class="comment"># [1, 1, 768] -&gt; [B, 1, 768]</span></span><br><span class="line">        cls_token = self.cls_token.expand(x.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        x = torch.cat((cls_token, x), dim=<span class="number">1</span>)  <span class="comment"># [B, 197, 768]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        x = self.pos_drop(x + self.pos_embed)</span><br><span class="line">        x = self.blocks(x)</span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.forward_features(x)</span><br><span class="line">        x = self.head(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_init_vit_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    ViT weight initialization</span></span><br><span class="line"><span class="string">    :param m: module</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">        nn.init.trunc_normal_(m.weight, std=<span class="number">.01</span>)</span><br><span class="line">        <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            nn.init.zeros_(m.bias)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">        nn.init.kaiming_normal_(m.weight, mode=<span class="string">&quot;fan_out&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            nn.init.zeros_(m.bias)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.LayerNorm):</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line">        nn.init.ones_(m.weight)</span><br></pre></td></tr></table></figure>
<h2 id="3-模型特点"><a href="#3-模型特点" class="headerlink" title="3. 模型特点"></a>3. 模型特点</h2><ul>
<li>作为CV领域最经典的 Transformer 算法之一，不同于传统的CNN算法，ViT尝试将标准的Transformer结构直接应用于图像，并对整个图像分类流程进行最少的修改。</li>
<li>为了满足 Transformer 输入结构的要求，将整幅图像拆分成小图像块，然后把这些小图像块的线性嵌入序列输入到网络。同时，使用了Class Token的方式进行分类预测。</li>
</ul>
]]></content>
      <categories>
        <category>图像分类</category>
      </categories>
      <tags>
        <tag>模型</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Swin Transformer</title>
    <url>/2022/09/06/Swin-Transformer/</url>
    <content><![CDATA[<h1 id="Swin-Transformer"><a href="#Swin-Transformer" class="headerlink" title="Swin Transformer"></a><a href="https://arxiv.org/pdf/2103.14030.pdf">Swin Transformer</a></h1><h2 id="1-模型介绍"><a href="#1-模型介绍" class="headerlink" title="1. 模型介绍"></a>1. 模型介绍</h2><p>Swin Transformer是由微软亚洲研究院在今年公布的一篇利用transformer架构处理计算机视觉任务的论文。Swin Transformer 在图像分类，图像分割，目标检测等各个领域已经屠榜，在论文中，作者分析表明，Transformer从NLP迁移到CV上没有大放异彩主要有两点原因：1. 两个领域涉及的scale不同，NLP的token是标准固定的大小，而CV的特征尺度变化范围非常大。2. CV比起NLP需要更大的分辨率，而且CV中使用Transformer的计算复杂度是图像尺度的平方，这会导致计算量过于庞大。为了解决这两个问题，Swin Transformer相比之前的ViT做了两个改进：1.引入CNN中常用的层次化构建方式构建层次化Transformer 2.引入locality思想，对无重合的window区域内进行self-attention计算。另外，Swin Transformer可以作为图像分类、目标检测和语义分割等任务的通用骨干网络，可以说，Swin Transformer可能是CNN的完美替代方案。<br><span id="more"></span></p>
<h2 id="2-模型结构及实现"><a href="#2-模型结构及实现" class="headerlink" title="2. 模型结构及实现"></a>2. 模型结构及实现</h2><p>下图为Swin Transformer与ViT在处理图片方式上的对比，可以看出，Swin Transformer有着ResNet一样的残差结构和CNN具有的多尺度图片结构。</p>
<p><img src="https://pic.imgdb.cn/item/6316f06c16f2c2beb1955b5e.png" alt="st"></p>
<p><strong>整体概括：</strong></p>
<p>下图为Swin Transformer的网络结构，输入的图像先经过一层卷积进行patch映射，将图像先分割成$4 × 4$的小块，图片是$224×224$输入，那么就是56个path块，如果是$384×384$的尺寸，则是96个path块。这里以$224 × 224$的输入为例，输入图像经过这一步操作，每个patch的特征维度为$4x4x3=48$的特征图。因此，输入的图像变成了$H/4×W/4×48$的特征图。然后，特征图开始输入到stage1，stage1中linear embedding将path特征维度变成C，因此变成了$H/4×W/4×C$。然后送入Swin Transformer Block，在进入stage2前，接下来先通过Patch Merging操作，Patch Merging和CNN中stride=2的1×1卷积十分相似，Patch Merging在每个Stage开始前做降采样，用于缩小分辨率，调整通道数，当$H/4×W/4×C$的特征图输送到Patch Merging，将输入按照$2×2$的相邻patches合并，这样子patch块的数量就变成了$H/8 × W/8$，特征维度就变成了4C，之后经过一个MLP，将特征维度降为2C。因此变为$H/8×W/8×2C$。接下来的stage就是重复上面的过程。</p>
<p><img src="https://pic.imgdb.cn/item/6316f12f16f2c2beb1965d27.png" alt="st"></p>
<h3 id="2-1-Linear-embedding"><a href="#2-1-Linear-embedding" class="headerlink" title="2.1 Linear embedding"></a>2.1 Linear embedding</h3><p>以下代码为Linear embedding的操作，整个操作可以看作一个patch大小的卷积核和patch大小的步长的卷积对输入的B，C，H，W的图片进行卷积，得到的自然就是大小为 B，C，H/patch，W/patch的特征图，如果放在第一个Linear embedding中，得到的特征图就为 B，96，56，56的大小。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    2D Image to Patch Embedding</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, patch_size=<span class="number">4</span>, in_c=<span class="number">3</span>, embed_dim=<span class="number">96</span>, norm_layer=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        patch_size = (patch_size, patch_size)</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.in_chans = in_c</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line">        self.norm = norm_layer(embed_dim) <span class="keyword">if</span> norm_layer <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        _, _, H, W = x.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># padding</span></span><br><span class="line">        <span class="comment"># 如果输入图片的H，W不是patch_size的整数倍，需要进行padding</span></span><br><span class="line">        pad_input = (H % self.patch_size[<span class="number">0</span>] != <span class="number">0</span>) <span class="keyword">or</span> (W % self.patch_size[<span class="number">1</span>] != <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> pad_input:</span><br><span class="line">            <span class="comment"># to pad the last 3 dimensions,</span></span><br><span class="line">            <span class="comment"># (W_left, W_right, H_top,H_bottom, C_front, C_back)</span></span><br><span class="line">            x = F.pad(x, (<span class="number">0</span>, self.patch_size[<span class="number">1</span>] - W % self.patch_size[<span class="number">1</span>],</span><br><span class="line">                          <span class="number">0</span>, self.patch_size[<span class="number">0</span>] - H % self.patch_size[<span class="number">0</span>],</span><br><span class="line">                          <span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 下采样patch_size倍</span></span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        _, _, H, W = x.shape</span><br><span class="line">        <span class="comment"># flatten: [B, C, H, W] -&gt; [B, C, HW]</span></span><br><span class="line">        <span class="comment"># transpose: [B, C, HW] -&gt; [B, HW, C]</span></span><br><span class="line">        x = x.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x, H, W</span><br></pre></td></tr></table></figure></p>
<h3 id="2-2-Patch-Merging"><a href="#2-2-Patch-Merging" class="headerlink" title="2.2 Patch Merging"></a>2.2 Patch Merging</h3><p>以下为PatchMerging的操作。该操作以2为步长，对输入的图片进行采样，总共得到4张下采样的特征图，H和W降低2倍，因此，通道级拼接后得到的是$B，4C，H/2，W/2$的特征图。然而这样的拼接不能够提取有用的特征信息，于是，一个线性层将4C的通道筛选为2C, 特征图变为了$B，2C， H/2， W/2$。总结一下，经过这个操作原本$B，C，H，W$的特征图就变为了$B，2C，H/2，W/2$的特征图，完成了下采样操作。如下图所示：<br><img src="https://pic.imgdb.cn/item/6317042116f2c2beb1acbb4b.png" alt=""><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchMerging</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Patch Merging Layer.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.reduction = nn.Linear(<span class="number">4</span> * dim, <span class="number">2</span> * dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.norm = norm_layer(<span class="number">4</span> * dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, H, W</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x: B, H*W, C</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        <span class="keyword">assert</span> L == H * W, <span class="string">&quot;input feature has wrong size&quot;</span></span><br><span class="line"></span><br><span class="line">        x = x.view(B, H, W, C)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># padding</span></span><br><span class="line">        <span class="comment"># 如果输入feature map的H，W不是2的整数倍，需要进行padding</span></span><br><span class="line">        pad_input = (H % <span class="number">2</span> == <span class="number">1</span>) <span class="keyword">or</span> (W % <span class="number">2</span> == <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> pad_input:</span><br><span class="line">            <span class="comment"># to pad the last 3 dimensions, starting from the last dimension and moving forward.</span></span><br><span class="line">            <span class="comment"># (C_front, C_back, W_left, W_right, H_top, H_bottom)</span></span><br><span class="line">            <span class="comment"># 注意这里的Tensor通道是[B, H, W, C]，所以会和官方文档有些不同</span></span><br><span class="line">            x = F.pad(x, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, W % <span class="number">2</span>, <span class="number">0</span>, H % <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        x0 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># [B, H/2, W/2, C]</span></span><br><span class="line">        x1 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># [B, H/2, W/2, C]</span></span><br><span class="line">        x2 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># [B, H/2, W/2, C]</span></span><br><span class="line">        x3 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># [B, H/2, W/2, C]</span></span><br><span class="line">        x = torch.cat([x0, x1, x2, x3], -<span class="number">1</span>)  <span class="comment"># [B, H/2, W/2, 4*C]</span></span><br><span class="line">        x = x.view(B, -<span class="number">1</span>, <span class="number">4</span> * C)  <span class="comment"># [B, H/2*W/2, 4*C]</span></span><br><span class="line"></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        x = self.reduction(x)  <span class="comment"># [B, H/2*W/2, 2*C]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p>
<h3 id="2-3-Swin-Transformer-Block"><a href="#2-3-Swin-Transformer-Block" class="headerlink" title="2.3 Swin Transformer Block"></a>2.3 Swin Transformer Block</h3><h4 id="2-3-1-Window-Partition-Reverse"><a href="#2-3-1-Window-Partition-Reverse" class="headerlink" title="2.3.1 Window Partition/Reverse"></a>2.3.1 Window Partition/Reverse</h4><p>下面的操作是根据window_size划分特征图的操作和还原的操作，原理很简单就是并排划分即可。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">window_partition</span>(<span class="params">x, window_size: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将feature map按照window_size划分成一个个没有重叠的window</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        x: (B, H, W, C)</span></span><br><span class="line"><span class="string">        window_size (int): window size(M)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        windows: (num_windows*B, window_size, window_size, C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    B, H, W, C = x.shape</span><br><span class="line">    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)</span><br><span class="line">    <span class="comment"># permute: [B, H//Mh, Mh, W//Mw, Mw, C] -&gt; [B, H//Mh, W//Mh, Mw, Mw, C]</span></span><br><span class="line">    <span class="comment"># view: [B, H//Mh, W//Mw, Mh, Mw, C] -&gt; [B*num_windows, Mh, Mw, C]</span></span><br><span class="line">    windows = x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>).contiguous().view(-<span class="number">1</span>, window_size, window_size, C)</span><br><span class="line">    <span class="keyword">return</span> windows</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">window_reverse</span>(<span class="params">windows, window_size: <span class="built_in">int</span>, H: <span class="built_in">int</span>, W: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将一个个window还原成一个feature map</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        windows: (num_windows*B, window_size, window_size, C)</span></span><br><span class="line"><span class="string">        window_size (int): Window size(M)</span></span><br><span class="line"><span class="string">        H (int): Height of image</span></span><br><span class="line"><span class="string">        W (int): Width of image</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        x: (B, H, W, C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    B = <span class="built_in">int</span>(windows.shape[<span class="number">0</span>] / (H * W / window_size / window_size))</span><br><span class="line">    <span class="comment"># view: [B*num_windows, Mh, Mw, C] -&gt; [B, H//Mh, W//Mw, Mh, Mw, C]</span></span><br><span class="line">    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># permute: [B, H//Mh, W//Mw, Mh, Mw, C] -&gt; [B, H//Mh, Mh, W//Mw, Mw, C]</span></span><br><span class="line">    <span class="comment"># view: [B, H//Mh, Mh, W//Mw, Mw, C] -&gt; [B, H, W, C]</span></span><br><span class="line">    x = x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>).contiguous().view(B, H, W, -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h4 id="2-3-2-Window-Attention"><a href="#2-3-2-Window-Attention" class="headerlink" title="2.3.2 Window Attention"></a>2.3.2 Window Attention</h4><p>这是这篇文章的关键。传统的Transformer都是<strong>基于全局来计算注意力</strong>的，因此计算复杂度十分高。而Swin Transformer则<strong>将注意力的计算限制在每个窗口内</strong>，进而减少了计算量。</p>
<script type="math/tex; mode=display">Attention(Q,K,V)=Softmax(\frac {QK^T}{\sqrt d} + B)V</script><p>主要区别是在原始计算Attention的公式中的Q,K时<strong>加入了相对位置编码</strong>。后续实验有证明相对位置编码的加入提升了模型性能。</p>
<p><strong>1. Relative Position Bias</strong><br>以 $2×2$ 的特征图为例，首先我们需要对特征图的各个块进行绝对位置的编号，得到每个块的绝对位置索引。然后对每个块计算其与其他块之间的相对位置，计算方法为该块的绝对位置索引减去其他块的绝对位置索引，可以得到每个块的相对位置索引矩阵。将每个块的相对位置索引矩阵展平连接构成了整个特征图的相对位置索引矩阵，具体的计算流程如下图所示<br><img src="https://svain-xyz.oss-cn-shenzhen.aliyuncs.com/image/Swin-T_%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%B4%A2%E5%BC%95%E5%9B%BE.png" alt=""><br>Swin-T并不是使用二维元组形式的相对位置索引矩阵，而是通过将二维元组形式的相对位置索引映射为一维的相对位置偏置（Relative position bias）来构成相应的矩阵，具体的映射方法如下：</p>
<ol>
<li>将对应的相对位置行索引和列索引分别加上 <code>window_size-1</code></li>
<li>将行索引乘以 <code>2window_size-1</code></li>
<li>将行索引和列索引相加，再使用对应的相对位置偏置表（Relative position bias table）进行映射即可得到最终的相对位置偏置B。具体的计算流程如下所示：<br><img src="https://svain-xyz.oss-cn-shenzhen.aliyuncs.com/image/Swin-T_%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E5%81%8F%E7%BD%AE%E6%B5%81%E7%A8%8B%E5%9B%BE.png" alt=""></li>
</ol>
<p>代码如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.relative_position_bias_table = nn.Parameter(</span><br><span class="line">    torch.zeros((<span class="number">2</span> * window_size[<span class="number">0</span>] - <span class="number">1</span>) * (<span class="number">2</span> * window_size[<span class="number">1</span>] - <span class="number">1</span>), num_heads))  <span class="comment"># [2*Mh-1 * 2*Mw-1, nH]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># get pair-wise relative position index for each token inside the window</span></span><br><span class="line">coords_h = torch.arange(self.window_size[<span class="number">0</span>])</span><br><span class="line">coords_w = torch.arange(self.window_size[<span class="number">1</span>])</span><br><span class="line">coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=<span class="string">&quot;ij&quot;</span>))  <span class="comment"># [2, Mh, Mw]</span></span><br><span class="line">coords_flatten = torch.flatten(coords, <span class="number">1</span>)  <span class="comment"># [2, Mh*Mw]</span></span><br><span class="line"><span class="comment"># [2, Mh*Mw, 1] - [2, 1, Mh*Mw]</span></span><br><span class="line">relative_coords = coords_flatten[:, :, <span class="literal">None</span>] - coords_flatten[:, <span class="literal">None</span>, :]  <span class="comment"># [2, Mh*Mw, Mh*Mw]</span></span><br><span class="line">relative_coords = relative_coords.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).contiguous()  <span class="comment"># [Mh*Mw, Mh*Mw, 2]</span></span><br><span class="line">relative_coords[:, :, <span class="number">0</span>] += self.window_size[<span class="number">0</span>] - <span class="number">1</span>  <span class="comment"># shift to start from 0</span></span><br><span class="line">relative_coords[:, :, <span class="number">1</span>] += self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">relative_coords[:, :, <span class="number">0</span>] *= <span class="number">2</span> * self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">relative_position_index = relative_coords.<span class="built_in">sum</span>(-<span class="number">1</span>)  <span class="comment"># [Mh*Mw, Mh*Mw]</span></span><br><span class="line">self.register_buffer(<span class="string">&quot;relative_position_index&quot;</span>, relative_position_index)</span><br></pre></td></tr></table></figure></p>
<p><strong>2. Attention Mask</strong><br>通过设置合理的mask，让<code>Shifted Window Attention</code>在与<code>Window Attention</code>相同的窗口个数下，达到等价的计算结果。<br>由于将窗口进行偏移后，由原来的4个窗口变成9个窗口了。后面又要对每个窗口内部进行MSA，这样做感觉又变麻烦了。<br><img src="https://pic.imgdb.cn/item/6317002616f2c2beb1a7fbd0.png" alt=""><br>为了解决这个麻烦，作者通过<code>cycle shift</code>将器还原为原来4个窗口。但是如果对每个窗口内部进行MSA，但是这样操作会使得 window 6 和 4 的 attention 混在一起，window 1,3,7 和 9 的 attention 混在一起。所以需要采用 masked MSA 机制将 self-attention 的计算限制在每个子窗口内。<br><img src="https://minio.cvmart.net/cvmart-community/images/202206/30/0/006C3FgEgy1guaolsl0fgj60rl0cf78502.jpg" alt=""></p>
<p>mask操作如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_mask</span>(<span class="params">self, x, H, W</span>):</span><br><span class="line">    <span class="comment"># calculate attention mask for SW-MSA</span></span><br><span class="line">    <span class="comment"># 保证Hp和Wp是window_size的整数倍</span></span><br><span class="line">    Hp = <span class="built_in">int</span>(np.ceil(H / self.window_size)) * self.window_size</span><br><span class="line">    Wp = <span class="built_in">int</span>(np.ceil(W / self.window_size)) * self.window_size</span><br><span class="line">    <span class="comment"># 拥有和feature map一样的通道排列顺序，方便后续window_partition</span></span><br><span class="line">    img_mask = torch.zeros((<span class="number">1</span>, Hp, Wp, <span class="number">1</span>), device=x.device)  <span class="comment"># [1, Hp, Wp, 1]</span></span><br><span class="line">    h_slices = (<span class="built_in">slice</span>(<span class="number">0</span>, -self.window_size),</span><br><span class="line">                <span class="built_in">slice</span>(-self.window_size, -self.shift_size),</span><br><span class="line">                <span class="built_in">slice</span>(-self.shift_size, <span class="literal">None</span>))</span><br><span class="line">    w_slices = (<span class="built_in">slice</span>(<span class="number">0</span>, -self.window_size),</span><br><span class="line">                <span class="built_in">slice</span>(-self.window_size, -self.shift_size),</span><br><span class="line">                <span class="built_in">slice</span>(-self.shift_size, <span class="literal">None</span>))</span><br><span class="line">    cnt = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> h_slices:</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> w_slices:</span><br><span class="line">            img_mask[:, h, w, :] = cnt</span><br><span class="line">            cnt += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    mask_windows = window_partition(img_mask, self.window_size)  <span class="comment"># [nW, Mh, Mw, 1]</span></span><br><span class="line">    mask_windows = mask_windows.view(-<span class="number">1</span>, self.window_size * self.window_size)  <span class="comment"># [nW, Mh*Mw]</span></span><br><span class="line">    attn_mask = mask_windows.unsqueeze(<span class="number">1</span>) - mask_windows.unsqueeze(<span class="number">2</span>)  <span class="comment"># [nW, 1, Mh*Mw] - [nW, Mh*Mw, 1]</span></span><br><span class="line">    <span class="comment"># [nW, Mh*Mw, Mh*Mw]</span></span><br><span class="line">    attn_mask = attn_mask.masked_fill(attn_mask != <span class="number">0</span>, <span class="built_in">float</span>(-<span class="number">100.0</span>)).masked_fill(attn_mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="number">0.0</span>))</span><br><span class="line">    <span class="keyword">return</span> attn_mask</span><br></pre></td></tr></table></figure></p>
<p><strong>Window Attention 完整代码如下：</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">WindowAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Window based multi-head self attention (W-MSA) module with relative position bias.</span></span><br><span class="line"><span class="string">    It supports both of shifted and non-shifted window.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        window_size (tuple[int]): The height and width of the window.</span></span><br><span class="line"><span class="string">        num_heads (int): Number of attention heads.</span></span><br><span class="line"><span class="string">        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0</span></span><br><span class="line"><span class="string">        proj_drop (float, optional): Dropout ratio of output. Default: 0.0</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, window_size, num_heads, qkv_bias=<span class="literal">True</span>, attn_drop=<span class="number">0.</span>, proj_drop=<span class="number">0.</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.window_size = window_size  <span class="comment"># [Mh, Mw]</span></span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        head_dim = dim // num_heads</span><br><span class="line">        self.scale = head_dim ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># define a parameter table of relative position bias</span></span><br><span class="line">        self.relative_position_bias_table = nn.Parameter(</span><br><span class="line">            torch.zeros((<span class="number">2</span> * window_size[<span class="number">0</span>] - <span class="number">1</span>) * (<span class="number">2</span> * window_size[<span class="number">1</span>] - <span class="number">1</span>), num_heads))  <span class="comment"># [2*Mh-1 * 2*Mw-1, nH]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get pair-wise relative position index for each token inside the window</span></span><br><span class="line">        coords_h = torch.arange(self.window_size[<span class="number">0</span>])</span><br><span class="line">        coords_w = torch.arange(self.window_size[<span class="number">1</span>])</span><br><span class="line">        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=<span class="string">&quot;ij&quot;</span>))  <span class="comment"># [2, Mh, Mw]</span></span><br><span class="line">        coords_flatten = torch.flatten(coords, <span class="number">1</span>)  <span class="comment"># [2, Mh*Mw]</span></span><br><span class="line">        <span class="comment"># [2, Mh*Mw, 1] - [2, 1, Mh*Mw]</span></span><br><span class="line">        relative_coords = coords_flatten[:, :, <span class="literal">None</span>] - coords_flatten[:, <span class="literal">None</span>, :]  <span class="comment"># [2, Mh*Mw, Mh*Mw]</span></span><br><span class="line">        relative_coords = relative_coords.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).contiguous()  <span class="comment"># [Mh*Mw, Mh*Mw, 2]</span></span><br><span class="line">        relative_coords[:, :, <span class="number">0</span>] += self.window_size[<span class="number">0</span>] - <span class="number">1</span>  <span class="comment"># shift to start from 0</span></span><br><span class="line">        relative_coords[:, :, <span class="number">1</span>] += self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">        relative_coords[:, :, <span class="number">0</span>] *= <span class="number">2</span> * self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">        relative_position_index = relative_coords.<span class="built_in">sum</span>(-<span class="number">1</span>)  <span class="comment"># [Mh*Mw, Mh*Mw]</span></span><br><span class="line">        self.register_buffer(<span class="string">&quot;relative_position_index&quot;</span>, relative_position_index)</span><br><span class="line"></span><br><span class="line">        self.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line"></span><br><span class="line">        nn.init.trunc_normal_(self.relative_position_bias_table, std=<span class="number">.02</span>)</span><br><span class="line">        self.softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input features with shape of (num_windows*B, Mh*Mw, C)</span></span><br><span class="line"><span class="string">            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># [batch_size*num_windows, Mh*Mw, total_embed_dim]</span></span><br><span class="line">        B_, N, C = x.shape</span><br><span class="line">        <span class="comment"># qkv(): -&gt; [batch_size*num_windows, Mh*Mw, 3 * total_embed_dim]</span></span><br><span class="line">        <span class="comment"># reshape: -&gt; [batch_size*num_windows, Mh*Mw, 3, num_heads, embed_dim_per_head]</span></span><br><span class="line">        <span class="comment"># permute: -&gt; [3, batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]</span></span><br><span class="line">        qkv = self.qkv(x).reshape(B_, N, <span class="number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="comment"># [batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]</span></span><br><span class="line">        q, k, v = qkv.unbind(<span class="number">0</span>)  <span class="comment"># make torchscript happy (cannot use tensor as tuple)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># transpose: -&gt; [batch_size*num_windows, num_heads, embed_dim_per_head, Mh*Mw]</span></span><br><span class="line">        <span class="comment"># @: multiply -&gt; [batch_size*num_windows, num_heads, Mh*Mw, Mh*Mw]</span></span><br><span class="line">        q = q * self.scale</span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># relative_position_bias_table.view: [Mh*Mw*Mh*Mw,nH] -&gt; [Mh*Mw,Mh*Mw,nH]</span></span><br><span class="line">        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-<span class="number">1</span>)].view(</span><br><span class="line">            self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], -<span class="number">1</span>)</span><br><span class="line">        relative_position_bias = relative_position_bias.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>).contiguous()  <span class="comment"># [nH, Mh*Mw, Mh*Mw]</span></span><br><span class="line">        attn = attn + relative_position_bias.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># mask: [nW, Mh*Mw, Mh*Mw]</span></span><br><span class="line">            nW = mask.shape[<span class="number">0</span>]  <span class="comment"># num_windows</span></span><br><span class="line">            <span class="comment"># attn.view: [batch_size, num_windows, num_heads, Mh*Mw, Mh*Mw]</span></span><br><span class="line">            <span class="comment"># mask.unsqueeze: [1, nW, 1, Mh*Mw, Mh*Mw]</span></span><br><span class="line">            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">            attn = attn.view(-<span class="number">1</span>, self.num_heads, N, N)</span><br><span class="line">            attn = self.softmax(attn)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attn = self.softmax(attn)</span><br><span class="line"></span><br><span class="line">        attn = self.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># @: multiply -&gt; [batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]</span></span><br><span class="line">        <span class="comment"># transpose: -&gt; [batch_size*num_windows, Mh*Mw, num_heads, embed_dim_per_head]</span></span><br><span class="line">        <span class="comment"># reshape: -&gt; [batch_size*num_windows, Mh*Mw, total_embed_dim]</span></span><br><span class="line">        x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B_, N, C)</span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p>
<h4 id="2-3-3-Block"><a href="#2-3-3-Block" class="headerlink" title="2.3.3 Block"></a>2.3.3 Block</h4><p><img src="https://pic.imgdb.cn/item/6317022616f2c2beb1aa61a5.png" alt=""><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SwinTransformerBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Swin Transformer Block.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        num_heads (int): Number of attention heads.</span></span><br><span class="line"><span class="string">        window_size (int): Window size.</span></span><br><span class="line"><span class="string">        shift_size (int): Shift size for SW-MSA.</span></span><br><span class="line"><span class="string">        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.</span></span><br><span class="line"><span class="string">        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        drop (float, optional): Dropout rate. Default: 0.0</span></span><br><span class="line"><span class="string">        attn_drop (float, optional): Attention dropout rate. Default: 0.0</span></span><br><span class="line"><span class="string">        drop_path (float, optional): Stochastic depth rate. Default: 0.0</span></span><br><span class="line"><span class="string">        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads, window_size=<span class="number">7</span>, shift_size=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">                 mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>, drop=<span class="number">0.</span>, attn_drop=<span class="number">0.</span>, drop_path=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 act_layer=nn.GELU, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.window_size = window_size</span><br><span class="line">        self.shift_size = shift_size</span><br><span class="line">        self.mlp_ratio = mlp_ratio</span><br><span class="line">        <span class="keyword">assert</span> <span class="number">0</span> &lt;= self.shift_size &lt; self.window_size, <span class="string">&quot;shift_size must in 0-window_size&quot;</span></span><br><span class="line"></span><br><span class="line">        self.norm1 = norm_layer(dim)</span><br><span class="line">        self.attn = WindowAttention(</span><br><span class="line">            dim, window_size=(self.window_size, self.window_size), num_heads=num_heads, qkv_bias=qkv_bias,</span><br><span class="line">            attn_drop=attn_drop, proj_drop=drop)</span><br><span class="line"></span><br><span class="line">        self.drop_path = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.norm2 = norm_layer(dim)</span><br><span class="line">        mlp_hidden_dim = <span class="built_in">int</span>(dim * mlp_ratio)</span><br><span class="line">        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, attn_mask</span>):</span><br><span class="line">        H, W = self.H, self.W</span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        <span class="keyword">assert</span> L == H * W, <span class="string">&quot;input feature has wrong size&quot;</span></span><br><span class="line"></span><br><span class="line">        shortcut = x</span><br><span class="line">        x = self.norm1(x)</span><br><span class="line">        x = x.view(B, H, W, C)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pad feature maps to multiples of window size</span></span><br><span class="line">        <span class="comment"># 把feature map给pad到window size的整数倍</span></span><br><span class="line">        pad_l = pad_t = <span class="number">0</span></span><br><span class="line">        pad_r = (self.window_size - W % self.window_size) % self.window_size</span><br><span class="line">        pad_b = (self.window_size - H % self.window_size) % self.window_size</span><br><span class="line">        x = F.pad(x, (<span class="number">0</span>, <span class="number">0</span>, pad_l, pad_r, pad_t, pad_b))</span><br><span class="line">        _, Hp, Wp, _ = x.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># cyclic shift</span></span><br><span class="line">        <span class="keyword">if</span> self.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shifted_x = x</span><br><span class="line">            attn_mask = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># partition windows</span></span><br><span class="line">        x_windows = window_partition(shifted_x, self.window_size)  <span class="comment"># [nW*B, Mh, Mw, C]</span></span><br><span class="line">        x_windows = x_windows.view(-<span class="number">1</span>, self.window_size * self.window_size, C)  <span class="comment"># [nW*B, Mh*Mw, C]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># W-MSA/SW-MSA</span></span><br><span class="line">        attn_windows = self.attn(x_windows, mask=attn_mask)  <span class="comment"># [nW*B, Mh*Mw, C]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># merge windows</span></span><br><span class="line">        attn_windows = attn_windows.view(-<span class="number">1</span>, self.window_size, self.window_size, C)  <span class="comment"># [nW*B, Mh, Mw, C]</span></span><br><span class="line">        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  <span class="comment"># [B, H&#x27;, W&#x27;, C]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># reverse cyclic shift</span></span><br><span class="line">        <span class="keyword">if</span> self.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = shifted_x</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> pad_r &gt; <span class="number">0</span> <span class="keyword">or</span> pad_b &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 把前面pad的数据移除掉</span></span><br><span class="line">            x = x[:, :H, :W, :].contiguous()</span><br><span class="line"></span><br><span class="line">        x = x.view(B, H * W, C)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># FFN</span></span><br><span class="line">        x = shortcut + self.drop_path(x)</span><br><span class="line">        x = x + self.drop_path(self.mlp(self.norm2(x)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p>
<h3 id="2-4-Swin-T"><a href="#2-4-Swin-T" class="headerlink" title="2.4 Swin-T"></a>2.4 Swin-T</h3><p><img src="https://pic.imgdb.cn/item/6317028916f2c2beb1aaddb5.png" alt=""><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.utils.checkpoint <span class="keyword">as</span> checkpoint</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">drop_path_f</span>(<span class="params">x, drop_prob: <span class="built_in">float</span> = <span class="number">0.</span>, training: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).</span></span><br><span class="line"><span class="string">    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,</span></span><br><span class="line"><span class="string">    the original name is misleading as &#x27;Drop Connect&#x27; is a different form of dropout in a separate paper...</span></span><br><span class="line"><span class="string">    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I&#x27;ve opted for</span></span><br><span class="line"><span class="string">    changing the layer and argument names to &#x27;drop path&#x27; rather than mix DropConnect as a layer name and use</span></span><br><span class="line"><span class="string">    &#x27;survival rate&#x27; as the argument.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> drop_prob == <span class="number">0.</span> <span class="keyword">or</span> <span class="keyword">not</span> training:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    keep_prob = <span class="number">1</span> - drop_prob</span><br><span class="line">    shape = (x.shape[<span class="number">0</span>],) + (<span class="number">1</span>,) * (x.ndim - <span class="number">1</span>)  <span class="comment"># work with diff dim tensors, not just 2D ConvNets</span></span><br><span class="line">    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)</span><br><span class="line">    random_tensor.floor_()  <span class="comment"># binarize</span></span><br><span class="line">    output = x.div(keep_prob) * random_tensor</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DropPath</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, drop_prob=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(DropPath, self).__init__()</span><br><span class="line">        self.drop_prob = drop_prob</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> drop_path_f(x, self.drop_prob, self.training)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">window_partition</span>(<span class="params">x, window_size: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将feature map按照window_size划分成一个个没有重叠的window</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        x: (B, H, W, C)</span></span><br><span class="line"><span class="string">        window_size (int): window size(M)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        windows: (num_windows*B, window_size, window_size, C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    B, H, W, C = x.shape</span><br><span class="line">    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)</span><br><span class="line">    <span class="comment"># permute: [B, H//Mh, Mh, W//Mw, Mw, C] -&gt; [B, H//Mh, W//Mh, Mw, Mw, C]</span></span><br><span class="line">    <span class="comment"># view: [B, H//Mh, W//Mw, Mh, Mw, C] -&gt; [B*num_windows, Mh, Mw, C]</span></span><br><span class="line">    windows = x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>).contiguous().view(-<span class="number">1</span>, window_size, window_size, C)</span><br><span class="line">    <span class="keyword">return</span> windows</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">window_reverse</span>(<span class="params">windows, window_size: <span class="built_in">int</span>, H: <span class="built_in">int</span>, W: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将一个个window还原成一个feature map</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        windows: (num_windows*B, window_size, window_size, C)</span></span><br><span class="line"><span class="string">        window_size (int): Window size(M)</span></span><br><span class="line"><span class="string">        H (int): Height of image</span></span><br><span class="line"><span class="string">        W (int): Width of image</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        x: (B, H, W, C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    B = <span class="built_in">int</span>(windows.shape[<span class="number">0</span>] / (H * W / window_size / window_size))</span><br><span class="line">    <span class="comment"># view: [B*num_windows, Mh, Mw, C] -&gt; [B, H//Mh, W//Mw, Mh, Mw, C]</span></span><br><span class="line">    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># permute: [B, H//Mh, W//Mw, Mh, Mw, C] -&gt; [B, H//Mh, Mh, W//Mw, Mw, C]</span></span><br><span class="line">    <span class="comment"># view: [B, H//Mh, Mh, W//Mw, Mw, C] -&gt; [B, H, W, C]</span></span><br><span class="line">    x = x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>).contiguous().view(B, H, W, -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    2D Image to Patch Embedding</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, patch_size=<span class="number">4</span>, in_c=<span class="number">3</span>, embed_dim=<span class="number">96</span>, norm_layer=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        patch_size = (patch_size, patch_size)</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.in_chans = in_c</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line">        self.norm = norm_layer(embed_dim) <span class="keyword">if</span> norm_layer <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        _, _, H, W = x.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># padding</span></span><br><span class="line">        <span class="comment"># 如果输入图片的H，W不是patch_size的整数倍，需要进行padding</span></span><br><span class="line">        pad_input = (H % self.patch_size[<span class="number">0</span>] != <span class="number">0</span>) <span class="keyword">or</span> (W % self.patch_size[<span class="number">1</span>] != <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> pad_input:</span><br><span class="line">            <span class="comment"># to pad the last 3 dimensions,</span></span><br><span class="line">            <span class="comment"># (W_left, W_right, H_top,H_bottom, C_front, C_back)</span></span><br><span class="line">            x = F.pad(x, (<span class="number">0</span>, self.patch_size[<span class="number">1</span>] - W % self.patch_size[<span class="number">1</span>],</span><br><span class="line">                          <span class="number">0</span>, self.patch_size[<span class="number">0</span>] - H % self.patch_size[<span class="number">0</span>],</span><br><span class="line">                          <span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 下采样patch_size倍</span></span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        _, _, H, W = x.shape</span><br><span class="line">        <span class="comment"># flatten: [B, C, H, W] -&gt; [B, C, HW]</span></span><br><span class="line">        <span class="comment"># transpose: [B, C, HW] -&gt; [B, HW, C]</span></span><br><span class="line">        x = x.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x, H, W</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PatchMerging</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Patch Merging Layer.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.reduction = nn.Linear(<span class="number">4</span> * dim, <span class="number">2</span> * dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.norm = norm_layer(<span class="number">4</span> * dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, H, W</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x: B, H*W, C</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        <span class="keyword">assert</span> L == H * W, <span class="string">&quot;input feature has wrong size&quot;</span></span><br><span class="line"></span><br><span class="line">        x = x.view(B, H, W, C)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># padding</span></span><br><span class="line">        <span class="comment"># 如果输入feature map的H，W不是2的整数倍，需要进行padding</span></span><br><span class="line">        pad_input = (H % <span class="number">2</span> == <span class="number">1</span>) <span class="keyword">or</span> (W % <span class="number">2</span> == <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> pad_input:</span><br><span class="line">            <span class="comment"># to pad the last 3 dimensions, starting from the last dimension and moving forward.</span></span><br><span class="line">            <span class="comment"># (C_front, C_back, W_left, W_right, H_top, H_bottom)</span></span><br><span class="line">            <span class="comment"># 注意这里的Tensor通道是[B, H, W, C]，所以会和官方文档有些不同</span></span><br><span class="line">            x = F.pad(x, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, W % <span class="number">2</span>, <span class="number">0</span>, H % <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        x0 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># [B, H/2, W/2, C]</span></span><br><span class="line">        x1 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># [B, H/2, W/2, C]</span></span><br><span class="line">        x2 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># [B, H/2, W/2, C]</span></span><br><span class="line">        x3 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># [B, H/2, W/2, C]</span></span><br><span class="line">        x = torch.cat([x0, x1, x2, x3], -<span class="number">1</span>)  <span class="comment"># [B, H/2, W/2, 4*C]</span></span><br><span class="line">        x = x.view(B, -<span class="number">1</span>, <span class="number">4</span> * C)  <span class="comment"># [B, H/2*W/2, 4*C]</span></span><br><span class="line"></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        x = self.reduction(x)  <span class="comment"># [B, H/2*W/2, 2*C]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Mlp</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; MLP as used in Vision Transformer, MLP-Mixer and related networks</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, hidden_features=<span class="literal">None</span>, out_features=<span class="literal">None</span>, act_layer=nn.GELU, drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        out_features = out_features <span class="keyword">or</span> in_features</span><br><span class="line">        hidden_features = hidden_features <span class="keyword">or</span> in_features</span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(in_features, hidden_features)</span><br><span class="line">        self.act = act_layer()</span><br><span class="line">        self.drop1 = nn.Dropout(drop)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_features, out_features)</span><br><span class="line">        self.drop2 = nn.Dropout(drop)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.act(x)</span><br><span class="line">        x = self.drop1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.drop2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WindowAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Window based multi-head self attention (W-MSA) module with relative position bias.</span></span><br><span class="line"><span class="string">    It supports both of shifted and non-shifted window.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        window_size (tuple[int]): The height and width of the window.</span></span><br><span class="line"><span class="string">        num_heads (int): Number of attention heads.</span></span><br><span class="line"><span class="string">        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0</span></span><br><span class="line"><span class="string">        proj_drop (float, optional): Dropout ratio of output. Default: 0.0</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, window_size, num_heads, qkv_bias=<span class="literal">True</span>, attn_drop=<span class="number">0.</span>, proj_drop=<span class="number">0.</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.window_size = window_size  <span class="comment"># [Mh, Mw]</span></span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        head_dim = dim // num_heads</span><br><span class="line">        self.scale = head_dim ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># define a parameter table of relative position bias</span></span><br><span class="line">        self.relative_position_bias_table = nn.Parameter(</span><br><span class="line">            torch.zeros((<span class="number">2</span> * window_size[<span class="number">0</span>] - <span class="number">1</span>) * (<span class="number">2</span> * window_size[<span class="number">1</span>] - <span class="number">1</span>), num_heads))  <span class="comment"># [2*Mh-1 * 2*Mw-1, nH]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get pair-wise relative position index for each token inside the window</span></span><br><span class="line">        coords_h = torch.arange(self.window_size[<span class="number">0</span>])</span><br><span class="line">        coords_w = torch.arange(self.window_size[<span class="number">1</span>])</span><br><span class="line">        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=<span class="string">&quot;ij&quot;</span>))  <span class="comment"># [2, Mh, Mw]</span></span><br><span class="line">        coords_flatten = torch.flatten(coords, <span class="number">1</span>)  <span class="comment"># [2, Mh*Mw]</span></span><br><span class="line">        <span class="comment"># [2, Mh*Mw, 1] - [2, 1, Mh*Mw]</span></span><br><span class="line">        relative_coords = coords_flatten[:, :, <span class="literal">None</span>] - coords_flatten[:, <span class="literal">None</span>, :]  <span class="comment"># [2, Mh*Mw, Mh*Mw]</span></span><br><span class="line">        relative_coords = relative_coords.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).contiguous()  <span class="comment"># [Mh*Mw, Mh*Mw, 2]</span></span><br><span class="line">        relative_coords[:, :, <span class="number">0</span>] += self.window_size[<span class="number">0</span>] - <span class="number">1</span>  <span class="comment"># shift to start from 0</span></span><br><span class="line">        relative_coords[:, :, <span class="number">1</span>] += self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">        relative_coords[:, :, <span class="number">0</span>] *= <span class="number">2</span> * self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">        relative_position_index = relative_coords.<span class="built_in">sum</span>(-<span class="number">1</span>)  <span class="comment"># [Mh*Mw, Mh*Mw]</span></span><br><span class="line">        self.register_buffer(<span class="string">&quot;relative_position_index&quot;</span>, relative_position_index)</span><br><span class="line"></span><br><span class="line">        self.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line"></span><br><span class="line">        nn.init.trunc_normal_(self.relative_position_bias_table, std=<span class="number">.02</span>)</span><br><span class="line">        self.softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input features with shape of (num_windows*B, Mh*Mw, C)</span></span><br><span class="line"><span class="string">            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># [batch_size*num_windows, Mh*Mw, total_embed_dim]</span></span><br><span class="line">        B_, N, C = x.shape</span><br><span class="line">        <span class="comment"># qkv(): -&gt; [batch_size*num_windows, Mh*Mw, 3 * total_embed_dim]</span></span><br><span class="line">        <span class="comment"># reshape: -&gt; [batch_size*num_windows, Mh*Mw, 3, num_heads, embed_dim_per_head]</span></span><br><span class="line">        <span class="comment"># permute: -&gt; [3, batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]</span></span><br><span class="line">        qkv = self.qkv(x).reshape(B_, N, <span class="number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="comment"># [batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]</span></span><br><span class="line">        q, k, v = qkv.unbind(<span class="number">0</span>)  <span class="comment"># make torchscript happy (cannot use tensor as tuple)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># transpose: -&gt; [batch_size*num_windows, num_heads, embed_dim_per_head, Mh*Mw]</span></span><br><span class="line">        <span class="comment"># @: multiply -&gt; [batch_size*num_windows, num_heads, Mh*Mw, Mh*Mw]</span></span><br><span class="line">        q = q * self.scale</span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># relative_position_bias_table.view: [Mh*Mw*Mh*Mw,nH] -&gt; [Mh*Mw,Mh*Mw,nH]</span></span><br><span class="line">        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-<span class="number">1</span>)].view(</span><br><span class="line">            self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], -<span class="number">1</span>)</span><br><span class="line">        relative_position_bias = relative_position_bias.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>).contiguous()  <span class="comment"># [nH, Mh*Mw, Mh*Mw]</span></span><br><span class="line">        attn = attn + relative_position_bias.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># mask: [nW, Mh*Mw, Mh*Mw]</span></span><br><span class="line">            nW = mask.shape[<span class="number">0</span>]  <span class="comment"># num_windows</span></span><br><span class="line">            <span class="comment"># attn.view: [batch_size, num_windows, num_heads, Mh*Mw, Mh*Mw]</span></span><br><span class="line">            <span class="comment"># mask.unsqueeze: [1, nW, 1, Mh*Mw, Mh*Mw]</span></span><br><span class="line">            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">            attn = attn.view(-<span class="number">1</span>, self.num_heads, N, N)</span><br><span class="line">            attn = self.softmax(attn)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attn = self.softmax(attn)</span><br><span class="line"></span><br><span class="line">        attn = self.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># @: multiply -&gt; [batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]</span></span><br><span class="line">        <span class="comment"># transpose: -&gt; [batch_size*num_windows, Mh*Mw, num_heads, embed_dim_per_head]</span></span><br><span class="line">        <span class="comment"># reshape: -&gt; [batch_size*num_windows, Mh*Mw, total_embed_dim]</span></span><br><span class="line">        x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B_, N, C)</span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SwinTransformerBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Swin Transformer Block.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        num_heads (int): Number of attention heads.</span></span><br><span class="line"><span class="string">        window_size (int): Window size.</span></span><br><span class="line"><span class="string">        shift_size (int): Shift size for SW-MSA.</span></span><br><span class="line"><span class="string">        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.</span></span><br><span class="line"><span class="string">        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        drop (float, optional): Dropout rate. Default: 0.0</span></span><br><span class="line"><span class="string">        attn_drop (float, optional): Attention dropout rate. Default: 0.0</span></span><br><span class="line"><span class="string">        drop_path (float, optional): Stochastic depth rate. Default: 0.0</span></span><br><span class="line"><span class="string">        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads, window_size=<span class="number">7</span>, shift_size=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">                 mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>, drop=<span class="number">0.</span>, attn_drop=<span class="number">0.</span>, drop_path=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 act_layer=nn.GELU, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.window_size = window_size</span><br><span class="line">        self.shift_size = shift_size</span><br><span class="line">        self.mlp_ratio = mlp_ratio</span><br><span class="line">        <span class="keyword">assert</span> <span class="number">0</span> &lt;= self.shift_size &lt; self.window_size, <span class="string">&quot;shift_size must in 0-window_size&quot;</span></span><br><span class="line"></span><br><span class="line">        self.norm1 = norm_layer(dim)</span><br><span class="line">        self.attn = WindowAttention(</span><br><span class="line">            dim, window_size=(self.window_size, self.window_size), num_heads=num_heads, qkv_bias=qkv_bias,</span><br><span class="line">            attn_drop=attn_drop, proj_drop=drop)</span><br><span class="line"></span><br><span class="line">        self.drop_path = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.norm2 = norm_layer(dim)</span><br><span class="line">        mlp_hidden_dim = <span class="built_in">int</span>(dim * mlp_ratio)</span><br><span class="line">        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, attn_mask</span>):</span><br><span class="line">        H, W = self.H, self.W</span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        <span class="keyword">assert</span> L == H * W, <span class="string">&quot;input feature has wrong size&quot;</span></span><br><span class="line"></span><br><span class="line">        shortcut = x</span><br><span class="line">        x = self.norm1(x)</span><br><span class="line">        x = x.view(B, H, W, C)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pad feature maps to multiples of window size</span></span><br><span class="line">        <span class="comment"># 把feature map给pad到window size的整数倍</span></span><br><span class="line">        pad_l = pad_t = <span class="number">0</span></span><br><span class="line">        pad_r = (self.window_size - W % self.window_size) % self.window_size</span><br><span class="line">        pad_b = (self.window_size - H % self.window_size) % self.window_size</span><br><span class="line">        x = F.pad(x, (<span class="number">0</span>, <span class="number">0</span>, pad_l, pad_r, pad_t, pad_b))</span><br><span class="line">        _, Hp, Wp, _ = x.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># cyclic shift</span></span><br><span class="line">        <span class="keyword">if</span> self.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shifted_x = x</span><br><span class="line">            attn_mask = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># partition windows</span></span><br><span class="line">        x_windows = window_partition(shifted_x, self.window_size)  <span class="comment"># [nW*B, Mh, Mw, C]</span></span><br><span class="line">        x_windows = x_windows.view(-<span class="number">1</span>, self.window_size * self.window_size, C)  <span class="comment"># [nW*B, Mh*Mw, C]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># W-MSA/SW-MSA</span></span><br><span class="line">        attn_windows = self.attn(x_windows, mask=attn_mask)  <span class="comment"># [nW*B, Mh*Mw, C]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># merge windows</span></span><br><span class="line">        attn_windows = attn_windows.view(-<span class="number">1</span>, self.window_size, self.window_size, C)  <span class="comment"># [nW*B, Mh, Mw, C]</span></span><br><span class="line">        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  <span class="comment"># [B, H&#x27;, W&#x27;, C]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># reverse cyclic shift</span></span><br><span class="line">        <span class="keyword">if</span> self.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = shifted_x</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> pad_r &gt; <span class="number">0</span> <span class="keyword">or</span> pad_b &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 把前面pad的数据移除掉</span></span><br><span class="line">            x = x[:, :H, :W, :].contiguous()</span><br><span class="line"></span><br><span class="line">        x = x.view(B, H * W, C)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># FFN</span></span><br><span class="line">        x = shortcut + self.drop_path(x)</span><br><span class="line">        x = x + self.drop_path(self.mlp(self.norm2(x)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A basic Swin Transformer layer for one stage.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        depth (int): Number of blocks.</span></span><br><span class="line"><span class="string">        num_heads (int): Number of attention heads.</span></span><br><span class="line"><span class="string">        window_size (int): Local window size.</span></span><br><span class="line"><span class="string">        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.</span></span><br><span class="line"><span class="string">        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        drop (float, optional): Dropout rate. Default: 0.0</span></span><br><span class="line"><span class="string">        attn_drop (float, optional): Attention dropout rate. Default: 0.0</span></span><br><span class="line"><span class="string">        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm</span></span><br><span class="line"><span class="string">        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None</span></span><br><span class="line"><span class="string">        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, depth, num_heads, window_size,</span></span><br><span class="line"><span class="params">                 mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>, drop=<span class="number">0.</span>, attn_drop=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 drop_path=<span class="number">0.</span>, norm_layer=nn.LayerNorm, downsample=<span class="literal">None</span>, use_checkpoint=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.depth = depth</span><br><span class="line">        self.window_size = window_size</span><br><span class="line">        self.use_checkpoint = use_checkpoint</span><br><span class="line">        self.shift_size = window_size // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># build blocks</span></span><br><span class="line">        self.blocks = nn.ModuleList([</span><br><span class="line">            SwinTransformerBlock(</span><br><span class="line">                dim=dim,</span><br><span class="line">                num_heads=num_heads,</span><br><span class="line">                window_size=window_size,</span><br><span class="line">                shift_size=<span class="number">0</span> <span class="keyword">if</span> (i % <span class="number">2</span> == <span class="number">0</span>) <span class="keyword">else</span> self.shift_size,</span><br><span class="line">                mlp_ratio=mlp_ratio,</span><br><span class="line">                qkv_bias=qkv_bias,</span><br><span class="line">                drop=drop,</span><br><span class="line">                attn_drop=attn_drop,</span><br><span class="line">                drop_path=drop_path[i] <span class="keyword">if</span> <span class="built_in">isinstance</span>(drop_path, <span class="built_in">list</span>) <span class="keyword">else</span> drop_path,</span><br><span class="line">                norm_layer=norm_layer)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># patch merging layer</span></span><br><span class="line">        <span class="keyword">if</span> downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.downsample = downsample(dim=dim, norm_layer=norm_layer)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.downsample = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">create_mask</span>(<span class="params">self, x, H, W</span>):</span><br><span class="line">        <span class="comment"># calculate attention mask for SW-MSA</span></span><br><span class="line">        <span class="comment"># 保证Hp和Wp是window_size的整数倍</span></span><br><span class="line">        Hp = <span class="built_in">int</span>(np.ceil(H / self.window_size)) * self.window_size</span><br><span class="line">        Wp = <span class="built_in">int</span>(np.ceil(W / self.window_size)) * self.window_size</span><br><span class="line">        <span class="comment"># 拥有和feature map一样的通道排列顺序，方便后续window_partition</span></span><br><span class="line">        img_mask = torch.zeros((<span class="number">1</span>, Hp, Wp, <span class="number">1</span>), device=x.device)  <span class="comment"># [1, Hp, Wp, 1]</span></span><br><span class="line">        h_slices = (<span class="built_in">slice</span>(<span class="number">0</span>, -self.window_size),</span><br><span class="line">                    <span class="built_in">slice</span>(-self.window_size, -self.shift_size),</span><br><span class="line">                    <span class="built_in">slice</span>(-self.shift_size, <span class="literal">None</span>))</span><br><span class="line">        w_slices = (<span class="built_in">slice</span>(<span class="number">0</span>, -self.window_size),</span><br><span class="line">                    <span class="built_in">slice</span>(-self.window_size, -self.shift_size),</span><br><span class="line">                    <span class="built_in">slice</span>(-self.shift_size, <span class="literal">None</span>))</span><br><span class="line">        cnt = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> h_slices:</span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> w_slices:</span><br><span class="line">                img_mask[:, h, w, :] = cnt</span><br><span class="line">                cnt += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        mask_windows = window_partition(img_mask, self.window_size)  <span class="comment"># [nW, Mh, Mw, 1]</span></span><br><span class="line">        mask_windows = mask_windows.view(-<span class="number">1</span>, self.window_size * self.window_size)  <span class="comment"># [nW, Mh*Mw]</span></span><br><span class="line">        attn_mask = mask_windows.unsqueeze(<span class="number">1</span>) - mask_windows.unsqueeze(<span class="number">2</span>)  <span class="comment"># [nW, 1, Mh*Mw] - [nW, Mh*Mw, 1]</span></span><br><span class="line">        <span class="comment"># [nW, Mh*Mw, Mh*Mw]</span></span><br><span class="line">        attn_mask = attn_mask.masked_fill(attn_mask != <span class="number">0</span>, <span class="built_in">float</span>(-<span class="number">100.0</span>)).masked_fill(attn_mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="number">0.0</span>))</span><br><span class="line">        <span class="keyword">return</span> attn_mask</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, H, W</span>):</span><br><span class="line">        attn_mask = self.create_mask(x, H, W)  <span class="comment"># [nW, Mh*Mw, Mh*Mw]</span></span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blocks:</span><br><span class="line">            blk.H, blk.W = H, W</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> torch.jit.is_scripting() <span class="keyword">and</span> self.use_checkpoint:</span><br><span class="line">                x = checkpoint.checkpoint(blk, x, attn_mask)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                x = blk(x, attn_mask)</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.downsample(x, H, W)</span><br><span class="line">            H, W = (H + <span class="number">1</span>) // <span class="number">2</span>, (W + <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x, H, W</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SwinTransformer</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Swin Transformer</span></span><br><span class="line"><span class="string">        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -</span></span><br><span class="line"><span class="string">          https://arxiv.org/pdf/2103.14030</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        patch_size (int | tuple(int)): Patch size. Default: 4</span></span><br><span class="line"><span class="string">        in_chans (int): Number of input image channels. Default: 3</span></span><br><span class="line"><span class="string">        num_classes (int): Number of classes for classification head. Default: 1000</span></span><br><span class="line"><span class="string">        embed_dim (int): Patch embedding dimension. Default: 96</span></span><br><span class="line"><span class="string">        depths (tuple(int)): Depth of each Swin Transformer layer.</span></span><br><span class="line"><span class="string">        num_heads (tuple(int)): Number of attention heads in different layers.</span></span><br><span class="line"><span class="string">        window_size (int): Window size. Default: 7</span></span><br><span class="line"><span class="string">        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4</span></span><br><span class="line"><span class="string">        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        drop_rate (float): Dropout rate. Default: 0</span></span><br><span class="line"><span class="string">        attn_drop_rate (float): Attention dropout rate. Default: 0</span></span><br><span class="line"><span class="string">        drop_path_rate (float): Stochastic depth rate. Default: 0.1</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.</span></span><br><span class="line"><span class="string">        patch_norm (bool): If True, add normalization after patch embedding. Default: True</span></span><br><span class="line"><span class="string">        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, patch_size=<span class="number">4</span>, in_chans=<span class="number">3</span>, num_classes=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 embed_dim=<span class="number">96</span>, depths=(<span class="params"><span class="number">2</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">2</span></span>), num_heads=(<span class="params"><span class="number">3</span>, <span class="number">6</span>, <span class="number">12</span>, <span class="number">24</span></span>),</span></span><br><span class="line"><span class="params">                 window_size=<span class="number">7</span>, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 drop_rate=<span class="number">0.</span>, attn_drop_rate=<span class="number">0.</span>, drop_path_rate=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                 norm_layer=nn.LayerNorm, patch_norm=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 use_checkpoint=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.num_layers = <span class="built_in">len</span>(depths)</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.patch_norm = patch_norm</span><br><span class="line">        <span class="comment"># stage4输出特征矩阵的channels</span></span><br><span class="line">        self.num_features = <span class="built_in">int</span>(embed_dim * <span class="number">2</span> ** (self.num_layers - <span class="number">1</span>))</span><br><span class="line">        self.mlp_ratio = mlp_ratio</span><br><span class="line"></span><br><span class="line">        <span class="comment"># split image into non-overlapping patches</span></span><br><span class="line">        self.patch_embed = PatchEmbed(</span><br><span class="line">            patch_size=patch_size, in_c=in_chans, embed_dim=embed_dim,</span><br><span class="line">            norm_layer=norm_layer <span class="keyword">if</span> self.patch_norm <span class="keyword">else</span> <span class="literal">None</span>)</span><br><span class="line">        self.pos_drop = nn.Dropout(p=drop_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># stochastic depth</span></span><br><span class="line">        dpr = [x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_rate, <span class="built_in">sum</span>(depths))]  <span class="comment"># stochastic depth decay rule</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># build layers</span></span><br><span class="line">        self.layers = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> i_layer <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line">            <span class="comment"># 注意这里构建的stage和论文图中有些差异</span></span><br><span class="line">            <span class="comment"># 这里的stage不包含该stage的patch_merging层，包含的是下个stage的</span></span><br><span class="line">            layers = BasicLayer(dim=<span class="built_in">int</span>(embed_dim * <span class="number">2</span> ** i_layer),</span><br><span class="line">                                depth=depths[i_layer],</span><br><span class="line">                                num_heads=num_heads[i_layer],</span><br><span class="line">                                window_size=window_size,</span><br><span class="line">                                mlp_ratio=self.mlp_ratio,</span><br><span class="line">                                qkv_bias=qkv_bias,</span><br><span class="line">                                drop=drop_rate,</span><br><span class="line">                                attn_drop=attn_drop_rate,</span><br><span class="line">                                drop_path=dpr[<span class="built_in">sum</span>(depths[:i_layer]):<span class="built_in">sum</span>(depths[:i_layer + <span class="number">1</span>])],</span><br><span class="line">                                norm_layer=norm_layer,</span><br><span class="line">                                downsample=PatchMerging <span class="keyword">if</span> (i_layer &lt; self.num_layers - <span class="number">1</span>) <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">                                use_checkpoint=use_checkpoint)</span><br><span class="line">            self.layers.append(layers)</span><br><span class="line"></span><br><span class="line">        self.norm = norm_layer(self.num_features)</span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool1d(<span class="number">1</span>)</span><br><span class="line">        self.head = nn.Linear(self.num_features, num_classes) <span class="keyword">if</span> num_classes &gt; <span class="number">0</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">        self.apply(self._init_weights)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_weights</span>(<span class="params">self, m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">            nn.init.trunc_normal_(m.weight, std=<span class="number">.02</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear) <span class="keyword">and</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.LayerNorm):</span><br><span class="line">            nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            nn.init.constant_(m.weight, <span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x: [B, L, C]</span></span><br><span class="line">        x, H, W = self.patch_embed(x)</span><br><span class="line">        x = self.pos_drop(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x, H, W = layer(x, H, W)</span><br><span class="line"></span><br><span class="line">        x = self.norm(x)  <span class="comment"># [B, L, C]</span></span><br><span class="line">        x = self.avgpool(x.transpose(<span class="number">1</span>, <span class="number">2</span>))  <span class="comment"># [B, C, 1]</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        x = self.head(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p>
<h2 id="3-模型特点"><a href="#3-模型特点" class="headerlink" title="3. 模型特点"></a>3. 模型特点</h2><ol>
<li><p>首次在cv领域的transformer模型中采用了分层结构。分层结构因为其不同大小的尺度，使不同层特征有了更加不同的意义，较浅层的特征具有大尺度和细节信息，较深层的特征具有小尺度和物体的整体轮廓信息，在图像分类领域，深层特征具有更加有用的作用，只需要根据这个信息判定物体的类别即可，但是在像素级的分割和检测任务中，则需要更为精细的细节信息，因此，分层结构的模型往往更适用于分割和检测这样的像素级要求的任务中。Swin Transformer 模仿ResNet采取了分层的结构，使其成为了cv领域的通用框架。</p>
</li>
<li><p>引入locality思想，对无重合的window区域内进行self-attention计算。不仅减少了计算量，而且多了不同窗口之间的交互。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>图像分类</category>
      </categories>
      <tags>
        <tag>模型</tag>
        <tag>backbone</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention UNet</title>
    <url>/2022/10/27/AttUNet/</url>
    <content><![CDATA[<h1 id="Attention-UNet论文详解"><a href="#Attention-UNet论文详解" class="headerlink" title="Attention UNet论文详解"></a>Attention UNet论文详解</h1><p>在医疗图像中，就是把注意力集中到对特定任务有用的显著特征（比如说相关组织或者是器官），抑制输入图像中的不相关区域。论文中是以U-net为基础进行集成，在<code>decoder</code>部分使用了<code>Attention Gates</code> ，得到了<code>Attention U-Net</code>模型。实验表明，融入<code>AG</code>后，<code>Unet</code>模型的精度更高了。<br><span id="more"></span></p>
<h2 id="1-论文简介"><a href="#1-论文简介" class="headerlink" title="1. 论文简介"></a>1. 论文简介</h2><h3 id="1-1-主要目标"><a href="#1-1-主要目标" class="headerlink" title="1.1 主要目标"></a>1.1 主要目标</h3><ul>
<li>抑制输入图像中的不相关区域，同时突出特定局部区域的显著特征</li>
<li>用soft-attention 代替hard-attention的思路（注意：sorf-attention可微，可以微分的attention就可以通过神经网络算出梯度并且前向传播和后向反馈来学习得到attention的权重）</li>
<li>集成到标准UNet网络结构中时要简单方便、计算开销小，最重要的是提高模型的灵敏度和预测的精度</li>
</ul>
<h3 id="1-2-网络结构"><a href="#1-2-网络结构" class="headerlink" title="1.2 网络结构"></a>1.2 网络结构</h3><p><img src="https://pic1.imgdb.cn/item/635a352f16f2c2beb1c176bb.png" alt=""></p>
<h2 id="2-代码实现-Paddle"><a href="#2-代码实现-Paddle" class="headerlink" title="2. 代码实现(Paddle)"></a>2. 代码实现(Paddle)</h2><p><img src="https://pic1.imgdb.cn/item/635a356c16f2c2beb1c1d6dd.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AttentionBlock</span>(nn.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Attention Gate 模块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, F_g, F_l, F_out</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.W_g = nn.Sequential(</span><br><span class="line">            nn.Conv2D(</span><br><span class="line">                F_g, F_out, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</span><br><span class="line">            nn.BatchNorm2D(F_out))</span><br><span class="line"></span><br><span class="line">        self.W_x = nn.Sequential(</span><br><span class="line">            nn.Conv2D(</span><br><span class="line">                F_l, F_out, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</span><br><span class="line">            nn.BatchNorm2D(F_out))</span><br><span class="line"></span><br><span class="line">        self.psi = nn.Sequential(</span><br><span class="line">            nn.Conv2D(</span><br><span class="line">                F_out, <span class="number">1</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</span><br><span class="line">            nn.BatchNorm2D(<span class="number">1</span>),</span><br><span class="line">            nn.Sigmoid())</span><br><span class="line"></span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, g, x</span>):</span><br><span class="line">        g1 = self.W_g(g)</span><br><span class="line">        x1 = self.W_x(x)</span><br><span class="line">        psi = self.relu(g1 + x1)</span><br><span class="line">        psi = self.psi(psi)</span><br><span class="line">        res = x * psi</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>完整代码：<a href="https://github.com/PaddlePaddle/PaddleSeg/blob/release%2F2.6/paddleseg/models/attention_unet.py">https://github.com/PaddlePaddle/PaddleSeg/blob/release%2F2.6/paddleseg/models/attention_unet.py</a></strong></p>
]]></content>
      <categories>
        <category>图像分割</category>
      </categories>
      <tags>
        <tag>模型</tag>
        <tag>图像分割</tag>
      </tags>
  </entry>
  <entry>
    <title>U2Net</title>
    <url>/2022/10/28/U2Net/</url>
    <content><![CDATA[<h1 id="U-2-Net-论文详解"><a href="#U-2-Net-论文详解" class="headerlink" title="$U^2-Net$论文详解"></a>$U^2-Net$论文详解</h1><h2 id="1-论文简介"><a href="#1-论文简介" class="headerlink" title="1. 论文简介"></a>1. 论文简介</h2><p>设计了一个简单而强大的深度网络架构$U^2-Net$，用于显著目标检测。我们的$U^2-Net$的体系结构是一个两层嵌套的$U$结构。</p>
<p>该设计有以下两点优势：</p>
<ul>
<li>它能够捕捉更多的上下文信息，因为提出了<code>RSU(ReSidual U-blocks)</code>结构，融合了不同尺度的感受野的特征</li>
<li>它增加了整个架构的深度但并没有显著增加计算成本，因为在这些RSU块中使用了池化操作</li>
</ul>
<span id="more"></span>
<p>这种架构使我们能够从头开始训练深度网络，而无需使用图像分类任务中的<code>backbone</code>。</p>
<h2 id="2-网络结构解析"><a href="#2-网络结构解析" class="headerlink" title="2. 网络结构解析"></a>2. 网络结构解析</h2><p><img src="https://pic1.imgdb.cn/item/635bc24116f2c2beb152b475.png" alt=""></p>
<p>上图展示了整个$U^2-Net$网络的结构。通过下图可以看到网络的主体是一个类似UNet的结构，网络的中的每个<code>Encoder</code>和<code>Decoder</code>模块也是类似<code>UNet</code>的结构，也就是在大的<code>UNet</code>中嵌入了一堆小<code>UNet</code>。</p>
<p>其中，<code>En_1</code>、<code>En_2</code>、<code>En_3</code>、<code>En_4</code>、<code>De_1</code>、<code>De_2</code>、<code>De_3</code>、<code>De_4</code>采用的是同一种<code>Block</code>，只不过深度不同，该Block就是论文中提出的<code>ReSidual U-block</code>简称<code>RSU</code>。最后还剩下<code>En_5</code>、<code>En_6</code>和<code>De_5</code>三个模块。这三个模块采用的是<code>RSU-4F</code>，注意<code>RSU-4F</code>和<code>RSU-4</code>两者结构并不相同。在<code>RSU-4F</code>中并没有进行下采样或上采样，而是将采样层全部替换成了膨胀卷积。(其中<code>4</code>代表<code>RSU</code>的深度)。下图为<code>RSU-7</code>的结构图：<br><img src="https://pic1.imgdb.cn/item/635bc3bf16f2c2beb154b9ae.png" alt=""></p>
<h2 id="3-代码实现-Paddle"><a href="#3-代码实现-Paddle" class="headerlink" title="3. 代码实现(Paddle)"></a>3. 代码实现(Paddle)</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">REBNCONV</span>(nn.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;实现Conv+BN+ReLU模块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_ch=<span class="number">3</span>, out_ch=<span class="number">3</span>, dirate=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(REBNCONV, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.conv_s1 = nn.Conv2D(</span><br><span class="line">            in_ch, out_ch, <span class="number">3</span>, padding=<span class="number">1</span> * dirate, dilation=<span class="number">1</span> * dirate)</span><br><span class="line">        self.bn_s1 = nn.BatchNorm2D(out_ch)</span><br><span class="line">        self.relu_s1 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line"></span><br><span class="line">        hx = x</span><br><span class="line">        xout = self.relu_s1(self.bn_s1(self.conv_s1(hx)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> xout</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_upsample_like</span>(<span class="params">src, tar</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;上采样&quot;&quot;&quot;</span></span><br><span class="line">    src = F.upsample(src, size=paddle.shape(tar)[<span class="number">2</span>:], mode=<span class="string">&#x27;bilinear&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> src</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RSU7</span>(nn.Layer):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;实现RSU-7即En_1&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_ch=<span class="number">3</span>, mid_ch=<span class="number">12</span>, out_ch=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(RSU7, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=<span class="number">1</span>)</span><br><span class="line">        self.pool1 = nn.MaxPool2D(<span class="number">2</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=<span class="number">1</span>)</span><br><span class="line">        self.pool2 = nn.MaxPool2D(<span class="number">2</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=<span class="number">1</span>)</span><br><span class="line">        self.pool3 = nn.MaxPool2D(<span class="number">2</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=<span class="number">1</span>)</span><br><span class="line">        self.pool4 = nn.MaxPool2D(<span class="number">2</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.rebnconv5 = REBNCONV(mid_ch, mid_ch, dirate=<span class="number">1</span>)</span><br><span class="line">        self.pool5 = nn.MaxPool2D(<span class="number">2</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.rebnconv6 = REBNCONV(mid_ch, mid_ch, dirate=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.rebnconv7 = REBNCONV(mid_ch, mid_ch, dirate=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.rebnconv6d = REBNCONV(mid_ch * <span class="number">2</span>, mid_ch, dirate=<span class="number">1</span>)</span><br><span class="line">        self.rebnconv5d = REBNCONV(mid_ch * <span class="number">2</span>, mid_ch, dirate=<span class="number">1</span>)</span><br><span class="line">        self.rebnconv4d = REBNCONV(mid_ch * <span class="number">2</span>, mid_ch, dirate=<span class="number">1</span>)</span><br><span class="line">        self.rebnconv3d = REBNCONV(mid_ch * <span class="number">2</span>, mid_ch, dirate=<span class="number">1</span>)</span><br><span class="line">        self.rebnconv2d = REBNCONV(mid_ch * <span class="number">2</span>, mid_ch, dirate=<span class="number">1</span>)</span><br><span class="line">        self.rebnconv1d = REBNCONV(mid_ch * <span class="number">2</span>, out_ch, dirate=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line"></span><br><span class="line">        hx = x</span><br><span class="line">        hxin = self.rebnconvin(hx)</span><br><span class="line"></span><br><span class="line">        hx1 = self.rebnconv1(hxin)</span><br><span class="line">        hx = self.pool1(hx1)</span><br><span class="line"></span><br><span class="line">        hx2 = self.rebnconv2(hx)</span><br><span class="line">        hx = self.pool2(hx2)</span><br><span class="line"></span><br><span class="line">        hx3 = self.rebnconv3(hx)</span><br><span class="line">        hx = self.pool3(hx3)</span><br><span class="line"></span><br><span class="line">        hx4 = self.rebnconv4(hx)</span><br><span class="line">        hx = self.pool4(hx4)</span><br><span class="line"></span><br><span class="line">        hx5 = self.rebnconv5(hx)</span><br><span class="line">        hx = self.pool5(hx5)</span><br><span class="line"></span><br><span class="line">        hx6 = self.rebnconv6(hx)</span><br><span class="line"></span><br><span class="line">        hx7 = self.rebnconv7(hx6)</span><br><span class="line"></span><br><span class="line">        hx6d = self.rebnconv6d(paddle.concat((hx7, hx6), <span class="number">1</span>))</span><br><span class="line">        hx6dup = _upsample_like(hx6d, hx5)</span><br><span class="line"></span><br><span class="line">        hx5d = self.rebnconv5d(paddle.concat((hx6dup, hx5), <span class="number">1</span>))</span><br><span class="line">        hx5dup = _upsample_like(hx5d, hx4)</span><br><span class="line"></span><br><span class="line">        hx4d = self.rebnconv4d(paddle.concat((hx5dup, hx4), <span class="number">1</span>))</span><br><span class="line">        hx4dup = _upsample_like(hx4d, hx3)</span><br><span class="line"></span><br><span class="line">        hx3d = self.rebnconv3d(paddle.concat((hx4dup, hx3), <span class="number">1</span>))</span><br><span class="line">        hx3dup = _upsample_like(hx3d, hx2)</span><br><span class="line"></span><br><span class="line">        hx2d = self.rebnconv2d(paddle.concat((hx3dup, hx2), <span class="number">1</span>))</span><br><span class="line">        hx2dup = _upsample_like(hx2d, hx1)</span><br><span class="line"></span><br><span class="line">        hx1d = self.rebnconv1d(paddle.concat((hx2dup, hx1), <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> hx1d + hxin</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RSU4F</span>(nn.Layer):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;实现RSU-4F模块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_ch=<span class="number">3</span>, mid_ch=<span class="number">12</span>, out_ch=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(RSU4F, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=<span class="number">1</span>)</span><br><span class="line">        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=<span class="number">2</span>)</span><br><span class="line">        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">        self.rebnconv3d = REBNCONV(mid_ch * <span class="number">2</span>, mid_ch, dirate=<span class="number">4</span>)</span><br><span class="line">        self.rebnconv2d = REBNCONV(mid_ch * <span class="number">2</span>, mid_ch, dirate=<span class="number">2</span>)</span><br><span class="line">        self.rebnconv1d = REBNCONV(mid_ch * <span class="number">2</span>, out_ch, dirate=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line"></span><br><span class="line">        hx = x</span><br><span class="line"></span><br><span class="line">        hxin = self.rebnconvin(hx)</span><br><span class="line"></span><br><span class="line">        hx1 = self.rebnconv1(hxin)</span><br><span class="line">        hx2 = self.rebnconv2(hx1)</span><br><span class="line">        hx3 = self.rebnconv3(hx2)</span><br><span class="line"></span><br><span class="line">        hx4 = self.rebnconv4(hx3)</span><br><span class="line"></span><br><span class="line">        hx3d = self.rebnconv3d(paddle.concat((hx4, hx3), <span class="number">1</span>))</span><br><span class="line">        hx2d = self.rebnconv2d(paddle.concat((hx3d, hx2), <span class="number">1</span>))</span><br><span class="line">        hx1d = self.rebnconv1d(paddle.concat((hx2d, hx1), <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> hx1d + hxin</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>完整代码：<a href="https://github.com/PaddlePaddle/PaddleSeg/blob/release%2F2.6/paddleseg/models/u2net.py">https://github.com/PaddlePaddle/PaddleSeg/blob/release%2F2.6/paddleseg/models/u2net.py</a></strong></p>
]]></content>
      <categories>
        <category>图像分割</category>
      </categories>
      <tags>
        <tag>模型</tag>
        <tag>图像分割</tag>
      </tags>
  </entry>
  <entry>
    <title>UNet3+</title>
    <url>/2022/10/31/UNet3plus/</url>
    <content><![CDATA[<h1 id="UNet3-论文详解"><a href="#UNet3-论文详解" class="headerlink" title="UNet3+论文详解"></a>UNet3+论文详解</h1><p>UNet3+利用了全尺度的跳跃连接<code>(skip connection)</code>和深度监督<code>(deep supervisions)</code>。全尺度的跳跃连接把来自不同尺度特征图中的高级语义与低级语义直接结合（当然需要必要的上采样操作）;而深度监督则从多尺度聚合的特征图中学习层次表示。<br><span id="more"></span></p>
<h2 id="1-UNet3-模型结构"><a href="#1-UNet3-模型结构" class="headerlink" title="1. UNet3+模型结构"></a>1. UNet3+模型结构</h2><p><img src="https://pic1.imgdb.cn/item/635f7f9816f2c2beb13cc361.png" alt=""></p>
<p>在<code>UNet3+</code>中，可以从全尺度捕获细粒度的细节和粗粒度的语义。</p>
<p>下图详细说明了构造$X_{De}^{3}$特征图的全过程:</p>
<p><img src="https://pic1.imgdb.cn/item/635f803616f2c2beb13e7a05.png" alt=""></p>
<ul>
<li>较大尺度特征图$X_{En}^1$和$X_{En}^2$。此部分融合了细粒度语义信息。$X_{En}^1$和$X_{En}^2$首先分别以4倍和2倍进行下采样(MaxPooling操作)，然后进行卷积层输出通道数为64，大小和$X_{En}^3$一样的特征图。</li>
<li>相同尺度特征图$X_{En}^3$，此层直接进行卷积输出通道数为64，大小和$X_{En}^3$一样的特征图。</li>
<li>较大尺度特征图$X_{En}^4$和$X_{En}^5$，此部分融合了粗粒度语义信息。$X_{En}^4$和$X_{En}^5$分别以2倍和4倍进行上采样(Bilinear插值插值)，然后进行卷积层输出通道数为64，大小和$X_{En}^3$一样的特征图。</li>
<li>最后将这5层特征图进行拼接，然后进行卷积进一步提取特征，形成新的特征图$X_{Dn}^3$。</li>
</ul>
<h2 id="2-代码实现-Paddle"><a href="#2-代码实现-Paddle" class="headerlink" title="2. 代码实现(Paddle)"></a>2. 代码实现(Paddle)</h2><p>以上述构造$X_{De}^{3}$特征图的来简单实现<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder3</span>(nn.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;De3解码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, filters, cat_channels=<span class="number">80</span>, up_channels=<span class="number">320</span></span>):</span><br><span class="line">        <span class="comment"># h1-&gt;320*320, hd3-&gt;80*80, Pooling 4 times</span></span><br><span class="line">        self.h1_PT_hd3 = nn.MaxPool2D(<span class="number">4</span>, <span class="number">4</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line">        self.h1_PT_hd3_cbr = ConvBnReLU2D(filters[<span class="number">0</span>], cat_channels)</span><br><span class="line">        <span class="comment"># h2-&gt;160*160, hd3-&gt;80*80, Pooling 2 times</span></span><br><span class="line">        self.h2_PT_hd3 = nn.MaxPool2D(<span class="number">2</span>, <span class="number">2</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line">        self.h2_PT_hd3_cbr = ConvBnReLU2D(filters[<span class="number">1</span>], cat_channels)</span><br><span class="line">        <span class="comment"># h3-&gt;80*80, hd3-&gt;80*80, Concatenation</span></span><br><span class="line">        self.h3_Cat_hd3_cbr = ConvBnReLU2D(filters[<span class="number">2</span>], cat_channels)</span><br><span class="line">        <span class="comment"># hd4-&gt;40*40, hd4-&gt;80*80, Upsample 2 times</span></span><br><span class="line">        self.hd4_UT_hd3 = nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">&#x27;bilinear&#x27;</span>)  <span class="comment"># 14*14</span></span><br><span class="line">        self.hd4_UT_hd3_cbr = ConvBnReLU2D(up_channels, cat_channels)</span><br><span class="line">        <span class="comment"># hd5-&gt;20*20, hd4-&gt;80*80, Upsample 4 times</span></span><br><span class="line">        self.hd5_UT_hd3 = nn.Upsample(scale_factor=<span class="number">4</span>, mode=<span class="string">&#x27;bilinear&#x27;</span>)  </span><br><span class="line">        self.hd5_UT_hd3_cbr = ConvBnReLU2D(filters[<span class="number">4</span>], cat_channels)</span><br><span class="line">        <span class="comment"># fusion(h1_PT_hd3, h2_PT_hd3, h3_Cat_hd3, hd4_UT_hd3, hd5_UT_hd3)</span></span><br><span class="line">        self.cbr3d_1 = ConvBnReLU2D(up_channels, up_channels)  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="comment"># h1-&gt;320*320*64 || h2-&gt;160*160*128 </span></span><br><span class="line">        <span class="comment"># h3-&gt;80*80*256 || h4-&gt;40*40*512 || h5-&gt;20*20*1024</span></span><br><span class="line">        h1, h2, h3, h4, hd5 = inputs</span><br><span class="line">        h1_PT_hd3 = self.h1_PT_hd3_cbr(self.h1_PT_hd3(h1))</span><br><span class="line">        h2_PT_hd3 = self.h2_PT_hd3_cbr(self.h2_PT_hd3(h2))</span><br><span class="line">        h3_Cat_hd3 = self.h3_Cat_hd3_cbr(h3)</span><br><span class="line">        hd4_UT_hd3 = self.hd4_UT_hd3_cbr(self.hd4_UT_hd3(hd4))</span><br><span class="line">        hd5_UT_hd3 = self.hd5_UT_hd3_cbr(self.hd5_UT_hd3(hd5))</span><br><span class="line">        <span class="comment"># hd3-&gt;80*80*up_channels</span></span><br><span class="line">        hd3 = self.cbr3d_1(</span><br><span class="line">            paddle.concat(</span><br><span class="line">                [h1_PT_hd3, h2_PT_hd3, h3_Cat_hd3, hd4_UT_hd3, hd5_UT_hd3], <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> hd3</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>完整代码：<a href="https://github.com/PaddlePaddle/PaddleSeg/blob/release%2F2.6/paddleseg/models/unet_3plus.py">https://github.com/PaddlePaddle/PaddleSeg/blob/release%2F2.6/paddleseg/models/unet_3plus.py</a></strong></p>
]]></content>
      <categories>
        <category>图像分割</category>
      </categories>
      <tags>
        <tag>模型</tag>
        <tag>图像分割</tag>
      </tags>
  </entry>
  <entry>
    <title>UNet++</title>
    <url>/2022/10/31/UNetplusplus/</url>
    <content><![CDATA[<h1 id="UNet-论文详解"><a href="#UNet-论文详解" class="headerlink" title="UNet++论文详解"></a>UNet++论文详解</h1><p>编码器-解码器网络广泛用于现代语义和实例分割模型中。作者把它们的成功归功于其跳接，跳接将来自解码器子网的深度、语义、粗粒度特征图与来自编码器子网的浅、低层、细粒度特征图结合在一起，并被证明，即使在复杂背景下，也可以有效地恢复目标对象的细粒度细节。跳接在实例分割模型的成功中也发挥了关键作用，其中的想法是分割并区分所需对象的每个实例。<br><span id="more"></span></p>
<h2 id="1-模型结构"><a href="#1-模型结构" class="headerlink" title="1. 模型结构"></a>1. 模型结构</h2><p><img src="https://pic1.imgdb.cn/item/635f40ca16f2c2beb184272f.png" alt=""></p>
<ul>
<li>红色区域：表示<code>UNet++</code>的模型结构，由编码器和解码器组成，通过一系列嵌套的密集卷积块连接，主要思想是在融合之前弥合编码器和解码器特征图之间的语义差距</li>
<li>蓝色区域：<code>UNet++</code>在推理时可以裁剪</li>
<li>橙色区域：<code>UNet++</code>第一次跳接路径详解</li>
</ul>
<h2 id="2-代码实现"><a href="#2-代码实现" class="headerlink" title="2. 代码实现"></a>2. 代码实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UNetPlusPlus</span>(nn.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    UNet++模型实现.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 in_channels,</span></span><br><span class="line"><span class="params">                 num_classes,</span></span><br><span class="line"><span class="params">                 use_deconv=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 align_corners=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 pretrained=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 is_ds=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(UNetPlusPlus, self).__init__()</span><br><span class="line">        self.pretrained = pretrained</span><br><span class="line">        self.is_ds = is_ds</span><br><span class="line">        channels = [<span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>]</span><br><span class="line">        self.pool = nn.MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.conv0_0 = DoubleConv(in_channels, channels[<span class="number">0</span>])</span><br><span class="line">        self.conv1_0 = DoubleConv(channels[<span class="number">0</span>], channels[<span class="number">1</span>])</span><br><span class="line">        self.conv2_0 = DoubleConv(channels[<span class="number">1</span>], channels[<span class="number">2</span>])</span><br><span class="line">        self.conv3_0 = DoubleConv(channels[<span class="number">2</span>], channels[<span class="number">3</span>])</span><br><span class="line">        self.conv4_0 = DoubleConv(channels[<span class="number">3</span>], channels[<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">        self.up_cat0_1 = UpSampling(</span><br><span class="line">            channels[<span class="number">1</span>],</span><br><span class="line">            channels[<span class="number">0</span>],</span><br><span class="line">            n_cat=<span class="number">2</span>,</span><br><span class="line">            use_deconv=use_deconv,</span><br><span class="line">            align_corners=align_corners)</span><br><span class="line">        self.up_cat1_1 = UpSampling(</span><br><span class="line">            channels[<span class="number">2</span>],</span><br><span class="line">            channels[<span class="number">1</span>],</span><br><span class="line">            n_cat=<span class="number">2</span>,</span><br><span class="line">            use_deconv=use_deconv,</span><br><span class="line">            align_corners=align_corners)</span><br><span class="line">        self.up_cat2_1 = UpSampling(</span><br><span class="line">            channels[<span class="number">3</span>],</span><br><span class="line">            channels[<span class="number">2</span>],</span><br><span class="line">            n_cat=<span class="number">2</span>,</span><br><span class="line">            use_deconv=use_deconv,</span><br><span class="line">            align_corners=align_corners)</span><br><span class="line">        self.up_cat3_1 = UpSampling(</span><br><span class="line">            channels[<span class="number">4</span>],</span><br><span class="line">            channels[<span class="number">3</span>],</span><br><span class="line">            n_cat=<span class="number">2</span>,</span><br><span class="line">            use_deconv=use_deconv,</span><br><span class="line">            align_corners=align_corners)</span><br><span class="line"></span><br><span class="line">        self.up_cat0_2 = UpSampling(</span><br><span class="line">            channels[<span class="number">1</span>],</span><br><span class="line">            channels[<span class="number">0</span>],</span><br><span class="line">            n_cat=<span class="number">3</span>,</span><br><span class="line">            use_deconv=use_deconv,</span><br><span class="line">            align_corners=align_corners)</span><br><span class="line">        self.up_cat1_2 = UpSampling(</span><br><span class="line">            channels[<span class="number">2</span>],</span><br><span class="line">            channels[<span class="number">1</span>],</span><br><span class="line">            n_cat=<span class="number">3</span>,</span><br><span class="line">            use_deconv=use_deconv,</span><br><span class="line">            align_corners=align_corners)</span><br><span class="line">        self.up_cat2_2 = UpSampling(</span><br><span class="line">            channels[<span class="number">3</span>],</span><br><span class="line">            channels[<span class="number">2</span>],</span><br><span class="line">            n_cat=<span class="number">3</span>,</span><br><span class="line">            use_deconv=use_deconv,</span><br><span class="line">            align_corners=align_corners)</span><br><span class="line"></span><br><span class="line">        self.up_cat0_3 = UpSampling(</span><br><span class="line">            channels[<span class="number">1</span>],</span><br><span class="line">            channels[<span class="number">0</span>],</span><br><span class="line">            n_cat=<span class="number">4</span>,</span><br><span class="line">            use_deconv=use_deconv,</span><br><span class="line">            align_corners=align_corners)</span><br><span class="line">        self.up_cat1_3 = UpSampling(</span><br><span class="line">            channels[<span class="number">2</span>],</span><br><span class="line">            channels[<span class="number">1</span>],</span><br><span class="line">            n_cat=<span class="number">4</span>,</span><br><span class="line">            use_deconv=use_deconv,</span><br><span class="line">            align_corners=align_corners)</span><br><span class="line"></span><br><span class="line">        self.up_cat0_4 = UpSampling(</span><br><span class="line">            channels[<span class="number">1</span>],</span><br><span class="line">            channels[<span class="number">0</span>],</span><br><span class="line">            n_cat=<span class="number">5</span>,</span><br><span class="line">            use_deconv=use_deconv,</span><br><span class="line">            align_corners=align_corners)</span><br><span class="line"></span><br><span class="line">        self.out_1 = nn.Conv2D(channels[<span class="number">0</span>], num_classes, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        self.out_2 = nn.Conv2D(channels[<span class="number">0</span>], num_classes, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        self.out_3 = nn.Conv2D(channels[<span class="number">0</span>], num_classes, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        self.out_4 = nn.Conv2D(channels[<span class="number">0</span>], num_classes, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        self.init_weight()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weight</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.pretrained <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            load_entire_model(self, self.pretrained)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> sublayer <span class="keyword">in</span> self.sublayers():</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">isinstance</span>(sublayer, nn.Conv2D):</span><br><span class="line">                    kaiming_normal_init(sublayer.weight)</span><br><span class="line">                <span class="keyword">elif</span> <span class="built_in">isinstance</span>(sublayer, (nn.BatchNorm, nn.SyncBatchNorm)):</span><br><span class="line">                    kaiming_normal_init(sublayer.weight)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="comment"># 0 down</span></span><br><span class="line">        X0_0 = self.conv0_0(inputs)  <span class="comment"># n,32,h,w</span></span><br><span class="line">        pool_0 = self.pool(X0_0)  <span class="comment"># n,32,h/2,w/2</span></span><br><span class="line">        X1_0 = self.conv1_0(pool_0)  <span class="comment"># n,64,h/2,w/2</span></span><br><span class="line">        pool_1 = self.pool(X1_0)  <span class="comment"># n,64,h/4,w/4</span></span><br><span class="line">        X2_0 = self.conv2_0(pool_1)  <span class="comment"># n,128,h/4,w/4</span></span><br><span class="line">        pool_2 = self.pool(X2_0)  <span class="comment"># n,128,h/8,n/8</span></span><br><span class="line">        X3_0 = self.conv3_0(pool_2)  <span class="comment"># n,256,h/8,w/8</span></span><br><span class="line">        pool_3 = self.pool(X3_0)  <span class="comment"># n,256,h/16,w/16</span></span><br><span class="line">        X4_0 = self.conv4_0(pool_3)  <span class="comment"># n,512,h/16,w/16</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1 up+concat</span></span><br><span class="line">        X0_1 = self.up_cat0_1(X1_0, X0_0)  <span class="comment"># n,32,h,w</span></span><br><span class="line">        X1_1 = self.up_cat1_1(X2_0, X1_0)  <span class="comment"># n,64,h/2,w/2</span></span><br><span class="line">        X2_1 = self.up_cat2_1(X3_0, X2_0)  <span class="comment"># n,128,h/4,w/4</span></span><br><span class="line">        X3_1 = self.up_cat3_1(X4_0, X3_0)  <span class="comment"># n,256,h/8,w/8</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2 up+concat</span></span><br><span class="line">        X0_2 = self.up_cat0_2(X1_1, X0_0, X0_1)  <span class="comment"># n,32,h,w</span></span><br><span class="line">        X1_2 = self.up_cat1_2(X2_1, X1_0, X1_1)  <span class="comment"># n,64,h/2,w/2</span></span><br><span class="line">        X2_2 = self.up_cat2_2(X3_1, X2_0, X2_1)  <span class="comment"># n,128,h/4,w/4</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3 up+concat</span></span><br><span class="line">        X0_3 = self.up_cat0_3(X1_2, X0_0, X0_1, X0_2)  <span class="comment"># n,32,h,w</span></span><br><span class="line">        X1_3 = self.up_cat1_3(X2_2, X1_0, X1_1, X1_2)  <span class="comment"># n,64,h/2,w/2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4 up+concat</span></span><br><span class="line">        X0_4 = self.up_cat0_4(X1_3, X0_0, X0_1, X0_2, X0_3)  <span class="comment"># n,32,h,w</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># out conv1*1</span></span><br><span class="line">        out_1 = self.out_1(X0_1)  <span class="comment"># n,num_classes,h,w</span></span><br><span class="line">        out_2 = self.out_2(X0_2)  <span class="comment"># n,num_classes,h,w</span></span><br><span class="line">        out_3 = self.out_3(X0_3)  <span class="comment"># n,num_classes,h,w</span></span><br><span class="line">        out_4 = self.out_4(X0_4)  <span class="comment"># n,num_classes,h,w</span></span><br><span class="line"></span><br><span class="line">        output = (out_1 + out_2 + out_3 + out_4) / <span class="number">4</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.is_ds:</span><br><span class="line">            <span class="keyword">return</span> [output]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> [out_4]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DoubleConv</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 in_channels,</span></span><br><span class="line"><span class="params">                 out_channels,</span></span><br><span class="line"><span class="params">                 filter_size=<span class="number">3</span>,</span></span><br><span class="line"><span class="params">                 stride=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 padding=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(DoubleConv, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2D(in_channels, out_channels, filter_size, stride, padding),</span><br><span class="line">            SyncBatchNorm(out_channels),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2D(out_channels, out_channels, filter_size, stride, padding),</span><br><span class="line">            SyncBatchNorm(out_channels), nn.ReLU())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        conv = self.conv(inputs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> conv</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">UpSampling</span>(nn.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 in_channels,</span></span><br><span class="line"><span class="params">                 out_channels,</span></span><br><span class="line"><span class="params">                 n_cat,</span></span><br><span class="line"><span class="params">                 use_deconv=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 align_corners=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(UpSampling, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> use_deconv:</span><br><span class="line">            self.up = nn.Conv2DTranspose(</span><br><span class="line">                in_channels, out_channels, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.up = nn.Sequential(</span><br><span class="line">                nn.Upsample(</span><br><span class="line">                    scale_factor=<span class="number">2</span>,</span><br><span class="line">                    mode=<span class="string">&#x27;bilinear&#x27;</span>,</span><br><span class="line">                    align_corners=align_corners),</span><br><span class="line">                nn.Conv2D(in_channels, out_channels, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        self.conv = DoubleConv(n_cat * out_channels, out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, high_feature, *low_features</span>):</span><br><span class="line">        features = [self.up(high_feature)]</span><br><span class="line">        <span class="keyword">for</span> feature <span class="keyword">in</span> low_features:</span><br><span class="line">            features.append(feature)</span><br><span class="line">        cat_features = paddle.concat(features, axis=<span class="number">1</span>)</span><br><span class="line">        out = self.conv(cat_features)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>完整代码：<a href="https://github.com/PaddlePaddle/PaddleSeg/blob/release%2F2.6/paddleseg/models/unet_plusplus.py">https://github.com/PaddlePaddle/PaddleSeg/blob/release%2F2.6/paddleseg/models/unet_plusplus.py</a></strong></p>
]]></content>
      <categories>
        <category>图像分割</category>
      </categories>
      <tags>
        <tag>模型</tag>
        <tag>图像分割</tag>
      </tags>
  </entry>
</search>
